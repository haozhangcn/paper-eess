# ArXiv eess --Tue, 21 Sep 2021
### 1.Deep Anomaly Generation: An Image Translation Approach of Synthesizing Abnormal Banded Chromosome Images  [ :arrow_down: ](https://arxiv.org/pdf/2109.09702.pdf)
>  Advances in deep-learning-based pipelines have led to breakthroughs in a variety of microscopy image diagnostics. However, a sufficiently big training data set is usually difficult to obtain due to high annotation costs. In the case of banded chromosome images, the creation of big enough libraries is difficult for multiple pathologies due to the rarity of certain genetic disorders. Generative Adversarial Networks (GANs) have proven to be effective in generating synthetic images and extending training data sets. In our work, we implement a conditional adversarial network that allows generation of realistic single chromosome images following user-defined banding patterns. To this end, an image-to-image translation approach based on self-generated 2D chromosome segmentation label maps is used. Our validation shows promising results when synthesizing chromosomes with seen as well as unseen banding patterns. We believe that this approach can be exploited for data augmentation of chromosome data sets with structural abnormalities. Therefore, the proposed method could help to tackle medical image analysis problems such as data simulation, segmentation, detection, or classification in the field of cytogenetics.      
### 2.Acoustic Echo Cancellation using Residual U-Nets  [ :arrow_down: ](https://arxiv.org/pdf/2109.09686.pdf)
>  This paper presents an acoustic echo canceler based on a U-Net convolutional neural network for single-talk and double-talk scenarios. U-Net networks have previously been used in the audio processing area for source separation problems because of their ability to reproduce the finest details of audio signals, but to our knowledge, this is the first time they have been used for acoustic echo cancellation (AEC). The U-Net hyperparameters have been optimized to obtain the best AEC performance, but using a reduced number of parameters to meet a latency restriction of 40 ms. The training and testing of our model have been carried out within the framework of the 'ICASSP 2021 AEC Challenge' organized by Microsoft. We have trained the optimized U-Net model with a synthetic dataset only (S-U-Net) and with a synthetic dataset and the single-talk set of a real dataset (SR-U-Net), both datasets were released for the challenge. The S-U-Net model presented better results for double-talk scenarios, thus their inferred near-end signals from the blind testset were submitted to the challenge. Our canceler ranked 12th among 17 teams, and 5th among 10 academia teams, obtaining an overall mean opinion score of 3.57.      
### 3.Development of In Situ Acoustic Instruments for The Aquatic Environment Study  [ :arrow_down: ](https://arxiv.org/pdf/2109.09684.pdf)
>  Based on the analysis of existing acoustic methods and instruments, a prototype of an automated instrument has been developed to perform joint measurements in situ of two parameters: sound speed and ultrasound attenuation. The device is based on existing sound velocity profilers. It was proposed to replace the TDC-GP22 converters used in the sound speed meter ISZ-1 with more advanced modern modified converters TDC-GP30, which can significantly improve the accuracy of measuring the amplitude of the reflected acoustic signal. The programs for processing signals from the primary acoustic transducer have been developed. The model of the device passed preliminary tests.      
### 4.Single-ended Coherent Receiver  [ :arrow_down: ](https://arxiv.org/pdf/2109.09683.pdf)
>  Commercial coherent receivers utilize balanced photodetectors (PDs) with high single-port rejection ratio (SPRR) to mitigate the signal-signal beat interference (SSBI) due to the square-law detection process. As the symbol rates of coherent transponders are increased to 100 Gbaud and beyond, maintaining a high SPRR in a cost-effective manner becomes more and more challenging. One potential approach for solving this problem is to leverage the concept of single-ended coherent receiver (SER) where single-ended PDs are used instead of the balanced PDs. In this case, the resulting SSBI should be mitigated in the digital domain. In this paper, we show that SSBI can be effectively mitigated using various low-complexity techniques, such as the direct filed reconstruction (DFR), clipped iterative SSBI cancellation (CIC) and gradient decent (GD). In addition, we present a self-calibration technique for SERs which can be extended for characterizing the optical-to-electrical (O/E) response of a conventional balanced coherent receiver (BR). Using the developed techniques, we then experimentally demonstrate a 90 Gbaud probabilistically constellation shaped 64-QAM (PCS-64QAM) transmission using a SER, achieving a net data rate of 882 Gb/s over 100 km of standard single mode fiber (SSMF). The sensitivity penalty compared to the BR is below 0.5 dB. We expect that when the symbol rate is increased further, a SER can potentially outperform a BR, especially when applied to cost-sensitive commercial pluggable coherent transceivers      
### 5.Convolution and Correlation Theorems for Wigner-Ville Distribution Associated with the Quaternion Offset Linear Canonical Transform  [ :arrow_down: ](https://arxiv.org/pdf/2109.09682.pdf)
>  The quaternion offset linear canonical transform(QOLCT) has gained much popularity in recent years because of its applications in many areas, including color image and signal processing. At the same time the applications of Wigner-Ville distribution (WVD) in signal analysis and image processing can not be excluded. In this paper we investigate the Winger-Ville Distribution associated with quaternion offset linear canonical transform (WVD-QOLCT). Firstly, we propose the definition of the WVD-QOLCT, and then several important properties of newly defined WVD-QOLCT, such as nonlinearity, bounded, reconstruction formula, orthogonality relation and Plancherel formula are derived. Secondly a novel canonical convolution operator and a related correlation operator for WVD-QOLCT are proposed. Moreover, based on the proposed operators, the corresponding generalized convolution, correlation theorems are studied.We also show that the convolution and correlation theorems of the QWVD and WVD-QLCT can be looked as a special case of our achieved results.      
### 6.Improving Text-Independent Speaker Verification with Auxiliary Speakers Using Graph  [ :arrow_down: ](https://arxiv.org/pdf/2109.09674.pdf)
>  The paper presents a novel approach to refining similarity scores between input utterances for robust speaker verification. Given the embeddings from a pair of input utterances, a graph model is designed to incorporate additional information from a group of embeddings representing the so-called auxiliary speakers. The relations between the input utterances and the auxiliary speakers are represented by the edges and vertices in the graph. The similarity scores are refined by iteratively updating the values of the graph's vertices using an algorithm similar to the random walk algorithm on graphs. Through this updating process, the information of auxiliary speakers is involved in determining the relation between input utterances and hence contributing to the verification process. We propose to create a set of artificial embeddings through the model training process. Utilizing the generated embeddings as auxiliary speakers, no extra data are required for the graph model in the verification stage. The proposed model is trained in an end-to-end manner within the whole system. Experiments are carried out with the Voxceleb datasets. The results indicate that involving auxiliary speakers with graph is effective to improve speaker verification performance.      
### 7.Hybrid Transceiver Design for Tera-Hertz MIMO Systems Relying on Bayesian Learning Aided Sparse Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2109.09664.pdf)
>  Hybrid transceiver design in multiple-input multiple-output (MIMO) Tera-Hertz (THz) systems relying on sparse channel state information (CSI) estimation techniques is conceived. To begin with, a practical MIMO channel model is developed for the THz band that incorporates its molecular absorption and reflection losses, as well as its non-line-of-sight (NLoS) rays associated with its diffused components. Subsequently, a novel CSI estimation model is derived by exploiting the angular-sparsity of the THz MIMO channel. Then an orthogonal matching pursuit (OMP)-based framework is conceived, followed by designing a sophisticated Bayesian learning (BL)-based approach for efficient estimation of the sparse THz MIMO channel. The Bayesian Cramer-Rao Lower Bound (BCRLB) is also determined for benchmarking the performance of the CSI estimation techniques developed. Finally, an optimal hybrid transmit precoder and receiver combiner pair is designed, which directly relies on the beamspace domain CSI estimates and only requires limited feedback. Finally, simulation results are provided for quantifying the improved mean square error (MSE), spectral-efficiency (SE) and bit-error rate (BER) performance for transmission on practical THz MIMO channel obtained from the HIgh resolution TRANsmission (HITRAN)-database.      
### 8.DEM Super-Resolution with EfficientNetV2  [ :arrow_down: ](https://arxiv.org/pdf/2109.09661.pdf)
>  Efficient climate change monitoring and modeling rely on high-quality geospatial and environmental datasets. Due to limitations in technical capabilities or resources, the acquisition of high-quality data for many environmental disciplines is costly. Digital Elevation Model (DEM) datasets are such examples whereas their low-resolution versions are widely available, high-resolution ones are scarce. In an effort to rectify this problem, we propose and assess an EfficientNetV2 based model. The proposed model increases the spatial resolution of DEMs up to 16times without additional information.      
### 9.Nonsmooth convex optimization to estimate the Covid-19 reproduction number space-time evolution with robustness against low quality data  [ :arrow_down: ](https://arxiv.org/pdf/2109.09595.pdf)
>  Daily pandemic surveillance, often achieved through the estimation of the reproduction number, constitutes a critical challenge for national health authorities to design countermeasures. In an earlier work, we proposed to formulate the estimation of the reproduction number as an optimization problem, combining data-model fidelity and space-time regularity constraints, solved by nonsmooth convex proximal minimizations. Though promising, that first formulation significantly lacks robustness against the Covid-19 data low quality (irrelevant or missing counts, pseudo-seasonalities,.. .) stemming from the emergency and crisis context, which significantly impairs accurate pandemic evolution assessments. The present work aims to overcome these limitations by carefully crafting a functional permitting to estimate jointly, in a single step, the reproduction number and outliers defined to model low quality data. This functional also enforces epidemiology-driven regularity properties for the reproduction number estimates, while preserving convexity, thus permitting the design of efficient minimization algorithms, based on proximity operators that are derived analytically. The explicit convergence of the proposed algorithm is proven theoretically. Its relevance is quantified on real Covid-19 data, consisting of daily new infection counts for 200+ countries and for the 96 metropolitan France counties, publicly available at Johns Hopkins University and Sant{Ã©}-Publique-France. The procedure permits automated daily updates of these estimates, reported via animated and interactive maps. Open-source estimation procedures will be made publicly available.      
### 10.Low-rank tensor recovery for Jacobian-based Volterra identification of parallel Wiener-Hammerstein systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.09584.pdf)
>  We consider the problem of identifying a parallel Wiener-Hammerstein structure from Volterra kernels. Methods based on Volterra kernels typically resort to coupled tensor decompositions of the kernels. However, in the case of parallel Wiener-Hammerstein systems, such methods require nontrivial constraints on the factors of the decompositions. In this paper, we propose an entirely different approach: by using special sampling (operating) points for the Jacobian of the nonlinear map from past inputs to the output, we can show that the Jacobian matrix becomes a linear projection of a tensor whose rank is equal to the number of branches. This representation allows us to solve the identification problem as a tensor recovery problem.      
### 11.Accelerated Stochastic Gradient for Nonnegative Tensor Completion and Parallel Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2109.09534.pdf)
>  We consider the problem of nonnegative tensor completion. We adopt the alternating optimization framework and solve each nonnegative matrix completion problem via a stochastic variation of the accelerated gradient algorithm. We experimentally test the effectiveness and the efficiency of our algorithm using both real-world and synthetic data. We develop a shared-memory implementation of our algorithm using the multi-threaded API OpenMP, which attains significant speedup. We believe that our approach is a very competitive candidate for the solution of very large nonnegative tensor completion problems.      
### 12.RibSeg Dataset and Strong Point Cloud Baselines for Rib Segmentation from CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2109.09521.pdf)
>  Manual rib inspections in computed tomography (CT) scans are clinically critical but labor-intensive, as 24 ribs are typically elongated and oblique in 3D volumes. Automatic rib segmentation methods can speed up the process through rib measurement and visualization. However, prior arts mostly use in-house labeled datasets that are publicly unavailable and work on dense 3D volumes that are computationally inefficient. To address these issues, we develop a labeled rib segmentation benchmark, named \emph{RibSeg}, including 490 CT scans (11,719 individual ribs) from a public dataset. For ground truth generation, we used existing morphology-based algorithms and manually refined its results. Then, considering the sparsity of ribs in 3D volumes, we thresholded and sampled sparse voxels from the input and designed a point cloud-based baseline method for rib segmentation. The proposed method achieves state-of-the-art segmentation performance (Dice~$\approx95\%$) with significant efficiency ($10\sim40\times$ faster than prior arts). The RibSeg dataset, code, and model in PyTorch are available at <a class="link-external link-https" href="https://github.com/M3DV/RibSeg" rel="external noopener nofollow">this https URL</a>.      
### 13.Primary Tumor and Inter-Organ Augmentations for Supervised Lymph Node Colon Adenocarcinoma Metastasis Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.09518.pdf)
>  The scarcity of labeled data is a major bottleneck for developing accurate and robust deep learning-based models for histopathology applications. The problem is notably prominent for the task of metastasis detection in lymph nodes, due to the tissue's low tumor-to-non-tumor ratio, resulting in labor- and time-intensive annotation processes for the pathologists. This work explores alternatives on how to augment the training data for colon carcinoma metastasis detection when there is limited or no representation of the target domain. Through an exhaustive study of cross-validated experiments with limited training data availability, we evaluate both an inter-organ approach utilizing already available data for other tissues, and an intra-organ approach, utilizing the primary tumor. Both these approaches result in little to no extra annotation effort. Our results show that these data augmentation strategies can be an efficient way of increasing accuracy on metastasis detection, but fore-most increase robustness.      
### 14.The Devil Is in the Details: An Efficient Convolutional Neural Network for Transport Mode Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.09504.pdf)
>  Transport mode detection is a classification problem aiming to design an algorithm that can infer the transport mode of a user given multimodal signals (GPS and/or inertial sensors). It has many applications, such as carbon footprint tracking, mobility behaviour analysis, or real-time door-to-door smart planning. Most current approaches rely on a classification step using Machine Learning techniques, and, like in many other classification problems, deep learning approaches usually achieve better results than traditional machine learning ones using handcrafted features. Deep models, however, have a notable downside: they are usually heavy, both in terms of memory space and processing cost. We show that a small, optimized model can perform as well as a current deep model. During our experiments on the GeoLife and SHL 2018 datasets, we obtain models with tens of thousands of parameters, that is, 10 to 1,000 times less parameters and operations than networks from the state of the art, which still reach a comparable performance. We also show, using the aforementioned datasets, that the current preprocessing used to deal with signals of different lengths is suboptimal, and we provide better replacements. Finally, we introduce a way to use signals with different lengths with the lighter Convolutional neural networks, without using the heavier Recurrent Neural Networks.      
### 15.Multi-Layer SIS Model with an Infrastructure Network  [ :arrow_down: ](https://arxiv.org/pdf/2109.09493.pdf)
>  This paper deals with the spread of diseases over both a population network and an infrastructure network. We develop a layered networked spread model for a susceptible-infected-susceptible (SIS) pathogen-borne disease spreading over a human contact network and an infrastructure network, and refer to it as a layered networked susceptible-infected-water-susceptible (SIWS) model. The SIWS network is in the healthy state (also referred to as the disease-free equilibrium) if none of the individuals in the population are infected nor is the infrastructure network contaminated; otherwise, we say that the network is in the endemic state (also referred to as the endemic equilibrium). First, we establish sufficient conditions for local exponential stability and global asymptotic stability (GAS) of the healthy state. Second, we provide sufficient conditions for existence, uniqueness, and GAS of the endemic state. Building off of these results, we provide a necessary, and sufficient, condition for the healthy state to be the unique equilibrium of our model. Third, we show that the endemic equilibrium of the SIWS model is worse than that of the networked SIS model without any infrastructure network, in the sense that at least one subpopulation has strictly larger infection proportion at the endemic equilibrium in the former model than that in the latter. Fourth, we study an observability problem, and, assuming that the measurements of the sickness-levels of the human contact network are available, provide a necessary and sufficient condition for estimation of the pathogen levels in the infrastructure network. Furthermore, we provide another sufficient, but not necessary, condition for estimation of pathogen levels in the infrastructure network.      
### 16.On Circuit-based Hybrid Quantum Neural Networks for Remote Sensing Imagery Classification  [ :arrow_down: ](https://arxiv.org/pdf/2109.09484.pdf)
>  This article aims to investigate how circuit-based hybrid Quantum Convolutional Neural Networks (QCNNs) can be successfully employed as image classifiers in the context of remote sensing. The hybrid QCNNs enrich the classical architecture of CNNs by introducing a quantum layer within a standard neural network. The novel QCNN proposed in this work is applied to the Land Use and Land Cover (LULC) classification, chosen as an Earth Observation (EO) use case, and tested on the EuroSAT dataset used as reference benchmark. The results of the multiclass classification prove the effectiveness of the presented approach, by demonstrating that the QCNN performances are higher than the classical counterparts. Moreover, investigation of various quantum circuits shows that the ones exploiting quantum entanglement achieve the best classification scores. This study underlines the potentialities of applying quantum computing to an EO case study and provides the theoretical and experimental background for futures investigations.      
### 17.Predicting Visual Improvement after Macular Hole Surgery: a Cautionary Tale on Deep Learning with Very Limited Data  [ :arrow_down: ](https://arxiv.org/pdf/2109.09463.pdf)
>  We investigate the potential of machine learning models for the prediction of visual improvement after macular hole surgery from preoperative data (retinal images and clinical features). Collecting our own data for the task, we end up with only 121 total samples, putting our work in the very limited data regime. We explore a variety of deep learning methods for limited data to train deep computer vision models, finding that all tested deep vision models are outperformed by a simple regression model on the clinical features. We believe this is compelling evidence of the extreme difficulty of using deep learning on very limited data.      
### 18.Towards Ubiquitous Indoor Positioning: Comparing Systems across Heterogeneous Datasets  [ :arrow_down: ](https://arxiv.org/pdf/2109.09436.pdf)
>  The evaluation of Indoor Positioning Systems (IPS) mostly relies on local deployments in the researchers' or partners' facilities. The complexity of preparing comprehensive experiments, collecting data, and considering multiple scenarios usually limits the evaluation area and, therefore, the assessment of the proposed systems. The requirements and features of controlled experiments cannot be generalized since the use of the same sensors or anchors density cannot be guaranteed. The dawn of datasets is pushing IPS evaluation to a similar level as machine-learning models, where new proposals are evaluated over many heterogeneous datasets. This paper proposes a way to evaluate IPSs in multiple scenarios, that is validated with three use cases. The results prove that the proposed aggregation of the evaluation metric values is a useful tool for high-level comparison of IPSs.      
### 19.Improved AI-based segmentation of apical and basal slices from clinical cine CMR  [ :arrow_down: ](https://arxiv.org/pdf/2109.09421.pdf)
>  Current artificial intelligence (AI) algorithms for short-axis cardiac magnetic resonance (CMR) segmentation achieve human performance for slices situated in the middle of the heart. However, an often-overlooked fact is that segmentation of the basal and apical slices is more difficult. During manual analysis, differences in the basal segmentations have been reported as one of the major sources of disagreement in human interobserver variability. In this work, we aim to investigate the performance of AI algorithms in segmenting basal and apical slices and design strategies to improve their segmentation. We trained all our models on a large dataset of clinical CMR studies obtained from two NHS hospitals (n=4,228) and evaluated them against two external datasets: ACDC (n=100) and M&amp;Ms (n=321). Using manual segmentations as a reference, CMR slices were assigned to one of four regions: non-cardiac, base, middle, and apex. Using the nnU-Net framework as a baseline, we investigated two different approaches to reduce the segmentation performance gap between cardiac regions: (1) non-uniform batch sampling, which allows us to choose how often images from different regions are seen during training; and (2) a cardiac-region classification model followed by three (i.e. base, middle, and apex) region-specific segmentation models. We show that the classification and segmentation approach was best at reducing the performance gap across all datasets. We also show that improvements in the classification performance can subsequently lead to a significantly better performance in the segmentation task.      
### 20.A novel optical needle probe for deep learning-based tissue elasticity characterization  [ :arrow_down: ](https://arxiv.org/pdf/2109.09362.pdf)
>  The distinction between malignant and benign tumors is essential to the treatment of cancer. The tissue's elasticity can be used as an indicator for the required tissue characterization. Optical coherence elastography (OCE) probes have been proposed for needle insertions but have so far lacked the necessary load sensing capabilities. We present a novel OCE needle probe that provides simultaneous optical coherence tomography (OCT) imaging and load sensing at the needle tip. We demonstrate the application of the needle probe in indentation experiments on gelatin phantoms with varying gelatin concentrations. We further implement two deep learning methods for the end-to-end sample characterization from the acquired OCT data. We report the estimation of gelatin sample concentrations in unseen samples with a mean error of $1.21 \pm 0.91$ wt\%. Both evaluated deep learning models successfully provide sample characterization with different advantages regarding the accuracy and inference time.      
### 21.Estimation Algorithm Non-Stationary Frequency of the Sinusoidal Signal  [ :arrow_down: ](https://arxiv.org/pdf/2109.09347.pdf)
>  The article considers the problem of identifying the variable frequency of a sinusoidal signal. To obtain a regression model of the signal, an iterative differentiation of the original analytical expression is performed, and the swapping lemma is applied. The estimation of the parameters of the non-stationary frequency is implemented using the dynamic expansion of the regressor and mixing (DREM) procedure and the Luenberger observer. As a result of the numerical simulation, the efficiency of the proposed algorithm is demonstrated, showing the convergence of the frequency estimate to the true value.      
### 22.Distributed Detection and Mitigation of Biasing Attacks over Multi-Agent Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.09329.pdf)
>  This paper proposes a distributed attack detection and mitigation technique based on distributed estimation over a multi-agent network, where the agents take partial system measurements susceptible to (possible) biasing attacks. In particular, we assume that the system is not locally observable via the measurements in the direct neighborhood of any agent. First, for performance analysis in the attack-free case, we show that the proposed distributed estimation is unbiased with bounded mean-square deviation in steady-state. Then, we propose a residual-based strategy to locally detect possible attacks at agents. In contrast to the deterministic thresholds in the literature assuming an upper bound on the noise support, we define the thresholds on the residuals in a probabilistic sense. After detecting and isolating the attacked agent, a system-digraph-based mitigation strategy is proposed to replace the attacked measurement with a new observationally-equivalent one to recover potential observability loss. We adopt a graph-theoretic method to classify the agents based on their measurements, to distinguish between the agents recovering the system rank-deficiency and the ones recovering output-connectivity of the system digraph. The attack detection/mitigation strategy is specifically described for each type, which is of polynomial-order complexity for large-scale applications. Illustrative simulations support our theoretical results.      
### 23.Design, Simulation and Feasibility Analysis of Bifacial Solar PV System in Marine Drive Road, Cox's Bazar  [ :arrow_down: ](https://arxiv.org/pdf/2109.09297.pdf)
>  This paper proposes a design and simulation based investigative analysis of a vertically mounted bifacial solar photovoltaic model in Marine Drive Road, Cox's Bazar. Cox's bazar is a famous tourist destination which seems to be a flexible site for implementing such energy harvesting system without affecting the nearby eco-system and solves the existing land shortage problem. Moreover, the infrastructure will provide insulation to noise related problem faced by nearby residents, arising from traffic noises. A model road of 200 meters is reconnoitered for energy harvesting by solar power using three prominent software namely PVSOL, PVsyst, and SAM where a promising mean annual yield of 70492.9 kWh is obtained, and the bifacial gain is calculated to be 12.26%. In addition, a deviation analysis is performed among each of the software and it is found that PVSOL and PVsyst have shown less deviation. Furthermore, a comprehensive financial analysis shows total installation cost to be 84759.74$.      
### 24.Automatic 3D Ultrasound Segmentation of Uterus Using Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.09283.pdf)
>  On-line segmentation of the uterus can aid effective image-based guidance for precise delivery of dose to the target tissue (the uterocervix) during cervix cancer radiotherapy. 3D ultrasound (US) can be used to image the uterus, however, finding the position of uterine boundary in US images is a challenging task due to large daily positional and shape changes in the uterus, large variation in bladder filling, and the limitations of 3D US images such as low resolution in the elevational direction and imaging aberrations. Previous studies on uterus segmentation mainly focused on developing semi-automatic algorithms where require manual initialization to be done by an expert clinician. Due to limited studies on the automatic 3D uterus segmentation, the aim of the current study was to overcome the need for manual initialization in the semi-automatic algorithms using the recent deep learning-based algorithms. Therefore, we developed 2D UNet-based networks that are trained based on two scenarios. In the first scenario, we trained 3 different networks on each plane (i.e., sagittal, coronal, axial) individually. In the second scenario, our proposed network was trained using all the planes of each 3D volume. Our proposed schematic can overcome the initial manual selection of previous semi-automatic algorithm.      
### 25.Interpolation variable rate image compression  [ :arrow_down: ](https://arxiv.org/pdf/2109.09280.pdf)
>  Compression standards have been used to reduce the cost of image storage and transmission for decades. In recent years, learned image compression methods have been proposed and achieved compelling performance to the traditional standards. However, in these methods, a set of different networks are used for various compression rates, resulting in a high cost in model storage and training. Although some variable-rate approaches have been proposed to reduce the cost by using a single network, most of them brought some performance degradation when applying fine rate control. To enable variable-rate control without sacrificing the performance, we propose an efficient Interpolation Variable-Rate (IVR) network, by introducing a handy Interpolation Channel Attention (InterpCA) module in the compression network. With the use of two hyperparameters for rate control and linear interpolation, the InterpCA achieves a fine PSNR interval of 0.001 dB and a fine rate interval of 0.0001 Bits-Per-Pixel (BPP) with 9000 rates in the IVR network. Experimental results demonstrate that the IVR network is the first variable-rate learned method that outperforms VTM 9.0 (intra) in PSNR and Multiscale Structural Similarity (MS-SSIM).      
### 26.DeepStationing: Thoracic Lymph Node Station Parsing in CT Scans using Anatomical Context Encoding and Key Organ Auto-Search  [ :arrow_down: ](https://arxiv.org/pdf/2109.09271.pdf)
>  Lymph node station (LNS) delineation from computed tomography (CT) scans is an indispensable step in radiation oncology workflow. High inter-user variabilities across oncologists and prohibitive laboring costs motivated the automated approach. Previous works exploit anatomical priors to infer LNS based on predefined ad-hoc margins. However, without voxel-level supervision, the performance is severely limited. LNS is highly context-dependent - LNS boundaries are constrained by anatomical organs - we formulate it as a deep spatial and contextual parsing problem via encoded anatomical organs. This permits the deep network to better learn from both CT appearance and organ context. We develop a stratified referencing organ segmentation protocol that divides the organs into anchor and non-anchor categories and uses the former's predictions to guide the later segmentation. We further develop an auto-search module to identify the key organs that opt for the optimal LNS parsing performance. Extensive four-fold cross-validation experiments on a dataset of 98 esophageal cancer patients (with the most comprehensive set of 12 LNSs + 22 organs in thoracic region to date) are conducted. Our LNS parsing model produces significant performance improvements, with an average Dice score of 81.1% +/- 6.1%, which is 5.0% and 19.2% higher over the pure CT-based deep model and the previous representative approach, respectively.      
### 27.Robust Automated Framework for COVID-19 Disease Identification from a Multicenter Dataset of Chest CT Scans  [ :arrow_down: ](https://arxiv.org/pdf/2109.09241.pdf)
>  The objective of this study is to develop a robust deep learning-based framework to distinguish COVID-19, Community-Acquired Pneumonia (CAP), and Normal cases based on chest CT scans acquired in different imaging centers using various protocols, and radiation doses. We showed that while our proposed model is trained on a relatively small dataset acquired from only one imaging center using a specific scanning protocol, the model performs well on heterogeneous test sets obtained by multiple scanners using different technical parameters. We also showed that the model can be updated via an unsupervised approach to cope with the data shift between the train and test sets and enhance the robustness of the model upon receiving a new external dataset from a different center. We adopted an ensemble architecture to aggregate the predictions from multiple versions of the model. For initial training and development purposes, an in-house dataset of 171 COVID-19, 60 CAP, and 76 Normal cases was used, which contained volumetric CT scans acquired from one imaging center using a constant standard radiation dose scanning protocol. To evaluate the model, we collected four different test sets retrospectively to investigate the effects of the shifts in the data characteristics on the model's performance. Among the test cases, there were CT scans with similar characteristics as the train set as well as noisy low-dose and ultra-low dose CT scans. In addition, some test CT scans were obtained from patients with a history of cardiovascular diseases or surgeries. The entire test dataset used in this study contained 51 COVID-19, 28 CAP, and 51 Normal cases. Experimental results indicate that our proposed framework performs well on all test sets achieving total accuracy of 96.15% (95%CI: [91.25-98.74]), COVID-19 sensitivity of 96.08% (95%CI: [86.54-99.5]), CAP sensitivity of 92.86% (95%CI: [76.50-99.19]).      
### 28.A Data-Driven Convergence Bidding Strategy Based on Reverse Engineering of Market Participants' Performance: A Case of California ISO  [ :arrow_down: ](https://arxiv.org/pdf/2109.09238.pdf)
>  Convergence bidding, a.k.a., virtual bidding, has been widely adopted in wholesale electricity markets in recent years. It provides opportunities for market participants to arbitrage on the difference between the day-ahead market locational marginal prices and the real-time market locational marginal prices. Given the fact that convergence bids (CBs) have a significant impact on the operation of electricity markets, it is important to understand how market participants strategically select their CBs in real-world. We address this open problem with focus on the electricity market that is operated by the California ISO. In this regard, we use the publicly available electricity market data to learn, characterize, and evaluate different types of convergence bidding strategies that are currently used by market participants. Our analysis includes developing a data-driven reverse engineering method that we apply to three years of real-world data. Our analysis involves feature selection and density-based data clustering. It results in identifying three main clusters of CB strategies in the California ISO market. Different characteristics and the performance of each cluster of strategies are analyzed. Interestingly, we unmask a common real-world strategy that does not match any of the existing strategic convergence bidding methods in the literature. Next, we build upon the lessons learned from the existing real-world strategies to propose a new CB strategy that can significantly outperform them. Our analysis includes developing a new strategy for convergence bidding. The new strategy has three steps: net profit maximization by capturing price spikes, dynamic node labeling, and strategy selection algorithm. We show through case studies that the annual net profit for the most lucrative market participants can increase by over 40% if the proposed convergence bidding strategy is used.      
### 29.Energy-Efficient Design for IRS-Assisted MEC Networks with NOMA  [ :arrow_down: ](https://arxiv.org/pdf/2109.09217.pdf)
>  Energy-efficient design is of crucial importance in wireless internet of things (IoT) networks. In order to serve massive users while achieving an energy-efficient operation, an intelligent reflecting surface (IRS) assisted mobile edge computing (MEC) network with non-orthogonal multiple access (NOMA) is studied in this paper. The energy efficiency (EE) is maximized by jointly optimizing the offloading power, local computing frequency, receiving beamforming, and IRS phase-shift matrix. The problem is challenging to solve due to the non-convex fractional objective functions and the coupling among the variables. A semidefinite programming relaxation (SDR) based alternating algorithm is developed. Simulation results demonstrate that the proposed design outperforms the benchmark schemes in terms of EE. Applying IRS and NOMA can effectively improve the performance of the MEC network.      
### 30.DeepPoint: A Deep Learning Model for 3D Reconstruction in Point Clouds via mmWave Radar  [ :arrow_down: ](https://arxiv.org/pdf/2109.09188.pdf)
>  Recent research has shown that mmWave radar sensing is effective for object detection in low visibility environments, which makes it an ideal technique in autonomous navigation systems such as autonomous vehicles. However, due to the characteristics of radar signals such as sparsity, low resolution, specularity, and high noise, it is still quite challenging to reconstruct 3D object shapes via mmWave radar sensing. Built on our recent proposed 3DRIMR (3D Reconstruction and Imaging via mmWave Radar), we introduce in this paper DeepPoint, a deep learning model that generates 3D objects in point cloud format that significantly outperforms the original 3DRIMR design. The model adopts a conditional Generative Adversarial Network (GAN) based deep neural network architecture. It takes as input the 2D depth images of an object generated by 3DRIMR's Stage 1, and outputs smooth and dense 3D point clouds of the object. The model consists of a novel generator network that utilizes a sequence of DeepPoint blocks or layers to extract essential features of the union of multiple rough and sparse input point clouds of an object when observed from various viewpoints, given that those input point clouds may contain many incorrect points due to the imperfect generation process of 3DRIMR's Stage 1. The design of DeepPoint adopts a deep structure to capture the global features of input point clouds, and it relies on an optimally chosen number of DeepPoint blocks and skip connections to achieve performance improvement over the original 3DRIMR design. Our experiments have demonstrated that this model significantly outperforms the original 3DRIMR and other standard techniques in reconstructing 3D objects.      
### 31.Autonomous orbit determination for satelite formations using relative sensing: observability analysis and optimization  [ :arrow_down: ](https://arxiv.org/pdf/2109.09186.pdf)
>  Orbit determination of spacecraft in orbit has been mostly dependent on either GNSS satellite signals or ground station telemetry. Both methods present their limitations, however: GNSS signals can only be used effectively in earth orbit, and ground-based orbit determination presents an inherent latency that increases with the Earth-spacecraft distance. For spacecraft flying formations, an alternative method of orbit determination, independent of external signals, consists in the observation of the spacecraft's position with respect to the central body through the relative positioning history of the spacecraft within the formation. In this paper, the potential of the relative positioning method is demonstrated in the context of the SunRISE mission, and compared with the mission's previously proposed orbit determination methods. An optimization study is then made to find the optimal placement of a new spacecraft in the formation so as to maximize the positioning accuracy of the system. Finally, the possibility of removing part of the system's relative bearing measurements while maintaing its observability is also studied. The resulting system is found to be observable, but ill-conditioned.      
### 32.Deep-Learning based Motion Correction for Myocardial T1 Mapping  [ :arrow_down: ](https://arxiv.org/pdf/2109.09146.pdf)
>  Myocardial T1 mapping is a cardiac MRI technique, used to assess myocardial fibrosis. In this technique, a series of T1-weighted MRI images are acquired with different saturation or inversion times. These images are fitted to the T1 model to estimate the model parameters and construct the desired T1 maps. In the presence of motion, the different T1-weighted images are not aligned. This, in turn, will cause errors and inaccuracies in the final estimation of the T1 maps. Therefore, motion correction is a necessary process for myocardial T1 mapping. We present a deep-learning (DL) based system for cardiac T1-weighted MRI images motion correction. When applying our DL-based motion correction system we achieve a statistically significant improved performance by means of R2 of the model fitting regression, in compared to the model fitting regression without motion correction (0.52 vs 0.29, p&lt;0.05).      
### 33.Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.09129.pdf)
>  Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that affect patients' social abilities. In recent years, deep learning methods have been employed to detect ASD through functional MRI (fMRI). However, existing approaches solely concentrated on the abnormal brain functional connections but ignored the importance of regional activities. Due to this biased prior knowledge, previous diagnosis models suffered from inter-site heterogeneity and inter-individual phenotypical differences. To address this issue, we propose a novel feature extraction method for fMRI that can learn a personalized lowe-resolution representation of the entire brain networking regarding both the functional connections and regional activities. First, we abstract the brain imaging as a graph structure, where nodes represent brain areas and edges denote functional connections, and downsample it to a sparse network by hierarchical graph pooling. Subsequently, by assigning each subject with the extracted features and building edges through inter-individual non-imaging characteristics, we build a population graph. The non-identically distributed node features are further recalibrated to node embeddings learned by graph convolutional networks. By these means, our framework can extract features directly and efficiently from the entire fMRI and be aware of implicit inter-individual differences. We have evaluated our framework on the ABIDE-I dataset with 10-fold cross-validation. The present model has achieved a mean classification accuracy of 85.95\% and a mean AUC of 0.92, which is better than the state-of-the-art methods.      
### 34.Double-RIS Versus Single-RIS Aided Systems: Tensor-Based MIMO Channel Estimation and Design Perspectives  [ :arrow_down: ](https://arxiv.org/pdf/2109.09099.pdf)
>  Reconfigurable intelligent surfaces (RISs) have been proposed recently as new technology to tune the wireless propagation channels in real-time. However, most of the current works assume single-RIS (S-RIS)-aided systems, which can be limited in some application scenarios where a transmitter might need a multi-RIS-aided channel to communicate with a receiver. In this paper, we consider a double-RIS (D-RIS)-aided MIMO system and propose an alternating least-squared-based channel estimation method by exploiting the Tucker2 tensor structure of the received signals. Using the proposed method, the cascaded MIMO channel parts can be estimated separately, up to trivial scaling factors. Compared with the S-RIS systems, we show that if the RIS elements of a S-RIS system are distributed carefully between the two RISs in a D-RIS system, the training overhead can be reduced and the estimation accuracy can also be increased. Therefore, D-RIS systems can be seen as an appealing approach to further increase the coverage, capacity, and efficiency of future wireless networks compared to S-RIS systems.      
### 35.Relaxation of PLMI in the Form of Double Sum  [ :arrow_down: ](https://arxiv.org/pdf/2109.09088.pdf)
>  This letter studies less conservative conditions for a parameterized linear matrix inequality (PLMI) in the form of double convex sum. Without any slack variables, sufficient linear matrix inequalities (LMIs) for the PLMI are derived by using the proposed sum relaxation based on Young's inequality. It is proved that the derived LMIs are not conservative than those of Tuan et al in 2001. An example is given to show the reduced conservatism of the derived LMIs.      
### 36.Simple and Efficient Unpaired Real-world Super-Resolution using Image Statistics  [ :arrow_down: ](https://arxiv.org/pdf/2109.09071.pdf)
>  Learning super-resolution (SR) network without the paired low resolution (LR) and high resolution (HR) image is difficult because direct supervision through the corresponding HR counterpart is unavailable. Recently, many real-world SR researches take advantage of the unpaired image-to-image translation technique. That is, they used two or more generative adversarial networks (GANs), each of which translates images from one domain to another domain, \eg, translates images from the HR domain to the LR domain. However, it is not easy to stably learn such a translation with GANs using unpaired data. In this study, we present a simple and efficient method of training of real-world SR network. To stably train the network, we use statistics of an image patch, such as means and variances. Our real-world SR framework consists of two GANs, one for translating HR images to LR images (degradation task) and the other for translating LR to HR (SR task). We argue that the unpaired image translation using GANs can be learned efficiently with our proposed data sampling strategy, namely, variance matching. We test our method on the NTIRE 2020 real-world SR dataset. Our method outperforms the current state-of-the-art method in terms of the SSIM metric as well as produces comparable results on the LPIPS metric.      
### 37.Channel Estimation in MIMO Systems with One-bit Spatial Sigma-delta ADCs  [ :arrow_down: ](https://arxiv.org/pdf/2109.09068.pdf)
>  This paper focuses on channel estimation in single-user and multi-user MIMO systems with multi-antenna base stations equipped with 1-bit spatial sigma-delta analog-to-digital converters (ADCs). A careful selection of the quantization voltage level and phase shift used in the feedback loop of 1-bit sigma-delta ADCs is critical to improve its effective resolution. We first develop a quantization noise model for 1-bit spatial sigma-delta ADCs. Using the developed noise model, we then present a two-step channel estimation algorithm to estimate a multipath channel parameterized by the gains, angles of arrival (AoAs), and angles of departure (AoDs). Specifically, in the first step, the AoAs and path gains are estimated using uplink pilots, which excite all the angles uniformly. Next, in the second step, the AoDs are estimated by progressively refining uplink beams through a recursive bisection procedure. For this algorithm, we propose a technique to select the quantization voltage level and phase shift. Through numerical simulations, we demonstrate that with the proposed parametric channel estimation algorithm, MIMO systems with 1-bit spatial sigma-delta ADCs perform significantly better than those with regular 1-bit ADCs and are on par with MIMO systems with high-resolution ADCs.      
### 38.Random Multi-Channel Image Synthesis for Multiplexed Immunofluorescence Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2109.09004.pdf)
>  Multiplex immunofluorescence (MxIF) is an emerging imaging technique that produces the high sensitivity and specificity of single-cell mapping. With a tenet of 'seeing is believing', MxIF enables iterative staining and imaging extensive antibodies, which provides comprehensive biomarkers to segment and group different cells on a single tissue section. However, considerable depletion of the scarce tissue is inevitable from extensive rounds of staining and bleaching ('missing tissue'). Moreover, the immunofluorescence (IF) imaging can globally fail for particular rounds ('missing stain''). In this work, we focus on the 'missing stain' issue. It would be appealing to develop digital image synthesis approaches to restore missing stain images without losing more tissue physically. Herein, we aim to develop image synthesis approaches for eleven MxIF structural molecular markers (i.e., epithelial and stromal) on real samples. We propose a novel multi-channel high-resolution image synthesis approach, called pixN2N-HD, to tackle possible missing stain scenarios via a high-resolution generative adversarial network (GAN). Our contribution is three-fold: (1) a single deep network framework is proposed to tackle missing stain in MxIF; (2) the proposed 'N-to-N' strategy reduces theoretical four years of computational time to 20 hours when covering all possible missing stains scenarios, with up to five missing stains (e.g., '(N-1)-to-1', '(N-2)-to-2'); and (3) this work is the first comprehensive experimental study of investigating cross-stain synthesis in MxIF. Our results elucidate a promising direction of advancing MxIF imaging with deep image synthesis.      
### 39.Data-driven yaw misalignment correction for utility-scale wind turbines  [ :arrow_down: ](https://arxiv.org/pdf/2109.08998.pdf)
>  In recent years, wind turbine yaw misalignment that tends to degrade the turbine power production and impact the blade fatigue loads raises more attention along with the rapid development of large-scale wind turbines. The state-of-the-art correction methods require additional instruments such as LiDAR to provide the ground truths and are not suitable for long-term operation and large-scale implementation due to the high costs. In the present study, we propose a framework that enables the effective and efficient detection and correction of static and dynamic yaw errors by using only turbine SCADA data, suitable for a low-cost regular inspection for large-scale wind farms in onshore, coastal, and offshore sites. This framework includes a short-period data collection of the turbine operating under multiple static yaw errors, a data mining correction for the static yaw error, and ultra-short-term dynamic yaw error forecasts with machine learning algorithms. Three regression algorithms, i.e., linear, support vector machine, and random forest, and a hybrid model based on the average prediction of the three, have been tested for dynamic yaw error prediction and compared using the field measurement data from a 2.5 MW turbine. For the data collected in the present study, the hybrid method shows the best performance and can reduce total yaw error by up to 85% (on average of 71%) compared to the cases without static and dynamic yaw error corrections. In addition, we have tested the transferability of the proposed method in the application of detecting other static and dynamic yaw errors.      
### 40.First Demonstration of Korean eLoran Accuracy in a Narrow Waterway using Improved ASF Maps  [ :arrow_down: ](https://arxiv.org/pdf/2109.08990.pdf)
>  The vulnerabilities of global navigation satellite systems (GNSSs) to radio frequency jamming and spoofing have attracted significant research attention. In particular, the large-scale jamming incidents that occurred in South Korea substantiate the practical importance of implementing a complementary navigation system. This letter briefly summarizes the efforts of South Korea to deploy an enhanced long-range navigation (eLoran) system, which is a terrestrial low-frequency radio navigation system that can complement GNSSs. After four years of research and development, the Korean eLoran testbed system has been recently deployed and is operational since June 1, 2021. Although its initial performance at sea is satisfactory, navigation through a narrow waterway is still challenging because a complete survey of the additional secondary factor (ASF), which is the largest source of error for eLoran, is practically difficult in a narrow waterway. This letter proposes an alternative way to survey the ASF in a narrow waterway and improve the ASF map generation methods. Moreover, the performance of the proposed approach was validated experimentally.      
### 41.Human Recognition based on Retinal Bifurcations and Modified Correlation Function  [ :arrow_down: ](https://arxiv.org/pdf/2109.08977.pdf)
>  Nowadays high security is an important issue for most of the secure places and recent advances increase the needs of high-security systems. Therefore, needs to high security for controlling and permitting the allowable people to enter the high secure places, increases and extends the use of conventional recognition methods. Therefore, a novel identification method using retinal images is proposed in this paper. For this purpose, new mathematical functions are applied on corners and bifurcations. To evaluate the proposed method we use 40 retinal images from the DRIVE database, 20 normal retinal image from STARE database and 140 normal retinal images from local collected database and the accuracy rate is 99.34 percent.      
### 42.iWave3D: End-to-end Brain Image Compression with Trainable 3-D Wavelet Transform  [ :arrow_down: ](https://arxiv.org/pdf/2109.08942.pdf)
>  With the rapid development of whole brain imaging technology, a large number of brain images have been produced, which puts forward a great demand for efficient brain image compression methods. At present, the most commonly used compression methods are all based on 3-D wavelet transform, such as JP3D. However, traditional 3-D wavelet transforms are designed manually with certain assumptions on the signal, but brain images are not as ideal as assumed. What's more, they are not directly optimized for compression task. In order to solve these problems, we propose a trainable 3-D wavelet transform based on the lifting scheme, in which the predict and update steps are replaced by 3-D convolutional neural networks. Then the proposed transform is embedded into an end-to-end compression scheme called iWave3D, which is trained with a large amount of brain images to directly minimize the rate-distortion loss. Experimental results demonstrate that our method outperforms JP3D significantly by 2.012 dB in terms of average BD-PSNR.      
### 43.KNN Learning Techniques for Proportional Myocontrol in Prosthetics  [ :arrow_down: ](https://arxiv.org/pdf/2109.08917.pdf)
>  This work has been conducted in the context of pattern-recognition-based control for electromyographic prostheses. It presents a k-nearest neighbour (kNN) classification technique for gesture recognition, extended by a proportionality scheme. The methods proposed are practically implemented and validated. Datasets are captured by means of a state-of-the-art 8-channel electromyography (EMG) armband positioned on the forearm. Based on this data, the influence of kNN's parameters is analyzed in pilot experiments. Moreover, the effect of proportionality scaling and rest thresholding schemes is investigated. A randomized, double-blind user study is conducted to compare the implemented method with the state-of-research algorithm Ridge Regression with Random Fourier Features (RR-RFF) for different levels of gesture exertion. The results from these experiments show a statistically significant improvement in favour of the kNN-based algorithm.      
### 44.Underwater Image Enhancement Using Convolutional Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2109.08916.pdf)
>  This work proposes a method for underwater image enhancement using the principle of histogram equalization. Since underwater images have a global strong dominant colour, their colourfulness and contrast are often degraded. Before applying the histogram equalisation technique on the image, the image is converted from coloured image to a gray scale image for further operations. Histogram equalization is a technique for adjusting image intensities to enhance contrast. The colours of the image are retained using a convolutional neural network model which is trained by the datasets of underwater images to give better results.      
### 45.FastHyMix: Fast and Parameter-free Hyperspectral Image Mixed Noise Removal  [ :arrow_down: ](https://arxiv.org/pdf/2109.08879.pdf)
>  Hyperspectral imaging with high spectral resolution plays an important role in finding objects, identifying materials, or detecting processes. The decrease of the widths of spectral bands leads to a decrease in the signal-to-noise ratio (SNR) of measurements. The decreased SNR reduces the reliability of measured features or information extracted from HSIs. Furthermore, the image degradations linked with various mechanisms also result in different types of noise, such as Gaussian noise, impulse noise, deadlines, and stripes. This paper introduces a fast and parameter-free hyperspectral image mixed noise removal method (termed FastHyMix), which characterizes the complex distribution of mixed noise by using a Gaussian mixture model and exploits two main characteristics of hyperspectral data, namely low-rankness in the spectral domain and high correlation in the spatial domain. The Gaussian mixture model enables us to make a good estimation of Gaussian noise intensity and the location of sparse noise. The proposed method takes advantage of the low-rankness using subspace representation and the spatial correlation of HSIs by adding a powerful deep image prior, which is extracted from a neural denoising network. An exhaustive array of experiments and comparisons with state-of-the-art denoisers were carried out. The experimental results show significant improvement in both synthetic and real datasets. A MATLAB demo of this work will be available at <a class="link-external link-https" href="https://github.com/LinaZhuang" rel="external noopener nofollow">this https URL</a> for the sake of reproducibility.      
### 46.Physical Layer Anonymous Precoding: The Path to Privacy-Preserving Communications  [ :arrow_down: ](https://arxiv.org/pdf/2109.08876.pdf)
>  Next-generation systems aim to increase both the speed and responsiveness of wireless communications, while supporting compelling applications such as edge and cloud computing, remote-Health, vehicle-to-infrastructure communications, etc. As these applications are expected to carry confidential personal data, ensuring user privacy becomes a critical issue. In contrast to traditional security and privacy designs that aim to prevent confidential information from being eavesdropped upon by adversaries, or learned by unauthorized parties, in this paper we consider designs that mask the users' identities during communication, hence resulting in anonymous communications. In particular, we examine the recent interest in physical layer (PHY) anonymous solutions. This line of research departs from conventional higher layer anonymous authentication, encryption and routing protocols, and judiciously manipulates the signaling pattern of transmitted signals in order to mask the senders' PHY characteristics. We first discuss the concept of anonymity at the PHY, and illustrate a strategy that is able to unmask the sender's identity by analyzing his or her PHY information only, i.e., signalling patterns and the inherent fading characteristics. Subsequently, we overview the emerging area of anonymous precoding to preserve the sender's anonymity, while ensuring high receiver-side signal-to-interference-plus-noise ratio (SINR) for communication. This family of anonymous precoding designs represents a new approach to providing anonymity at the PHY, introducing a new dimension for privacy-preserving techniques.      
### 47.Fast query-by-example speech search using separable model  [ :arrow_down: ](https://arxiv.org/pdf/2109.08870.pdf)
>  Traditional Query-by-Example (QbE) speech search approaches usually use methods based on frame-level features, while state-of-the-art approaches tend to use models based on acoustic word embeddings (AWEs) to transform variable length audio signals into fixed length feature vector representations. However, these approaches cannot meet the requirements of the search quality as well as speed at the same time. In this paper, we propose a novel fast QbE speech search method based on separable models to fix this problem. First, a QbE speech search training framework is introduced. Second, we design a novel model inference scheme based on RepVGG which can efficiently improve the QbE search quality. Third, we modify and improve our QbE speech search model according to the proposed model inference scheme. Experiments on keywords dataset shows that our proposed method can improve the GPU Real-time Factor (RTF) from 1/150 to 1/2300 by just applying separable model scheme and outperforms other state-of-the-art methods.      
### 48.A survey on deep learning approaches for breast cancer diagnosis  [ :arrow_down: ](https://arxiv.org/pdf/2109.08853.pdf)
>  Deep learning has introduced several learning-based methods to recognize breast tumours and presents high applicability in breast cancer diagnostics. It has presented itself as a practical installment in Computer-Aided Diagnostic (CAD) systems to further assist radiologists in diagnostics for different modalities. A deep learning network trained on images provided by hospitals or public databases can perform classification, detection, and segmentation of lesion types. Significant progress has been made in recognizing tumours on 2D images but recognizing 3D images remains a frontier so far. The interconnection of deep learning networks between different fields of study help propels discoveries for more efficient, accurate, and robust networks. In this review paper, the following topics will be explored: (i) theory and application of deep learning, (ii) progress of 2D, 2.5D, and 3D CNN approaches in breast tumour recognition from a performance metric perspective, and (iii) challenges faced in CNN approaches.      
### 49.Domain Composition and Attention for Unseen-Domain Generalizable Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.08852.pdf)
>  Domain generalizable model is attracting increasing attention in medical image analysis since data is commonly acquired from different institutes with various imaging protocols and scanners. To tackle this challenging domain generalization problem, we propose a Domain Composition and Attention-based network (DCA-Net) to improve the ability of domain representation and generalization. First, we present a domain composition method that represents one certain domain by a linear combination of a set of basis representations (i.e., a representation bank). Second, a novel plug-and-play parallel domain preceptor is proposed to learn these basis representations and we introduce a divergence constraint function to encourage the basis representations to be as divergent as possible. Then, a domain attention module is proposed to learn the linear combination coefficients of the basis representations. The result of linear combination is used to calibrate the feature maps of an input image, which enables the model to generalize to different and even unseen domains. We validate our method on public prostate MRI dataset acquired from six different institutions with apparent domain shift. Experimental results show that our proposed model can generalize well on different and even unseen domains and it outperforms state-of-the-art methods on the multi-domain prostate segmentation task.      
### 50.The Report on China-Spain Joint Clinical Testing for Rapid COVID-19 Risk Screening by Eye-region Manifestations  [ :arrow_down: ](https://arxiv.org/pdf/2109.08807.pdf)
>  Background: The worldwide surge in coronavirus cases has led to the COVID-19 testing demand surge. Rapid, accurate, and cost-effective COVID-19 screening tests working at a population level are in imperative demand globally. <br>Methods: Based on the eye symptoms of COVID-19, we developed and tested a COVID-19 rapid prescreening model using the eye-region images captured in China and Spain with cellphone cameras. The convolutional neural networks (CNNs)-based model was trained on these eye images to complete binary classification task of identifying the COVID-19 cases. The performance was measured using area under receiver-operating-characteristic curve (AUC), sensitivity, specificity, accuracy, and F1. The application programming interface was open access. <br>Findings: The multicenter study included 2436 pictures corresponding to 657 subjects (155 COVID-19 infection, 23.6%) in development dataset (train and validation) and 2138 pictures corresponding to 478 subjects (64 COVID-19 infections, 13.4%) in test dataset. The image-level performance of COVID-19 prescreening model in the China-Spain multicenter study achieved an AUC of 0.913 (95% CI, 0.898-0.927), with a sensitivity of 0.695 (95% CI, 0.643-0.748), a specificity of 0.904 (95% CI, 0.891 -0.919), an accuracy of 0.875(0.861-0.889), and a F1 of 0.611(0.568-0.655). <br>Interpretation: The CNN-based model for COVID-19 rapid prescreening has reliable specificity and sensitivity. This system provides a low-cost, fully self-performed, non-invasive, real-time feedback solution for continuous surveillance and large-scale rapid prescreening for COVID-19. <br>Funding: This project is supported by Aimomics (Shanghai) Intelligent      
### 51.Small Lesion Segmentation in Brain MRIs with Subpixel Embedding  [ :arrow_down: ](https://arxiv.org/pdf/2109.08791.pdf)
>  We present a method to segment MRI scans of the human brain into ischemic stroke lesion and normal tissues. We propose a neural network architecture in the form of a standard encoder-decoder where predictions are guided by a spatial expansion embedding network. Our embedding network learns features that can resolve detailed structures in the brain without the need for high-resolution training images, which are often unavailable and expensive to acquire. Alternatively, the encoder-decoder learns global structures by means of striding and max pooling. Our embedding network complements the encoder-decoder architecture by guiding the decoder with fine-grained details lost to spatial downsampling during the encoder stage. Unlike previous works, our decoder outputs at 2 times the input resolution, where a single pixel in the input resolution is predicted by four neighboring subpixels in our output. To obtain the output at the original scale, we propose a learnable downsampler (as opposed to hand-crafted ones e.g. bilinear) that combines subpixel predictions. Our approach improves the baseline architecture by approximately 11.7% and achieves the state of the art on the ATLAS public benchmark dataset with a smaller memory footprint and faster runtime than the best competing method. Our source code has been made available at: <a class="link-external link-https" href="https://github.com/alexklwong/subpixel-embedding-segmentation" rel="external noopener nofollow">this https URL</a>.      
### 52.Minimum-fuel Spacecraft Rendezvous based on Sparsity Promoting Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2109.08781.pdf)
>  In this paper, we consider the classical spacecraft rendezvous problem in which the so-called active spacecraft has to approach the target spacecraft which is moving in an elliptical orbit around a planet by using the minimum possible amount of fuel. Instead of using standard convex optimization tools which can be computationally expensive, we use modified versions of the Iteratively Reweighted Least Squares (IRLS) algorithm from compressive sensing to compute sparse optimal control sequences which minimize the fuel consumption for both thrust vectoring and orthogonal vectoring (active) spacecraft. Numerical simulations are performed to verify the efficacy of our approach.      
### 53.Locally Weighted Mean Phase Angle (LWMPA) Based Tone Mapping Quality Index (TMQI-3)  [ :arrow_down: ](https://arxiv.org/pdf/2109.08774.pdf)
>  High Dynamic Range (HDR) images are the ones that contain a greater range of luminosity as compared to the standard images. HDR images have a higher detail and clarity of structure, objects, and color, which the standard images lack. HDR images are useful in capturing scenes that pose high brightness, darker areas, and shadows, etc. An HDR image comprises multiple narrow-range-exposure images combined into one high-quality image. As these HDR images cannot be displayed on standard display devices, the real challenge comes while converting these HDR images to Low dynamic range (LDR) images. The conversion of HDR image to LDR image is performed using Tone-mapped operators (TMOs). This conversion results in the loss of much valuable information in structure, color, naturalness, and exposures. The loss of information in the LDR image may not directly be visible to the human eye. To calculate how good an LDR image is after conversion, various metrics have been proposed previously. Some are not noise resilient, some work on separate color channels (Red, Green, and Blue one by one), and some lack capacity to identify the structure. To deal with this problem, we propose a metric in this paper called the Tone Mapping Quality Index (TMQI-3), which evaluates the quality of the LDR image based on its objective score. TMQI-3 is noise resilient, takes account of structure and naturalness, and works on all three color channels combined into one luminosity component. This eliminates the need to use multiple metrics at the same time. We compute results for several HDR and LDR images from the literature and show that our quality index metric performs better than the baseline models.      
### 54.Dual-Encoder Architecture with Encoder Selection for Joint Close-Talk and Far-Talk Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.08744.pdf)
>  In this paper, we propose a dual-encoder ASR architecture for joint modeling of close-talk (CT) and far-talk (FT) speech, in order to combine the advantages of CT and FT devices for better accuracy. The key idea is to add an encoder selection network to choose the optimal input source (CT or FT) and the corresponding encoder. We use a single-channel encoder for CT speech and a multi-channel encoder with Spatial Filtering neural beamforming for FT speech, which are jointly trained with the encoder selection. We validate our approach on both attention-based and RNN Transducer end-to-end ASR systems. The experiments are done with conversational speech from a medical use case, which is recorded simultaneously with a CT device and a microphone array. Our results show that the proposed dual-encoder architecture obtains up to 9% relative WER reduction when using both CT and FT input, compared to the best single-encoder system trained and tested in matched condition.      
### 55.Hodgelets: Localized Spectral Representations of Flows on Simplicial Complexes  [ :arrow_down: ](https://arxiv.org/pdf/2109.08728.pdf)
>  We develop wavelet representations for edge-flows on simplicial complexes, using ideas rooted in combinatorial Hodge theory and spectral graph wavelets. We first show that the Hodge Laplacian can be used in lieu of the graph Laplacian to construct a family of wavelets for higher-order signals on simplicial complexes. Then, we refine this idea to construct wavelets that respect the Hodge-Helmholtz decomposition. For these Hodgelets, familiar notions of curl-free and divergence-free flows from vector calculus are preserved. We characterize the representational quality of our Hodgelets for edge flows in terms of frame bounds and demonstrate the use of these spectral wavelets for sparse representation of edge flows on real and synthetic data.      
### 56.ChipQA: No-Reference Video Quality Prediction via Space-Time Chips  [ :arrow_down: ](https://arxiv.org/pdf/2109.08726.pdf)
>  We propose a new model for no-reference video quality assessment (VQA). Our approach uses a new idea of highly-localized space-time (ST) slices called Space-Time Chips (ST Chips). ST Chips are localized cuts of video data along directions that \textit{implicitly} capture motion. We use perceptually-motivated bandpass and normalization models to first process the video data, and then select oriented ST Chips based on how closely they fit parametric models of natural video statistics. We show that the parameters that describe these statistics can be used to reliably predict the quality of videos, without the need for a reference video. The proposed method implicitly models ST video naturalness, and deviations from naturalness. We train and test our model on several large VQA databases, and show that our model achieves state-of-the-art performance at reduced cost, without requiring motion computation.      
### 57.Experimental Evaluation of Computational Complexity for Different Neural Network Equalizers in Optical Communications  [ :arrow_down: ](https://arxiv.org/pdf/2109.08711.pdf)
>  Addressing the neural network-based optical channel equalizers, we quantify the trade-off between their performance and complexity by carrying out the comparative analysis of several neural network architectures, presenting the results for TWC and SSMF set-ups.      
### 58.On-device neural speech synthesis  [ :arrow_down: ](https://arxiv.org/pdf/2109.08710.pdf)
>  Recent advances in text-to-speech (TTS) synthesis, such as Tacotron and WaveRNN, have made it possible to construct a fully neural network based TTS system, by coupling the two components together. Such a system is conceptually simple as it only takes grapheme or phoneme input, uses Mel-spectrogram as an intermediate feature, and directly generates speech samples. The system achieves quality equal or close to natural speech. However, the high computational cost of the system and issues with robustness have limited their usage in real-world speech synthesis applications and products. In this paper, we present key modeling improvements and optimization strategies that enable deploying these models, not only on GPU servers, but also on mobile devices. The proposed system can generate high-quality 24 kHz speech at 5x faster than real time on server and 3x faster than real time on mobile devices.      
### 59.Segmentation of Brain MRI using an Altruistic Harris Hawks' Optimization algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2109.08688.pdf)
>  Segmentation is an essential requirement in medicine when digital images are used in illness diagnosis, especially, in posterior tasks as analysis and disease identification. An efficient segmentation of brain Magnetic Resonance Images (MRIs) is of prime concern to radiologists due to their poor illumination and other conditions related to de acquisition of the images. Thresholding is a popular method for segmentation that uses the histogram of an image to label different homogeneous groups of pixels into different classes. However, the computational cost increases exponentially according to the number of thresholds. In this paper, we perform the multi-level thresholding using an evolutionary metaheuristic. It is an improved version of the Harris Hawks Optimization (HHO) algorithm that combines the chaotic initialization and the concept of altruism. Further, for fitness assignment, we use a hybrid objective function where along with the cross-entropy minimization, we apply a new entropy function, and leverage weights to the two objective functions to form a new hybrid approach. The HHO was originally designed to solve numerical optimization problems. Earlier, the statistical results and comparisons have demonstrated that the HHO provides very promising results compared with well-established metaheuristic techniques. In this article, the altruism has been incorporated into the HHO algorithm to enhance its exploitation capabilities. We evaluate the proposed method over 10 benchmark images from the WBA database of the Harvard Medical School and 8 benchmark images from the Brainweb dataset using some standard evaluation metrics.      
### 60.Self-supervised learning methods and applications in medical imaging analysis: A survey  [ :arrow_down: ](https://arxiv.org/pdf/2109.08685.pdf)
>  The availability of high quality annotated medical imaging datasets is a major problem that collides with machine learning applications in the field of medical imaging analysis and impedes its advancement. Self-supervised learning is a recent training paradigm that enables learning robust representations without the need for human annotation which can be considered as an effective solution for the scarcity in annotated medical data. This article reviews the state-of-the-art research directions in self-supervised learning approaches for image data with concentration on their applications in the field of medical imaging analysis. The article covers a set of the most recent self-supervised learning methods from the computer vision field as they are applicable to the medical imaging analysis and categorize them as predictive, generative and contrastive approaches. Moreover, the article covers (40) of the most recent researches in the field of self-supervised learning in medical imaging analysis aiming at shedding the light on the recent innovation in the field. Ultimately, the article concludes with possible future research directions in the field.      
### 61.Asymmetric 3D Context Fusion for Universal Lesion Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.08684.pdf)
>  Modeling 3D context is essential for high-performance 3D medical image analysis. Although 2D networks benefit from large-scale 2D supervised pretraining, it is weak in capturing 3D context. 3D networks are strong in 3D context yet lack supervised pretraining. As an emerging technique, \emph{3D context fusion operator}, which enables conversion from 2D pretrained networks, leverages the advantages of both and has achieved great success. Existing 3D context fusion operators are designed to be spatially symmetric, i.e., performing identical operations on each 2D slice like convolutions. However, these operators are not truly equivariant to translation, especially when only a few 3D slices are used as inputs. In this paper, we propose a novel asymmetric 3D context fusion operator (A3D), which uses different weights to fuse 3D context from different 2D slices. Notably, A3D is NOT translation-equivariant while it significantly outperforms existing symmetric context fusion operators without introducing large computational overhead. We validate the effectiveness of the proposed method by extensive experiments on DeepLesion benchmark, a large-scale public dataset for universal lesion detection from computed tomography (CT). The proposed A3D consistently outperforms symmetric context fusion operators by considerable margins, and establishes a new \emph{state of the art} on DeepLesion. To facilitate open research, our code and model in PyTorch are available at <a class="link-external link-https" href="https://github.com/M3DV/AlignShift" rel="external noopener nofollow">this https URL</a>.      
### 62.BFAR-Bounded False Alarm Rate detector for improved radar odometry estimation  [ :arrow_down: ](https://arxiv.org/pdf/2109.09669.pdf)
>  This paper presents a new detector for filtering noise from true detections in radar data, which improves the state of the art in radar odometry. Scanning Frequency-Modulated Continuous Wave (FMCW) radars can be useful for localization and mapping in low visibility, but return a lot of noise compared to (more commonly used) lidar, which makes the detection task more challenging. Our Bounded False-Alarm Rate (BFAR) detector is different from the classical Constant False-Alarm Rate (CFAR) detector in that it applies an affine transformation on the estimated noise level after which the parameters that minimize the estimation error can be learned. BFAR is an optimized combination between CFAR and fixed-level thresholding. Only a single parameter needs to be learned from a training dataset. We apply BFAR to the use case of radar odometry, and adapt a state-of-the-art odometry pipeline (CFEAR), replacing its original conservative filtering with BFAR. In this way we reduce the state-of-the-art translation/rotation odometry errors from 1.76%/0.5deg/100 m to 1.55%/0.46deg/100 m; an improvement of 12.5%.      
### 63.Predicting vehicles parking behaviour in shared premises for aggregated EV electricity demand response programs  [ :arrow_down: ](https://arxiv.org/pdf/2109.09666.pdf)
>  The global electric car sales in 2020 continued to exceed the expectations climbing to over 3 millions and reaching a market share of over 4%. However, uncertainty of generation caused by higher penetration of renewable energies and the advent of Electrical Vehicles (EV) with their additional electricity demand could cause strains to the power system, both at distribution and transmission levels. Demand response aggregation and load control will enable greater grid stability and greater penetration of renewable energies into the grid. The present work fits this context in supporting charging optimization for EV in parking premises assuming a incumbent high penetration of EVs in the system. We propose a methodology to predict an estimation of the parking duration in shared parking premises with the objective of estimating the energy requirement of a specific parking lot, evaluate optimal EVs charging schedule and integrate the scheduling into a smart controller. We formalize the prediction problem as a supervised machine learning task to predict the duration of the parking event before the car leaves the slot. This predicted duration feeds the energy management system that will allocate the power over the duration reducing the overall peak electricity demand. We structure our experiments inspired by two research questions aiming to discover the accuracy of the proposed machine learning approach and the most relevant features for the prediction models. We experiment different algorithms and features combination for 4 datasets from 2 different campus facilities in Italy and Brazil. Using both contextual and time of the day features, the overall results of the models shows an higher accuracy compared to a statistical analysis based on frequency, indicating a viable route for the development of accurate predictors for sharing parking premises energy management systems      
### 64.Stability Analysis of Nonlinear Inviscid Microscopic and Macroscopic Traffic Flow Models of Bidirectional Cruise-Controlled Vehicles  [ :arrow_down: ](https://arxiv.org/pdf/2109.09622.pdf)
>  The paper introduces a new bidirectional microscopic inviscid Adaptive Cruise Control (ACC) model that uses only spacing information from the preceding and following vehicles in order to select the proper control action to avoid collisions and maintain a desired speed. KL estimates that guarantee uniform convergence of the ACC model to the set of equilibria are provided. Moreover, the corresponding macroscopic model is derived, consisting of a conservation equation and a momentum equation that contains a nonlinear relaxation term. It is shown that, if the density is sufficiently small, then the macroscopic model has a solution that approaches exponentially the equilibrium speed (in the sup norm) while the density converges exponentially to a traveling wave. Numerical simulations are also provided, illustrating the properties of the microscopic and macroscopic inviscid ACC models.      
### 65.TeleMelody: Lyric-to-Melody Generation with a Template-Based Two-Stage Method  [ :arrow_down: ](https://arxiv.org/pdf/2109.09617.pdf)
>  Lyric-to-melody generation is an important task in automatic songwriting. Previous lyric-to-melody generation systems usually adopt end-to-end models that directly generate melodies from lyrics, which suffer from several issues: 1) lack of paired lyric-melody training data; 2) lack of control on generated melodies. In this paper, we develop TeleMelody, a two-stage lyric-to-melody generation system with music template (e.g., tonality, chord progression, rhythm pattern, and cadence) to bridge the gap between lyrics and melodies (i.e., the system consists of a lyric-to-template module and a template-to-melody module). TeleMelody has two advantages. First, it is data efficient. The template-to-melody module is trained in a self-supervised way (i.e., the source template is extracted from the target melody) that does not need any lyric-melody paired data. The lyric-to-template module is made up of some rules and a lyric-to-rhythm model, which is trained with paired lyric-rhythm data that is easier to obtain than paired lyric-melody data. Second, it is controllable. The design of template ensures that the generated melodies can be controlled by adjusting the musical elements in template. Both subjective and objective experimental evaluations demonstrate that TeleMelody generates melodies with higher quality, better controllability, and less requirement on paired lyric-melody data than previous generation systems.      
### 66."Hello, It's Me": Deep Learning-based Speech Synthesis Attacks in the Real World  [ :arrow_down: ](https://arxiv.org/pdf/2109.09598.pdf)
>  Advances in deep learning have introduced a new wave of voice synthesis tools, capable of producing audio that sounds as if spoken by a target speaker. If successful, such tools in the wrong hands will enable a range of powerful attacks against both humans and software systems (aka machines). This paper documents efforts and findings from a comprehensive experimental study on the impact of deep-learning based speech synthesis attacks on both human listeners and machines such as speaker recognition and voice-signin systems. We find that both humans and machines can be reliably fooled by synthetic speech and that existing defenses against synthesized speech fall short. These findings highlight the need to raise awareness and develop new protections against synthetic speech for both humans and machines.      
### 67.Contrastive Learning of Subject-Invariant EEG Representations for Cross-Subject Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.09559.pdf)
>  Emotion recognition plays a vital role in human-machine interactions and daily healthcare. EEG signals have been reported to be informative and reliable for emotion recognition in recent years. However, the inter-subject variability of emotion-related EEG signals poses a great challenge for the practical use of EEG-based emotion recognition. Inspired by the recent neuroscience studies on inter-subject correlation, we proposed a Contrastive Learning method for Inter-Subject Alignment (CLISA) for reliable cross-subject emotion recognition. Contrastive learning was employed to minimize the inter-subject differences by maximizing the similarity in EEG signals across subjects when they received the same stimuli in contrast to different ones. Specifically, a convolutional neural network with depthwise spatial convolution and temporal convolution layers was applied to learn inter-subject aligned spatiotemporal representations from raw EEG signals. Then the aligned representations were used to extract differential entropy features for emotion classification. The performance of the proposed method was evaluated on our THU-EP dataset with 80 subjects and the publicly available SEED dataset with 15 subjects. Comparable or better cross-subject emotion recognition accuracy (i.e., 72.1% and 47.0% for binary and nine-class classification, respectively, on the THU-EP dataset and 86.3% on the SEED dataset for three-class classification) was achieved as compared to the state-of-the-art methods. The proposed method could be generalized well to unseen emotional stimuli as well. The CLISA method is therefore expected to considerably increase the practicality of EEG-based emotion recognition by operating in a "plug-and-play" manner. Furthermore, the learned spatiotemporal representations by CLISA could provide insights into the neural mechanisms of human emotion processing.      
### 68.Deep Reinforcement Learning Based Multidimensional Resource Management for Energy Harvesting Cognitive NOMA Communications  [ :arrow_down: ](https://arxiv.org/pdf/2109.09503.pdf)
>  The combination of energy harvesting (EH), cognitive radio (CR), and non-orthogonal multiple access (NOMA) is a promising solution to improve energy efficiency and spectral efficiency of the upcoming beyond fifth generation network (B5G), especially for support the wireless sensor communications in Internet of things (IoT) system. However, how to realize intelligent frequency, time, and energy resource allocation to support better performances is an important problem to be solved. In this paper, we study joint spectrum, energy, and time resource management for the EH-CR-NOMA IoT systems. Our goal is to minimize the number of data packets losses for all secondary sensing users (SSU), while satisfying the constraints on the maximum charging battery capacity, maximum transmitting power, maximum buffer capacity, and minimum data rate of primary users (PU) and SSUs. Due to the non-convexity of this optimization problem and the stochastic nature of the wireless environment, we propose a distributed multidimensional resource management algorithm based on deep reinforcement learning (DRL). Considering the continuity of the resources to be managed, the deep deterministic policy gradient (DDPG) algorithm is adopted, based on which each agent (SSU) can manage its own multidimensional resources without collaboration. In addition, a simplified but practical action adjuster (AA) is introduced for improving the training efficiency and battery performance protection. The provided results show that the convergence speed of the proposed algorithm is about 4 times faster than that of DDPG, and the average number of packet losses (ANPL) is about 8 times lower than that of the greedy algorithm.      
### 69.Design of Conformal Array of Rectangular Waveguide-fed Metasurfaces  [ :arrow_down: ](https://arxiv.org/pdf/2109.09450.pdf)
>  We present a systematic design method for a cylindrical conformal array of rectangular waveguide-fed metasurfaces. The conformal metasurface consists of multiple curved rectangular waveguides loaded with metamaterial elements\textemdash electrically small irises\textemdash inserted into the upper conducting walls of the waveguides. Each element radiates energy into free space to contribute to an overall radiation pattern. Thus, the geometry or electrical configuration of each of the individual metamaterial elements needs to be tailored to generate a desired pattern. In general, due to difficulties in modeling the effect of curvature, the design of conformal metasurface arrays has relied on full-wave simulations or experiments. In this study, we propose a design method utilizing the analytic model of a planar metasurface accounting for metamaterial elements' locations and orientations over a surface with curvature. Although approximate, we demonstrate that such an alteration along with the framework of dipolar modeling of planar elements can be used for the analysis of conformal arrays with small curvature. We then design a conformal array metasurface using the method combined with CMA-ES optimizer. Through numerical simulations, we confirm the validity of the proposed design method. Applications include the design of metasurfaces for radar, communications, and imaging systems for automobiles and airplanes.      
### 70.Incremental Learning Techniques for Online Human Activity Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.09435.pdf)
>  Unobtrusive and smart recognition of human activities using smartphones inertial sensors is an interesting topic in the field of artificial intelligence acquired tremendous popularity among researchers, especially in recent years. A considerable challenge that needs more attention is the real-time detection of physical activities, since for many real-world applications such as health monitoring and elderly care, it is required to recognize users' activities immediately to prevent severe damages to individuals' wellness. In this paper, we propose a human activity recognition (HAR) approach for the online prediction of physical movements, benefiting from the capabilities of incremental learning algorithms. We develop a HAR system containing monitoring software and a mobile application that collects accelerometer and gyroscope data and send them to a remote server via the Internet for classification and recognition operations. Six incremental learning algorithms are employed and evaluated in this work and compared with several batch learning algorithms commonly used for developing offline HAR systems. The Final results indicated that considering all performance evaluation metrics, Incremental K-Nearest Neighbors and Incremental Naive Bayesian outperformed other algorithms, exceeding a recognition accuracy of 95% in real-time.      
### 71.Anomaly Detection in Radar Data Using PointNets  [ :arrow_down: ](https://arxiv.org/pdf/2109.09401.pdf)
>  For autonomous driving, radar is an important sensor type. On the one hand, radar offers a direct measurement of the radial velocity of targets in the environment. On the other hand, in literature, radar sensors are known for their robustness against several kinds of adverse weather conditions. However, on the downside, radar is susceptible to ghost targets or clutter which can be caused by several different causes, e.g., reflective surfaces in the environment. Ghost targets, for instance, can result in erroneous object detections. To this end, it is desirable to identify anomalous targets as early as possible in radar data. In this work, we present an approach based on PointNets to detect anomalous radar targets. Modifying the PointNet-architecture driven by our task, we developed a novel grouping variant which contributes to a multi-form grouping module. Our method is evaluated on a real-world dataset in urban scenarios and shows promising results for the detection of anomalous radar targets.      
### 72.Unsupervised Cycle-consistent Generative Adversarial Networks for Pan-sharpening  [ :arrow_down: ](https://arxiv.org/pdf/2109.09395.pdf)
>  Deep learning based pan-sharpening has received significant research interest in recent years. Most of existing methods fall into the supervised learning framework in which they down-sample the multi-spectral (MS) and panchromatic (PAN) images and regard the original MS images as ground truths to form training samples. Although impressive performance could be achieved, they have difficulties generalizing to the original full-scale images due to the scale gap, which makes them lack of practicability. In this paper, we propose an unsupervised generative adversarial framework that learns from the full-scale images without the ground truths to alleviate this problem. We extract the modality-specific features from the PAN and MS images with a two-stream generator, perform fusion in the feature domain, and then reconstruct the pan-sharpened images. Furthermore, we introduce a novel hybrid loss based on the cycle-consistency and adversarial scheme to improve the performance. Comparison experiments with the state-of-the-art methods are conducted on GaoFen-2 and WorldView-3 satellites. Results demonstrate that the proposed method can greatly improve the pan-sharpening performance on the full-scale images, which clearly show its practical value. Codes and datasets will be made publicly available.      
### 73.Deep Quantile Regression for Uncertainty Estimation in Unsupervised and Supervised Lesion Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.09374.pdf)
>  Despite impressive state-of-the-art performance on a wide variety of machine learning tasks in multiple applications, deep learning methods can produce over-confident predictions, particularly with limited training data. Therefore, quantifying uncertainty is particularly important in critical applications such as anomaly or lesion detection and clinical diagnosis, where a realistic assessment of uncertainty is essential in determining surgical margins, disease status and appropriate treatment. In this work, we focus on using quantile regression to estimate aleatoric uncertainty and use it for estimating uncertainty in both supervised and unsupervised lesion detection problems. In the unsupervised settings, we apply quantile regression to a lesion detection task using Variational AutoEncoder (VAE). The VAE models the output as a conditionally independent Gaussian characterized by means and variances for each output dimension. Unfortunately, joint optimization of both mean and variance in the VAE leads to the well-known problem of shrinkage or underestimation of variance. We describe an alternative VAE model, Quantile-Regression VAE (QR-VAE), that avoids this variance shrinkage problem by estimating conditional quantiles for the given input image. Using the estimated quantiles, we compute the conditional mean and variance for input images under the conditionally Gaussian model. We then compute reconstruction probability using this model as a principled approach to outlier or anomaly detection applications. In the supervised setting, we develop binary quantile regression (BQR) for the supervised lesion segmentation task. BQR segmentation can capture uncertainty in label boundaries. We show how quantile regression can be used to characterize expert disagreement in the location of lesion boundaries.      
### 74.Deep Spatio-temporal Sparse Decomposition for Trend Prediction and Anomaly Detection in Cardiac Electrical Conduction  [ :arrow_down: ](https://arxiv.org/pdf/2109.09317.pdf)
>  Electrical conduction among cardiac tissue is commonly modeled with partial differential equations, i.e., reaction-diffusion equation, where the reaction term describes cellular stimulation and diffusion term describes electrical propagation. Detecting and identifying of cardiac cells that produce abnormal electrical impulses in such nonlinear dynamic systems are important for efficient treatment and planning. To model the nonlinear dynamics, simulation has been widely used in both cardiac research and clinical study to investigate cardiac disease mechanisms and develop new treatment designs. However, existing cardiac models have a great level of complexity, and the simulation is often time-consuming. We propose a deep spatio-temporal sparse decomposition (DSTSD) approach to bypass the time-consuming cardiac partial differential equations with the deep spatio-temporal model and detect the time and location of the anomaly (i.e., malfunctioning cardiac cells). This approach is validated from the data set generated from the Courtemanche-Ramirez-Nattel (CRN) model, which is widely used to model the propagation of the transmembrane potential across the cross neuron membrane. The proposed DSTSD achieved the best accuracy in terms of spatio-temporal mean trend prediction and anomaly detection.      
### 75.Intelligent Reflecting Surfaces and Classical Relays: Coexistence and Co-Design  [ :arrow_down: ](https://arxiv.org/pdf/2109.09267.pdf)
>  This paper investigates a multiuser downlink communication system with coexisting intelligent reflecting surface (IRS) and classical half-duplex decode-and-forward (DF) relay. In this system, the IRS and the DF relay interact with each other and assist transmission simultaneously. In particular, active beamforming at the base station (BS) and at the DF relay, and passive beamforming at the IRS, are jointly designed to maximize the sum-rate of all users. The sum-rate maximization problem is nonconvex due to the coupled beamforming vectors. We propose an alternating optimization (AO) based algorithm to tackle this complex co-design problem. Numerical validation and discussion on the superiority of the coexistence system and the tradeoffs therein are presented.      
### 76.From the Heisenberg to the SchrÃ¶dinger Picture: Quantum Stochastic Processes and Process Tensors  [ :arrow_down: ](https://arxiv.org/pdf/2109.09256.pdf)
>  A general theory of quantum stochastic processes was formulated by Accardi, Frigerio and Lewis in 1982 within the operator-algebraic framework of quantum probability theory, as a non-commutative extension of the Kolmogorovian classical stochastic processes. More recently, studies on non-Markovian quantum processes have led to the discrete-time process tensor formalism in the SchrÃ¶dinger picture to describe the outcomes of sequential interventions on open quantum systems. However, there has been no treatment of the relationship of the process tensor formalism to the quantum probabilistic theory of quantum stochastic processes. This paper gives an exposition of quantum stochastic processes and the process tensor and the relationship between them. In particular, it is shown how the latter emerges from the former via extended correlation kernels incorporating ancillas.      
### 77.ARCA23K: An audio dataset for investigating open-set label noise  [ :arrow_down: ](https://arxiv.org/pdf/2109.09227.pdf)
>  The availability of audio data on sound sharing platforms such as Freesound gives users access to large amounts of annotated audio. Utilising such data for training is becoming increasingly popular, but the problem of label noise that is often prevalent in such datasets requires further investigation. This paper introduces ARCA23K, an Automatically Retrieved and Curated Audio dataset comprised of over 23000 labelled Freesound clips. Unlike past datasets such as FSDKaggle2018 and FSDnoisy18K, ARCA23K facilitates the study of label noise in a more controlled manner. We describe the entire process of creating the dataset such that it is fully reproducible, meaning researchers can extend our work with little effort. We show that the majority of labelling errors in ARCA23K are due to out-of-vocabulary audio clips, and we refer to this type of label noise as open-set label noise. Experiments are carried out in which we study the impact of label noise in terms of classification performance and representation learning.      
### 78.Architecture and Its Vulnerabilities in Smart-Lighting Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.09171.pdf)
>  Industry 4.0 embodies one of the significant technological changes of this decade. Cyber-physical systems and the Internet Of Things are two central technologies in this change that embed or connect with sensors and actuators and interact with the physical environment. However, such systems-of-systems undergo additional restrictions in an endeavor to maintain reliability and security when building and interconnecting components to a heterogeneous, multi-domain \textit{Smart-*} systems architecture. This paper presents an application-specific, layer-based approach to an offline security analysis inspired by design science that merges preceding expertise from relevant domains. With the example of a Smart-lighting system, we create a dedicated unified taxonomy for the use case and analyze its distributed Smart-* architecture by multiple layer-based models. We derive potential attacks from the system specifications in an iterative and incremental process and discuss resulting threats and vulnerabilities. Finally, we suggest immediate countermeasures for the latter potential multiple-domain security concerns.      
### 79.CaTGrasp: Learning Category-Level Task-Relevant Grasping in Clutter from Simulation  [ :arrow_down: ](https://arxiv.org/pdf/2109.09163.pdf)
>  Task-relevant grasping is critical for industrial assembly, where downstream manipulation tasks constrain the set of valid grasps. Learning how to perform this task, however, is challenging, since task-relevant grasp labels are hard to define and annotate. There is also yet no consensus on proper representations for modeling or off-the-shelf tools for performing task-relevant grasps. This work proposes a framework to learn task-relevant grasping for industrial objects without the need of time-consuming real-world data collection or manual annotation. To achieve this, the entire framework is trained solely in simulation, including supervised training with synthetic label generation and self-supervised, hand-object interaction. In the context of this framework, this paper proposes a novel, object-centric canonical representation at the category level, which allows establishing dense correspondence across object instances and transferring task-relevant grasps to novel instances. Extensive experiments on task-relevant grasping of densely-cluttered industrial objects are conducted in both simulation and real-world setups, demonstrating the effectiveness of the proposed framework. Code and data will be released upon acceptance at <a class="link-external link-https" href="https://sites.google.com/view/catgrasp" rel="external noopener nofollow">this https URL</a>.      
### 80.Wav-BERT: Cooperative Acoustic and Linguistic Representation Learning for Low-Resource Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.09161.pdf)
>  Unifying acoustic and linguistic representation learning has become increasingly crucial to transfer the knowledge learned on the abundance of high-resource language data for low-resource speech recognition. Existing approaches simply cascade pre-trained acoustic and language models to learn the transfer from speech to text. However, how to solve the representation discrepancy of speech and text is unexplored, which hinders the utilization of acoustic and linguistic information. Moreover, previous works simply replace the embedding layer of the pre-trained language model with the acoustic features, which may cause the catastrophic forgetting problem. In this work, we introduce Wav-BERT, a cooperative acoustic and linguistic representation learning method to fuse and utilize the contextual information of speech and text. Specifically, we unify a pre-trained acoustic model (wav2vec 2.0) and a language model (BERT) into an end-to-end trainable framework. A Representation Aggregation Module is designed to aggregate acoustic and linguistic representation, and an Embedding Attention Module is introduced to incorporate acoustic information into BERT, which can effectively facilitate the cooperation of two pre-trained models and thus boost the representation learning. Extensive experiments show that our Wav-BERT significantly outperforms the existing approaches and achieves state-of-the-art performance on low-resource speech recognition.      
### 81.Photoacoustic digital brain: numerical modelling and image reconstruction via deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.09127.pdf)
>  Photoacoustic tomography (PAT) is a newly developed medical imaging modality, which combines the advantages of pure optical imaging and ultrasound imaging, owning both high optical contrast and deep penetration depth. Very recently, PAT is studied in human brain imaging. Nevertheless, while ultrasound waves are passing through the human skull tissues, the strong acoustic attenuation and aberration will happen, which causes photoacoustic signals' distortion. In this work, we use 10 magnetic resonance angiography (MRA) human brain volumes, and manually segment them to obtain the 2D human brain numerical phantoms for PAT. The numerical phantoms contain six kinds of tissues which are scalp, skull, white matter, gray matter, blood vessel and cerebral cortex. For every numerical phantom, optical properties are assigned to every kind of tissues. Then, Monte-Carlo based optical simulation is deployed to obtain the photoacoustic initial pressure. Then, we made two k-wave simulation cases: one takes inhomogeneous medium and uneven sound velocity into consideration, and the other not. Then we use the sensor data of the former one as the input of U-net, and the sensor data of the latter one as the output of U-net to train the network. We randomly choose 7 human brain PA sinograms as the training dataset and 3 human brain PA sinograms as the testing set. The testing result shows that our method could correct the skull acoustic aberration and obtain the blood vessel distribution inside the human brain satisfactorily.      
### 82.What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study  [ :arrow_down: ](https://arxiv.org/pdf/2109.09105.pdf)
>  Language Models (LMs) have been ubiquitously leveraged in various tasks including spoken language understanding (SLU). Spoken language requires careful understanding of speaker interactions, dialog states and speech induced multimodal behaviors to generate a meaningful representation of the conversation. In this work, we propose to dissect SLU into three representative properties:conversational (disfluency, pause, overtalk), channel (speaker-type, turn-tasks) and ASR (insertion, deletion,substitution). We probe BERT based language models (BERT, RoBERTa) trained on spoken transcripts to investigate its ability to understand multifarious properties in absence of any speech cues. Empirical results indicate that LM is surprisingly good at capturing conversational properties such as pause prediction and overtalk detection from lexical tokens. On the downsides, the LM scores low on turn-tasks and ASR errors predictions. Additionally, pre-training the LM on spoken transcripts restrain its linguistic understanding. Finally, we establish the efficacy and transferability of the mentioned properties on two benchmark datasets: Switchboard Dialog Act and Disfluency datasets.      
### 83.Generalized PoincarÃ© Orthogonality: A New Approach to POLSAR Data Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2109.09093.pdf)
>  In this paper we outline a new approach to the analysis of polarimetric synthetic aperture (POLSAR) data. Here we exploit target orthogonality as a multi-dimensional extension of wave orthogonality, familiar on the PoincarÃ© sphere. We first show how to formulate a general basis for a complex orthogonal scattering space using a generalization of the PoincarÃ© formulation, and then show how to optimize the backscattered signal in this space for both monostatic and bistatic radar systems. We illustrate application of the new approach, first to ship detection, using data collected off the north-west of Scotland and then land-use applications in a mixed scene around Glasgow, Scotland, both using L-band ALOS-2 POLSAR data.      
### 84.Embedding Model Based Fast Meta Learning for Downlink Beamforming Adaptation  [ :arrow_down: ](https://arxiv.org/pdf/2109.09086.pdf)
>  This paper studies the fast adaptive beamforming for the multiuser multiple-input single-output downlink. Existing deep learning-based approaches assume that training and testing channels follow the same distribution which causes task mismatch, when the testing environment changes. Although meta learning can deal with the task mismatch, it relies on labelled data and incurs high complexity in the pre-training and fine tuning stages. We propose a simple yet effective adaptive framework to solve the mismatch issue, which trains an embedding model as a transferable feature extractor, followed by fitting the support vector regression. Compared to the existing meta learning algorithm, our method does not necessarily need labelled data in the pre-training and does not need fine-tuning of the pre-trained model in the adaptation. The effectiveness of the proposed method is verified through two well-known applications, i.e., the signal to interference plus noise ratio balancing problem and the sum rate maximization problem. Furthermore, we extend our proposed method to online scenarios in non-stationary environments. Simulation results demonstrate the advantages of the proposed algorithm in terms of both performance and complexity. The proposed framework can also be applied to general radio resource management problems.      
### 85.Model-Free Safety-Critical Control for Robotic Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.09047.pdf)
>  This paper presents a framework for the safety-critical control of robotic systems, when safety is defined on safe regions in the configuration space. To maintain safety, we synthesize a safe velocity based on control barrier function theory without relying on a -- potentially complicated -- high-fidelity dynamical model of the robot. Then, we track the safe velocity with a tracking controller. This culminates in model-free safety critical control. We prove theoretical safety guarantees for the proposed method. Finally, we demonstrate that this approach is application-agnostic. We execute an obstacle avoidance task with a Segway in high-fidelity simulation, as well as with a Drone and a Quadruped in hardware experiments.      
### 86.Online Distributed Trajectory Planning for Quadrotor Swarm with Feasibility Guarantee using Linear Safe Corridor  [ :arrow_down: ](https://arxiv.org/pdf/2109.09041.pdf)
>  This paper presents a new online multi-agent trajectory planning algorithm that guarantees to generate safe, dynamically feasible trajectories in a cluttered environment. The proposed algorithm utilizes a linear safe corridor (LSC) to formulate the distributed trajectory optimization problem with only feasible constraints, so it does not resort to slack variables or soft constraints to avoid optimization failure. Also, we adopt a priority-based goal planning method to prevent the deadlock without additional communication for decision making. The proposed algorithm can compute the trajectories for 60 agents on average 15.5 ms per agent with an Intel i7 laptop and can find the trajectory that reaches the goal without deadlock in both random forest and indoor space. We validated safety and operability of the proposed algorithm through a real flight test with ten quadrotors in a maze-like environment.      
### 87.Hybrid Data Augmentation and Deep Attention-based Dilated Convolutional-Recurrent Neural Networks for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.09026.pdf)
>  Speech emotion recognition (SER) has been one of the significant tasks in Human-Computer Interaction (HCI) applications. However, it is hard to choose the optimal features and deal with imbalance labeled data. In this article, we investigate hybrid data augmentation (HDA) methods to generate and balance data based on traditional and generative adversarial networks (GAN) methods. To evaluate the effectiveness of HDA methods, a deep learning framework namely (ADCRNN) is designed by integrating deep dilated convolutional-recurrent neural networks with an attention mechanism. Besides, we choose 3D log Mel-spectrogram (MelSpec) features as the inputs for the deep learning framework. Furthermore, we reconfigure a loss function by combining a softmax loss and a center loss to classify the emotions. For validating our proposed methods, we use the EmoDB dataset that consists of several emotions with imbalanced samples. Experimental results prove that the proposed methods achieve better accuracy than the state-of-the-art methods on the EmoDB with 87.12% and 88.47% for the traditional and GAN-based methods, respectively.      
### 88.Hydroelectric Generation Forecasting with Long Short Term Memory (LSTM) Based Deep Learning Model for Turkey  [ :arrow_down: ](https://arxiv.org/pdf/2109.09013.pdf)
>  Hydroelectricity is one of the renewable energy source, has been used for many years in Turkey. The production of hydraulic power plants based on water reservoirs varies based on different parameters. For this reason, the estimation of hydraulic production gains importance in terms of the planning of electricity generation. In this article, the estimation of Turkey's monthly hydroelectricity production has been made with the long-short-term memory (LSTM) network-based deep learning model. The designed deep learning model is based on hydraulic production time series and future production planning for many years. By using real production data and different LSTM deep learning models, their performance on the monthly forecast of hydraulic electricity generation of the next year has been examined. The obtained results showed that the use of time series based on real production data for many years and deep learning model together is successful in long-term prediction. In the study, it is seen that the 100-layer LSTM model, in which 120 months (10 years) hydroelectric generation time data are used according to the RMSE and MAPE values, are the highest model in terms of estimation accuracy, with a MAPE value of 0.1311 (13.1%) in the annual total and 1.09% as the monthly average distribution. In this model, the best results were obtained for the 100-layer LSTM model, in which the time data of 144 months (12 years) hydroelectric generation data are used, with a RMSE value of 29,689 annually and 2474.08 in monthly distribution. According to the results of the study, time data covering at least 120 months of production is recommended to create an acceptable hydropower forecasting model with LSTM.      
### 89.Atrial Fibrillation: A Medical and Technological Review  [ :arrow_down: ](https://arxiv.org/pdf/2109.08974.pdf)
>  Atrial Fibrillation (AF) is the most common type of arrhythmia (Greek a-, loss + rhythmos, rhythm = loss of rhythm) leading to hospitalization in the United States. Though sometimes AF is asymptomatic, it increases the risk of stroke and heart failure in patients, in addition to lowering the health-related quality of life (HRQOL). AF-related care costs the healthcare system between $6.0 to $26 billion each year. Early detection of AF and clinical attention can help improve symptoms and HRQOL of the patient, as well as bring down the cost of care. However, the prevalent paradigm of AF detection depends on electrocardiogram (ECG) recorded at a single point in time and does not shed light on the relation of the symptoms with heart rhythm or AF. In the recent decade, due to the democratization of health monitors and the advent of high-performing computers, Machine Learning algorithms have been proven effective in identifying AF, from the ECG of patients. This paper provides an overview of the symptoms of AF, its diagnosis, and future prospects for research in the field.      
### 90.Removing Noise from Extracellular Neural Recordings Using Fully Convolutional Denoising Autoencoders  [ :arrow_down: ](https://arxiv.org/pdf/2109.08945.pdf)
>  Extracellular recordings are severely contaminated by a considerable amount of noise sources, rendering the denoising process an extremely challenging task that should be tackled for efficient spike sorting. To this end, we propose an end-to-end deep learning approach to the problem, utilizing a Fully Convolutional Denoising Autoencoder, which learns to produce a clean neuronal activity signal from a noisy multichannel input. The experimental results on simulated data show that our proposed method can improve significantly the quality of noise-corrupted neural signals, outperforming widely-used wavelet denoising techniques.      
### 91.Intelligent Reflecting Surface Aided MIMO with Cascaded Line-of-Sight Links: Channel Modelling and Capacity Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2109.08913.pdf)
>  In this paper, we build up a new intelligent reflecting surface (IRS) aided multiple-input multiple-output (MIMO) channel model, named the cascaded LoS MIMO channel. The proposed channel model consists of a transmitter (Tx) and a receiver (Rx) both equipped with uniform linear arrays (ULAs), and an IRS used to enable communications between the transmitter and the receiver through the line-of-sight (LoS) links seen by the IRS. To model the reflection of electromagnetic waves at the IRS, we take into account the curvature of the wavefront on different reflecting elements (REs), which is distinct from most existing works that take the plane-wave assumption. Based on the established model, we study the spatial multiplexing capability and input-output mutual information (MI) of the cascaded LoS MIMO system. We generalize the notion of Rayleigh distance originally coined for the single-hop MIMO channel to the full multiplexing region (FMR) for the cascaded LoS MIMO channel, where the FMR is, roughly speaking, the union of Tx-IRS and IRS-Rx distance pairs that enable full multiplexing communication between the Tx and the Rx. We propose a new passive beamforming (PB) strategy named reflective focusing, which aims to coherently superimpose the waves originating from a transmit antenna, reflected by the IRS, and focused on a receive antenna. With reflective focusing, we derive an inner bound of the FMR, and provide the corresponding orientation settings of the antenna arrays that enable full multiplexing. We further employ the MI to measure the quality of the cascaded LoS MIMO channel, and formulate an optimization problem to maximize the MI over PB and antenna array orientations. We give analytical solutions to the problem under asymptotic conditions such as high or low signal-to-noise ratio (SNR) regimes. For general cases, we propose an alternating optimization method to solve the problem.      
### 92.MS-SincResNet: Joint learning of 1D and 2D kernels using multi-scale SincNet and ResNet for music genre classification  [ :arrow_down: ](https://arxiv.org/pdf/2109.08910.pdf)
>  In this study, we proposed a new end-to-end convolutional neural network, called MS-SincResNet, for music genre classification. MS-SincResNet appends 1D multi-scale SincNet (MS-SincNet) to 2D ResNet as the first convolutional layer in an attempt to jointly learn 1D kernels and 2D kernels during the training stage. First, an input music signal is divided into a number of fixed-duration (3 seconds in this study) music clips, and the raw waveform of each music clip is fed into 1D MS-SincNet filter learning module to obtain three-channel 2D representations. The learned representations carry rich timbral, harmonic, and percussive characteristics comparing with spectrograms, harmonic spectrograms, percussive spectrograms and Mel-spectrograms. ResNet is then used to extract discriminative embeddings from these 2D representations. The spatial pyramid pooling (SPP) module is further used to enhance the feature discriminability, in terms of both time and frequency aspects, to obtain the classification label of each music clip. Finally, the voting strategy is applied to summarize the classification results from all 3-second music clips. In our experimental results, we demonstrate that the proposed MS-SincResNet outperforms the baseline SincNet and many well-known hand-crafted features. Considering individual 2D representation, MS-SincResNet also yields competitive results with the state-of-the-art methods on the GTZAN dataset and the ISMIR2004 dataset. The code is available at <a class="link-external link-https" href="https://github.com/PeiChunChang/MS-SincResNet" rel="external noopener nofollow">this https URL</a>      
### 93.Measuring the rogue wave pattern triggered from Gaussian perturbations by deep learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.08909.pdf)
>  Weak Gaussian perturbations on a plane wave background could trigger lots of rogue waves, due to modulational instability. Numerical simulations showed that these rogue waves seemed to have similar unit structure. However, to the best of our knowledge, there is no relative result to prove that these rogue waves have the similar patterns for different perturbations, partly due to that it is hard to measure the rogue wave pattern automatically. In this work, we address these problems from the perspective of computer vision via using deep neural networks. We propose a Rogue Wave Detection Network (RWD-Net) model to automatically and accurately detect RWs on the images, which directly indicates they have the similar computer vision patterns. For this purpose, we herein meanwhile have designed the related dataset, termed as Rogue Wave Dataset-$10$K (RWD-$10$K), which has $10,191$ RW images with bounding box annotations for each RW unit. In our detection experiments, we get $99.29\%$ average precision on the test splits of the RWD-$10$K dataset. Finally, we derive our novel metric, the density of RW units (DRW), to characterize the evolution of Gaussian perturbations and obtain the statistical results on them.      
### 94.Intra-Inter Subject Self-supervised Learning for Multivariate Cardiac Signals  [ :arrow_down: ](https://arxiv.org/pdf/2109.08908.pdf)
>  Learning information-rich and generalizable representations effectively from unlabeled multivariate cardiac signals to identify abnormal heart rhythms (cardiac arrhythmias) is valuable in real-world clinical settings but often challenging due to its complex temporal dynamics. Cardiac arrhythmias can vary significantly in temporal patterns even for the same patient ($i.e.$, intra subject difference). Meanwhile, the same type of cardiac arrhythmia can show different temporal patterns among different patients due to different cardiac structures ($i.e.$, inter subject difference). In this paper, we address the challenges by proposing an Intra-inter Subject self-supervised Learning (ISL) model that is customized for multivariate cardiac signals. Our proposed ISL model integrates medical knowledge into self-supervision to effectively learn from intra-inter subject differences. In intra subject self-supervision, ISL model first extracts heartbeat-level features from each subject using a channel-wise attentional CNN-RNN encoder. Then a stationarity test module is employed to capture the temporal dependencies between heartbeats. In inter subject self-supervision, we design a set of data augmentations according to the clinical characteristics of cardiac signals and perform contrastive learning among subjects to learn distinctive representations for various types of patients. Extensive experiments on three real-world datasets were conducted. In a semi-supervised transfer learning scenario, our pre-trained ISL model leads about 10% improvement over supervised training when only 1% labeled data is available, suggesting strong generalizability and robustness of the model.      
### 95.Computational Imaging and Artificial Intelligence: The Next Revolution of Mobile Vision  [ :arrow_down: ](https://arxiv.org/pdf/2109.08880.pdf)
>  Signal capture stands in the forefront to perceive and understand the environment and thus imaging plays the pivotal role in mobile vision. Recent explosive progresses in Artificial Intelligence (AI) have shown great potential to develop advanced mobile platforms with new imaging devices. Traditional imaging systems based on the "capturing images first and processing afterwards" mechanism cannot meet this unprecedented demand. Differently, Computational Imaging (CI) systems are designed to capture high-dimensional data in an encoded manner to provide more information for mobile vision systems.Thanks to AI, CI can now be used in real systems by integrating deep learning algorithms into the mobile vision platform to achieve the closed loop of intelligent acquisition, processing and decision making, thus leading to the next revolution of mobile vision.Starting from the history of mobile vision using digital cameras, this work first introduces the advances of CI in diverse applications and then conducts a comprehensive review of current research topics combining CI and AI. Motivated by the fact that most existing studies only loosely connect CI and AI (usually using AI to improve the performance of CI and only limited works have deeply connected them), in this work, we propose a framework to deeply integrate CI and AI by using the example of self-driving vehicles with high-speed communication, edge computing and traffic planning. Finally, we outlook the future of CI plus AI by investigating new materials, brain science and new computing techniques to shed light on new directions of mobile vision systems.      
### 96.V-SlowFast Network for Efficient Visual Sound Separation  [ :arrow_down: ](https://arxiv.org/pdf/2109.08867.pdf)
>  The objective of this paper is to perform visual sound separation: i) we study visual sound separation on spectrograms of different temporal resolutions; ii) we propose a new light yet efficient three-stream framework V-SlowFast that operates on Visual frame, Slow spectrogram, and Fast spectrogram. The Slow spectrogram captures the coarse temporal resolution while the Fast spectrogram contains the fine-grained temporal resolution; iii) we introduce two contrastive objectives to encourage the network to learn discriminative visual features for separating sounds; iv) we propose an audio-visual global attention module for audio and visual feature fusion; v) the introduced V-SlowFast model outperforms previous state-of-the-art in single-frame based visual sound separation on small- and large-scale datasets: MUSIC-21, AVE, and VGG-Sound. We also propose a small V-SlowFast architecture variant, which achieves 74.2% reduction in the number of model parameters and 81.4% reduction in GMACs compared to the previous multi-stage models. Project page: <a class="link-external link-https" href="https://ly-zhu.github.io/V-SlowFast" rel="external noopener nofollow">this https URL</a>      
### 97.SpeechNAS: Towards Better Trade-off between Latency and Accuracy for Large-Scale Speaker Verification  [ :arrow_down: ](https://arxiv.org/pdf/2109.08839.pdf)
>  Recently, x-vector has been a successful and popular approach for speaker verification, which employs a time delay neural network (TDNN) and statistics pooling to extract speaker characterizing embedding from variable-length utterances. Improvement upon the x-vector has been an active research area, and enormous neural networks have been elaborately designed based on the x-vector, eg, extended TDNN (E-TDNN), factorized TDNN (F-TDNN), and densely connected TDNN (D-TDNN). In this work, we try to identify the optimal architectures from a TDNN based search space employing neural architecture search (NAS), named SpeechNAS. Leveraging the recent advances in the speaker recognition, such as high-order statistics pooling, multi-branch mechanism, D-TDNN and angular additive margin softmax (AAM) loss with a minimum hyper-spherical energy (MHE), SpeechNAS automatically discovers five network architectures, from SpeechNAS-1 to SpeechNAS-5, of various numbers of parameters and GFLOPs on the large-scale text-independent speaker recognition dataset VoxCeleb1. Our derived best neural network achieves an equal error rate (EER) of 1.02% on the standard test set of VoxCeleb1, which surpasses previous TDNN based state-of-the-art approaches by a large margin. Code and trained weights are in <a class="link-external link-https" href="https://github.com/wentaozhu/speechnas.git" rel="external noopener nofollow">this https URL</a>      
### 98.A Robust and Efficient Multi-Scale Seasonal-Trend Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2109.08800.pdf)
>  Many real-world time series exhibit multiple seasonality with different lengths. The removal of seasonal components is crucial in numerous applications of time series, including forecasting and anomaly detection. However, many seasonal-trend decomposition algorithms suffer from high computational cost and require a large amount of data when multiple seasonal components exist, especially when the periodic length is long. In this paper, we propose a general and efficient multi-scale seasonal-trend decomposition algorithm for time series with multiple seasonality. We first down-sample the original time series onto a lower resolution, and then convert it to a time series with single seasonality. Thus, existing seasonal-trend decomposition algorithms can be applied directly to obtain the rough estimates of trend and the seasonal component corresponding to the longer periodic length. By considering the relationship between different resolutions, we formulate the recovery of different components on the high resolution as an optimization problem, which is solved efficiently by our alternative direction multiplier method (ADMM) based algorithm. Our experimental results demonstrate the accurate decomposition results with significantly improved efficiency.      
### 99.The Optimization of the Constant Flow Parallel Micropump Using RBF Neural Network  [ :arrow_down: ](https://arxiv.org/pdf/2109.08717.pdf)
>  The objective of this work is to optimize the performance of a constant flow parallel mechanical displacement micropump, which has parallel pump chambers and incorporates passive check valves. The critical task is to minimize the pressure pulse caused by regurgitation, which negatively impacts the constant flow rate, during the reciprocating motion when the left and right pumps interchange their role of aspiration and transfusion. Previous works attempt to solve this issue via the mechanical design of passive check valves. In this work, the novel concept of overlap time is proposed, and the issue is solved from the aspect of control theory by implementing a RBF neural network trained by both unsupervised and supervised learning. The experimental results indicate that the pressure pulse is optimized in the range of 0.15 - 0.25 MPa, which is a significant improvement compared to the maximum pump working pressure of 40 MPa.      
### 100.Online Traffic Routing: Deterministic Limits and Data-driven Enhancements  [ :arrow_down: ](https://arxiv.org/pdf/2109.08706.pdf)
>  Over the past decade, GPS enabled traffic applications, such as Google Maps and Waze, have become ubiquitous and have had a significant influence on billions of daily commuters' travel patterns. A consequence of the online route suggestions of such applications, e.g., via greedy routing, has often been an increase in traffic congestion since the induced travel patterns may be far from the system optimum. Spurred by the widespread impact of traffic applications on travel patterns, this work studies online traffic routing in the context of capacity-constrained parallel road networks and analyzes this problem from two perspectives. First, we perform a worst-case analysis to identify the limits of deterministic online routing and show that the ratio between the total travel cost of the online solution of any deterministic algorithm and that of the optimal offline solution is unbounded, even in simple settings. This result motivates us to move beyond worst-case analysis. Here, we consider algorithms that exploit knowledge of past problem instances and show how to design data-driven algorithms whose performance can be quantified and formally generalized to unseen future instances. We present numerical experiments based on an application case for the San Francisco Bay Area to evaluate the performance of our approach. Our results show that the data-driven algorithms we develop outperform commonly used greedy online-routing algorithms.      
### 101.Speaker Placement Agnosticism: Improving the Distance-based Amplitude Panning Algorithm  [ :arrow_down: ](https://arxiv.org/pdf/2109.08704.pdf)
>  Lossius et. al introduced the distance-based amplitude panning algorithm, or DBAP, to enable flexibility of loudspeaker placement in artistic and scientific contexts. The algorithm allows for arbitrary loudspeaker locations in a 2D plane so that a virtual sound source may navigate the 2D space. The gains for each speaker are calculated as a function of the source's distance to each loudspeaker, thus creating a sound field. This gives the listener the impression of a source moving through the field of loudspeakers. This paper introduces a heuristically developed robust variation of DBAP that corrects for faulty assumptions in the implementation of Lossius. Specifically, this paper develops a method for working with sound sources outside the field of loudspeakers in which the Lossius version produces distorted aural impressions and wildly undulating amplitudes caused by spatial discontinuities in the gains of the various loudspeakers. In smoothing the spatial impression of the virtual source, we are also able to eliminate the calculation of the convex hull entirely, a necessary component of the original implementation. This significantly simplifies and reduces the calculations required for any space in either two or three dimensions.      
### 102.Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths  [ :arrow_down: ](https://arxiv.org/pdf/2109.03310.pdf)
>  Malignant melanoma is a common skin cancer that is mostly curable before metastasis -when growths spawn in organs away from the original site. Melanoma is the most dangerous type of skin cancer if left untreated due to the high risk of metastasis. This paper presents Melatect, a machine learning (ML) model embedded in an iOS app that identifies potential malignant melanoma. Melatect accurately classifies lesions as malignant or benign over 96.6% of the time with no apparent bias or overfitting. Using the Melatect app, users have the ability to take pictures of skin lesions (moles) and subsequently receive a mole classification. The Melatect app provides a convenient way to get free advice on lesions and track these lesions over time. A recursive computer image analysis algorithm and modified MLOps pipeline was developed to create a model that performs at a higher accuracy than existing models. Our training dataset included 18,400 images of benign and malignant lesions, including 18,000 from the International Skin Imaging Collaboration (ISIC) archive, as well as 400 images gathered from local dermatologists; these images were augmented using DeepAugment, an AutoML tool, to 54,054 images.      
### 103.Surgical data science for safe cholecystectomy: a protocol for segmentation of hepatocystic anatomy and assessment of the critical view of safety  [ :arrow_down: ](https://arxiv.org/pdf/2106.10916.pdf)
>  Minimally invasive image-guided surgery heavily relies on vision. Deep learning models for surgical video analysis could therefore support visual tasks such as assessing the critical view of safety (CVS) in laparoscopic cholecystectomy (LC), potentially contributing to surgical safety and efficiency. However, the performance, reliability and reproducibility of such models are deeply dependent on the quality of data and annotations used in their development. Here, we present a protocol, checklists, and visual examples to promote consistent annotation of hepatocystic anatomy and CVS criteria. We believe that sharing annotation guidelines can help build trustworthy multicentric datasets for assessing generalizability of performance, thus accelerating the clinical translation of deep learning models for surgical video analysis.      
### 104.Efficient Pairwise Neuroimage Analysis using the Soft Jaccard Index and 3D Keypoint Sets  [ :arrow_down: ](https://arxiv.org/pdf/2103.06966.pdf)
>  We propose a novel pairwise distance measure between image keypoint sets, for the purpose of large-scale medical image indexing. Our measure generalizes the Jaccard index to account for soft set equivalence (SSE) between keypoint elements, via an adaptive kernel framework modeling uncertainty in keypoint appearance and geometry. A new kernel is proposed to quantify the variability of keypoint geometry in location and scale. Our distance measure may be estimated between $O(N^2)$ image pairs in $O(N~\log~N)$ operations via keypoint indexing. Experiments report the first results for the task of predicting family relationships from medical images, using 1010 T1-weighted MRI brain volumes of 434 families including monozygotic and dizygotic twins, siblings and half-siblings sharing 100%-25% of their polymorphic genes. Soft set equivalence and the keypoint geometry kernel improve upon standard hard set equivalence (HSE) and appearance kernels alone in predicting family relationships. Monozygotic twin identification is near 100%, and three subjects with uncertain genotyping are automatically paired with their self-reported families, the first reported practical application of image-based family identification. Our distance measure can also be used to predict group categories, sex is predicted with an AUC=0.97. Software is provided for efficient fine-grained curation of large, generic image datasets.      
