# ArXiv eess --Mon, 20 Sep 2021
### 1.Learning Sparse Graph with Minimax Concave Penalty under Gaussian Markov Random Fields  [ :arrow_down: ](https://arxiv.org/pdf/2109.08666.pdf)
>  This paper presents a convex-analytic framework to learn sparse graphs from data. While our problem formulation is inspired by an extension of the graphical lasso using the so-called combinatorial graph Laplacian framework, a key difference is the use of a nonconvex alternative to the $\ell_1$ norm to attain graphs with better interpretability. Specifically, we use the weakly-convex minimax concave penalty (the difference between the $\ell_1$ norm and the Huber function) which is known to yield sparse solutions with lower estimation bias than $\ell_1$ for regression problems. In our framework, the graph Laplacian is replaced in the optimization by a linear transform of the vector corresponding to its upper triangular part. Via a reformulation relying on Moreau's decomposition, we show that overall convexity is guaranteed by introducing a quadratic function to our cost function. The problem can be solved efficiently by the primal-dual splitting method, of which the admissible conditions for provable convergence are presented. Numerical examples show that the proposed method significantly outperforms the existing graph learning methods with reasonable CPU time.      
### 2.Nonlinear Deterministic Filter for Inertial Navigation and Bias Estimation with Guaranteed Performance  [ :arrow_down: ](https://arxiv.org/pdf/2109.08654.pdf)
>  Unmanned vehicle navigation concerns estimating attitude, position, and linear velocity of the vehicle the six degrees of freedom (6 DoF). It has been known that the true navigation dynamics are highly nonlinear modeled on the Lie Group of $\mathbb{SE}_{2}(3)$. In this paper, a nonlinear filter for inertial navigation is proposed. The filter ensures systematic convergence of the error components starting from almost any initial condition. Also, the errors converge asymptotically to the origin. Experimental results validates the robustness of the proposed filter.      
### 3.Unraveling the paradox of intensity-dependent DVS pixel noise  [ :arrow_down: ](https://arxiv.org/pdf/2109.08640.pdf)
>  Dynamic vision sensor (DVS) event camera output is affected by noise, particularly in dim lighting conditions. A theory explaining how photon and electron noise affect DVS output events has so far not been developed. Moreover, there is no clear understanding of how DVS parameters and operating conditions affect noise. There is an apparent paradox between the real noise data observed from the DVS output and the reported noise measurements of the logarithmic photoreceptor. While measurements of the logarithmic photoreceptor predict that the photoreceptor is approximately a first-order system with RMS noise voltage independent of the photocurrent, DVS output shows higher noise event rates at low light intensity. This paper unravels this paradox by showing how the DVS photoreceptor is a second-order system, and the assumption that it is first-order is generally not reasonable. As we show, at higher photocurrents, the photoreceptor amplifier dominates the frequency response, causing a drop in RMS noise voltage and noise event rate. We bring light to the noise performance of the DVS photoreceptor by presenting a theoretical explanation supported by both transistor-level simulation results and chip measurements.      
### 4.A review of deep learning methods for MRI reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2109.08618.pdf)
>  Following the success of deep learning in a wide range of applications, neural network-based machine-learning techniques have received significant interest for accelerating magnetic resonance imaging (MRI) acquisition and reconstruction strategies. A number of ideas inspired by deep learning techniques for computer vision and image processing have been successfully applied to nonlinear image reconstruction in the spirit of compressed sensing for accelerated MRI. Given the rapidly growing nature of the field, it is imperative to consolidate and summarize the large number of deep learning methods that have been reported in the literature, to obtain a better understanding of the field in general. This article provides an overview of the recent developments in neural-network based approaches that have been proposed specifically for improving parallel imaging. A general background and introduction to parallel MRI is also given from a classical view of k-space based reconstruction methods. Image domain based techniques that introduce improved regularizers are covered along with k-space based methods which focus on better interpolation strategies using neural networks. While the field is rapidly evolving with thousands of papers published each year, in this review, we attempt to cover broad categories of methods that have shown good performance on publicly available data sets. Limitations and open problems are also discussed and recent efforts for producing open data sets and benchmarks for the community are examined.      
### 5.Continuous Streaming Multi-Talker ASR with Dual-path Transducers  [ :arrow_down: ](https://arxiv.org/pdf/2109.08555.pdf)
>  Streaming recognition of multi-talker conversations has so far been evaluated only for 2-speaker single-turn sessions. In this paper, we investigate it for multi-turn meetings containing multiple speakers using the Streaming Unmixing and Recognition Transducer (SURT) model, and show that naively extending the single-turn model to this harder setting incurs a performance penalty. As a solution, we propose the dual-path (DP) modeling strategy first used for time-domain speech separation. We experiment with LSTM and Transformer based DP models, and show that they improve word error rate (WER) performance while yielding faster convergence. We also explore training strategies such as chunk width randomization and curriculum learning for these models, and demonstrate their importance through ablation studies. Finally, we evaluate our models on the LibriCSS meeting data, where they perform competitively with offline separation-based methods.      
### 6.Improvement of Flood Extent Representation with Remote Sensing Data and Data Assimilation Applied to Hydrodynamic Numerical Models  [ :arrow_down: ](https://arxiv.org/pdf/2109.08487.pdf)
>  Flood simulation and forecast capability have been greatly improved thanks to advances in data assimilation. Such an approach combines in-situ gauge measurements with numerical hydrodynamic models to correct the hydraulic states and reduce the uncertainties in the model parameters. However, these methods depend strongly on the availability and quality of observations, thus necessitating other data sources to improve the flood simulation and forecast performances. Using Sentinel-1 images, a flood extent mapping method was carried out by applying a Random Forest algorithm trained on past flood events using manually delineated flood maps. The study area concerns a 50-km reach of the Garonne Marmandaise catchment. Two recent flood events are simulated in analysis and forecast modes, with a +24h lead time. This study demonstrates the merits of using SAR-derived flood extent maps to validate and improve the forecast results based on hydrodynamic numerical models with Telemac2D-EnKF. Quantitative 1D and 2D metrics were computed to assess water level time-series and flood extents between the simulations and observations. It was shown that the free run experiment without DA under-estimates flooding. On the other hand, the validation of DA results with respect to independent SAR-derived flood extent allows to diagnose a model-observation bias that leads to over-flooding. Once this bias is taken into account, DA provides a sequential correction of area-based friction coefficients and inflow discharge, yielding a better flood extent representation. This study paves the way towards a reliable solution for flood forecasting over poorly gauged catchments, thanks to available remote sensing datasets.      
### 7.Hybrid Open Points for Increasing Network Capacity in Distribution Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.08486.pdf)
>  This letter introduces the Hybrid Open Point (HOP), a device consisting of an electromechanical switch connected in parallel with a power converter, for the purpose of providing additional network capacity in interconnected distribution systems. The HOP switch is used for bulk power transfer at low-cost, whilst the HOP converter provides targeted power transfer when the HOP switch is open. The device can replace either a Normally Open Point (Type 1 HOP) or a Normally Closed Point (Type 2 HOP). Simple interconnection and teed interconnection configurations are studied considering fault level and radiality constraints, with realistic use-cases identified for both HOP types. The HOP is shown to provide secure network capacity more cost-effectively than the classical Soft Open Point.      
### 8.CardiSort: a convolutional neural network for cross vendor automated sorting of cardiac MR images  [ :arrow_down: ](https://arxiv.org/pdf/2109.08479.pdf)
>  Objectives: To develop an image-based automatic deep learning method to classify cardiac MR images by sequence type and imaging plane for improved clinical post-processing efficiency. Methods: Multi-vendor cardiac MRI studies were retrospectively collected from 4 centres and 3 vendors. A two-head convolutional neural network ('CardiSort') was trained to classify 35 sequences by imaging sequence (n=17) and plane (n=10). Single vendor training (SVT) on single centre images (n=234 patients) and multi-vendor training (MVT) with multicentre images (n = 479 patients, 3 centres) was performed. Model accuracy was compared to manual ground truth labels by an expert radiologist on a hold-out test set for both SVT and MVT. External validation of MVT (MVTexternal) was performed on data from 3 previously unseen magnet systems from 2 vendors (n=80 patients). Results: High sequence and plane accuracies were observed for SVT (85.2% and 93.2% respectively), and MVT (96.5% and 98.1% respectively) on the hold-out test set. MVTexternal yielded sequence accuracy of 92.7% and plane accuracy of 93.0%. There was high accuracy for common sequences and conventional cardiac planes. Poor accuracy was observed for underrepresented classes and sequences where there was greater variability in acquisition parameters across centres, such as perfusion imaging. Conclusions: A deep learning network was developed on multivendor data to classify MRI studies into component sequences and planes, with external validation. With refinement, it has potential to improve workflow by enabling automated sequence selection, an important first step in completely automated post-processing pipelines.      
### 9.Transformer-Unet: Raw Image Processing with Unet  [ :arrow_down: ](https://arxiv.org/pdf/2109.08417.pdf)
>  Medical image segmentation have drawn massive attention as it is important in biomedical image analysis. Good segmentation results can assist doctors with their judgement and further improve patients' experience. Among many available pipelines in medical image analysis, Unet is one of the most popular neural networks as it keeps raw features by adding concatenation between encoder and decoder, which makes it still widely used in industrial field. In the mean time, as a popular model which dominates natural language process tasks, transformer is now introduced to computer vision tasks and have seen promising results in object detection, image classification and semantic segmentation tasks. Therefore, the combination of transformer and Unet is supposed to be more efficient than both methods working individually. In this article, we propose Transformer-Unet by adding transformer modules in raw images instead of feature maps in Unet and test our network in CT82 datasets for Pancreas segmentation accordingly. We form an end-to-end network and gain segmentation results better than many previous Unet based algorithms in our experiment. We demonstrate our network and show our experimental results in this paper accordingly.      
### 10.Estimation de ligne de base de capteurs d'humectation : int{é}gration et minimum locaux {à} diff{é}rentes {é}chelles  [ :arrow_down: ](https://arxiv.org/pdf/2109.08410.pdf)
>  Dielectric wetness sensors are used in agriculture to detect the presence of water on foliage and to predict the risk of disease development. The measured electrical signal has a base level drift that skews the alerts. We propose a method for estimating this baseline using L1 and selecting local minimums at an observation scale. The performance of the estimator is evaluated on simulated data and compared to the literature estimators.      
### 11.Mode Clustering Based Dynamic Equivalent Modeling of Wind Farm for Small-Signal Stability Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2109.08383.pdf)
>  Dynamic equivalent models (DEMs) are necessities for the small-signal stability (SSS) analysis of the power system with large-scale wind farms (WFs). This paper proposes a mode clustering based dynamic equivalent modeling method of WFs for the SSS analysis. It is deemed that a DEM can be used to represent the whole WF to evaluate its impact on the SSS of power systems, as long as the frequency response of the DEM adequately matches that of the detailed WF model around the frequency of oscillation modes of concern. Focusing on the concerned oscillation modes in the small-signal model of the whole WF, closely distributed modes of them on the complex plane are classified into a cluster and then represented by a single mode. By the linear superposition principle, the modal participation factor (MPF) regarding each of modal clusters are superimposed, generating a feature vector for each wind turbine (WT). Based on the feature vectors, the oscillation feature similarity of the WTs as well as their aggregation can be evaluated and performed easily. Taking the DC-link voltage control (DVC) mode of full-power WT converters as an example, the aggregated DEM is verified by frequency-domain analyses and time-domain simulations.      
### 12.Maximum-Rate Optimization of Hybrid Intelligent Reflective Surface and Relay Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.08378.pdf)
>  We consider a wireless communication system, where a transmitting source is assisted by both a reconfigurable intelligent reflecting surface (IRS) and a decode-and-forward half-duplex relay (hybrid IRS-relay scheme) to communicate with a destination receiver. All devices are equipped with multiple antennas, and transmissions occur in two stages. In stage 1, the source splits the transmit message into two sub-messages, transmitted to the destination and the relay, respectively, using block diagonalization to avoid interference. Both transmissions will benefit from the IRS. In stage 2, the relay re-encodes the received sub-message and forwards it (still through the IRS) to the destination. We optimize power allocations, beamformers, and configurations of the IRS in both stages, in order to maximize the achievable rate at the destination. We compare the proposed hybrid approach with other schemes (with/without relay and IRS), and confirm that high data rate is achieved for the hybrid scheme in case of optimal IRS configurations.      
### 13.Challenges of Driver Drowsiness Prediction: The Remaining Steps to Implementation  [ :arrow_down: ](https://arxiv.org/pdf/2109.08355.pdf)
>  Driver drowsiness has caused a large number of serious injuries and deaths on public roads and incurred billions of taxpayer dollars in costs. Hence, monitoring of drowsiness is critical to reduce this burden on society. This paper surveys the broad range of solutions proposed to address the challenges of driver drowsiness, and identifies the key steps required for successful implementation. Although some commercial products already exist, with vehicle-based methods most commonly implemented by automotive manufacturers, these systems may not have the level of accuracy required to properly predict and monitor drowsiness. State-of-the-art models use physiological, behavioural and vehicle-based methods to detect drowsiness, with hybrid methods emerging as a superior approach. Current setbacks to implementing these methods include late detection, intrusiveness and subject diversity. In particular, physiological monitoring methods such as Electroencephalography (EEG) are intrusive to drivers; while behavioural monitoring is least robust, affected by external factors such as lighting, as well as being subject to privacy concerns. Drowsiness detection models are often developed and validated based on subjective measures, with the Karolinska Sleepiness Scale being the most popular. Subjective and incoherent labelling of drowsiness, lack of on road data and inconsistent protocols for data collection are among other challenges to be addressed to progress drowsiness detection for reliable on-road use.      
### 14.The Stackelberg Equilibrium for One-sided Zero-sum Partially Observable Stochastic Games  [ :arrow_down: ](https://arxiv.org/pdf/2109.08339.pdf)
>  Formulating cyber-security problems with attackers and defenders as a partially observable stochastic game has become a trend recently. Among them, the one-sided two-player zero-sum partially observable stochastic game (OTZ-POSG) has emerged as a popular model because it allows players to compete for multiple stages based on partial knowledge of the system. All existing work on OTZ-POSG has focused on the simultaneous move scenario and assumed that one player's actions are private in the execution process. However, this assumption may become questionable since one player's action may be detected by the opponent through deploying action detection strategies. Hence, in this paper, we propose a turn-based OTZ-POSG with the assumption of public actions and investigate the existence and properties of a Stackelberg equilibrium for this game. We first prove the existence of the Stackelberg equilibrium for the one-stage case and show that the one-stage game can be converted into a linear-fractional programming problem and therefore solved by linear programming. For multiple stages, the main challenge is the information leakage issue as the public run-time action reveals certain private information to the opponent and allows the opponent to achieve more rewards in the future. To deal with this issue, we adopt the concept of $\epsilon$-Stackelberg equilibrium and prove that this equilibrium can be achieved for finite-horizon OTZ-POSGs. We propose a space partition approach to solve the game iteratively and show that the value function of the leader is piece-wise linear and the value function of the follower is piece-wise constant for multiple stages. Finally, examples are given to illustrate the space partition approach and show that value functions are piece-wise linear and piece-wise constant.      
### 15.Cell-Level State of Charge Estimation for Battery Packs Under Minimal Sensing  [ :arrow_down: ](https://arxiv.org/pdf/2109.08332.pdf)
>  This manuscript presents an algorithm for individual Lithium-ion (Li-ion) battery cell state of charge (SOC) estimation in a large-scale battery pack under minimal sensing, where only pack-level voltage and current are measured. For battery packs consisting of up to thousands of cells in electric vehicle or stationary energy storage applications, it is desirable to estimate individual cell SOCs without cell local measurements in order to reduce sensing costs. Mathematically, pure series connected cells yield dynamics given by ordinary differential equations under classical full voltage sensing. In contrast, parallel--series connected battery packs are evidently more challenging because the dynamics are governed by a nonlinear differential--algebraic equations (DAE) system. The majority of the conventional studies on SOC estimation for battery packs benefit from idealizing the pack as a lumped single cell which ultimately lose track of cell-level conditions and are blind to potential risks of cell-level over-charge and over-discharge. This work explicitly models a battery pack with high fidelity cell-by-cell resolution based on the interconnection of single cell models, and examines the observability of cell-level state with only pack-level measurements. A DAE-based state observer with linear output error injection is formulated, where the individual cell SOC and current can be reconstructed from minimal number of pack sensing. The mathematically guaranteed asymptotic convergence of differential and algebraic state estimates is established by considering local Lipschitz continuity property of system nonlinearities. Simulation results for Graphite/NMC cells illustrate convergence for cell SOCs, currents, and voltages.      
### 16.Mass Segmentation in Automated 3-D Breast Ultrasound Using Dual-Path U-net  [ :arrow_down: ](https://arxiv.org/pdf/2109.08330.pdf)
>  Automated 3-D breast ultrasound (ABUS) is a newfound system for breast screening that has been proposed as a supplementary modality to mammography for breast cancer detection. While ABUS has better performance in dense breasts, reading ABUS images is exhausting and time-consuming. So, a computer-aided detection system is necessary for interpretation of these images. Mass segmentation plays a vital role in the computer-aided detection systems and it affects the overall performance. Mass segmentation is a challenging task because of the large variety in size, shape, and texture of masses. Moreover, an imbalanced dataset makes segmentation harder. A novel mass segmentation approach based on deep learning is introduced in this paper. The deep network that is used in this study for image segmentation is inspired by U-net, which has been used broadly for dense segmentation in recent years. The system's performance was determined using a dataset of 50 masses including 38 malign and 12 benign lesions. The proposed segmentation method attained a mean Dice of 0.82 which outperformed a two-stage supervised edge-based method with a mean Dice of 0.74 and an adaptive region growing method with a mean Dice of 0.65.      
### 17.Adaptive Hierarchical Dual Consistency for Semi-Supervised Left Atrium Segmentation on Cross-Domain Data  [ :arrow_down: ](https://arxiv.org/pdf/2109.08311.pdf)
>  Semi-supervised learning provides great significance in left atrium (LA) segmentation model learning with insufficient labelled data. Generalising semi-supervised learning to cross-domain data is of high importance to further improve model robustness. However, the widely existing distribution difference and sample mismatch between different data domains hinder the generalisation of semi-supervised learning. In this study, we alleviate these problems by proposing an Adaptive Hierarchical Dual Consistency (AHDC) for the semi-supervised LA segmentation on cross-domain data. The AHDC mainly consists of a Bidirectional Adversarial Inference module (BAI) and a Hierarchical Dual Consistency learning module (HDC). The BAI overcomes the difference of distributions and the sample mismatch between two different domains. It mainly learns two mapping networks adversarially to obtain two matched domains through mutual adaptation. The HDC investigates a hierarchical dual learning paradigm for cross-domain semi-supervised segmentation based on the obtained matched domains. It mainly builds two dual-modelling networks for mining the complementary information in both intra-domain and inter-domain. For the intra-domain learning, a consistency constraint is applied to the dual-modelling targets to exploit the complementary modelling information. For the inter-domain learning, a consistency constraint is applied to the LAs modelled by two dual-modelling networks to exploit the complementary knowledge among different data domains. We demonstrated the performance of our proposed AHDC on four 3D late gadolinium enhancement cardiac MR (LGE-CMR) datasets from different centres and a 3D CT dataset. Compared to other state-of-the-art methods, our proposed AHDC achieved higher segmentation accuracy, which indicated its capability in the cross-domain semi-supervised LA segmentation.      
### 18.Decentralized Secure State-Tracking in Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.08264.pdf)
>  This paper addresses the problem of decentralized state-tracking in the presence of sensor attacks. We consider a network of nodes where each node has the objective of tracking the state of a linear dynamical system based on its measurements and messages exchanged with neighboring nodes notwithstanding some measurements being spoofed by an adversary. We propose a novel decentralized attack-resilient state-tracking algorithm based on the simple observation that a compressed version of all the network measurements suffices to reconstruct the state. This motivates a 2-step solution to the decentralized secure state-tracking problem: (1) each node tracks the compressed version of all the network measurements, and (2) each node asymptotically reconstructs the state from the output of step (1). We prove that, under mild technical assumptions, our algorithm enables each node to track the state of the linear system and thus solves the decentralized secure state-tracking problem.      
### 19.Neural Network Based Lidar Gesture Recognition for Realtime Robot Teleoperation  [ :arrow_down: ](https://arxiv.org/pdf/2109.08263.pdf)
>  We propose a novel low-complexity lidar gesture recognition system for mobile robot control robust to gesture variation. Our system uses a modular approach, consisting of a pose estimation module and a gesture classifier. Pose estimates are predicted from lidar scans using a Convolutional Neural Network trained using an existing stereo-based pose estimation system. Gesture classification is accomplished using a Long Short-Term Memory network and uses a sequence of estimated body poses as input to predict a gesture. Breaking down the pipeline into two modules reduces the dimensionality of the input, which could be lidar scans, stereo imagery, or any other modality from which body keypoints can be extracted, making our system lightweight and suitable for mobile robot control with limited computing power. The use of lidar contributes to the robustness of the system, allowing it to operate in most outdoor conditions, to be independent of lighting conditions, and for input to be detected 360 degrees around the robot. The lidar-based pose estimator and gesture classifier use data augmentation and automated labeling techniques, requiring a minimal amount of data collection and avoiding the need for manual labeling. We report experimental results for each module of our system and demonstrate its effectiveness by testing it in a real-world robot teleoperation setting.      
### 20.Stereo Video Reconstruction Without Explicit Depth Maps for Endoscopic Surgery  [ :arrow_down: ](https://arxiv.org/pdf/2109.08227.pdf)
>  We introduce the task of stereo video reconstruction or, equivalently, 2D-to-3D video conversion for minimally invasive surgical video. We design and implement a series of end-to-end U-Net-based solutions for this task by varying the input (single frame vs. multiple consecutive frames), loss function (MSE, MAE, or perceptual losses), and network architecture. We evaluate these solutions by surveying ten experts - surgeons who routinely perform endoscopic surgery. We run two separate reader studies: one evaluating individual frames and the other evaluating fully reconstructed 3D video played on a VR headset. In the first reader study, a variant of the U-Net that takes as input multiple consecutive video frames and outputs the missing view performs best. We draw two conclusions from this outcome. First, motion information coming from multiple past frames is crucial in recreating stereo vision. Second, the proposed U-Net variant can indeed exploit such motion information for solving this task. The result from the second study further confirms the effectiveness of the proposed U-Net variant. The surgeons reported that they could successfully perceive depth from the reconstructed 3D video clips. They also expressed a clear preference for the reconstructed 3D video over the original 2D video. These two reader studies strongly support the usefulness of the proposed task of stereo reconstruction for minimally invasive surgical video and indicate that deep learning is a promising approach to this task. Finally, we identify two automatic metrics, LPIPS and DISTS, that are strongly correlated with expert judgement and that could serve as proxies for the latter in future studies.      
### 21.A Data-Driven Uncertainty Quantification Method for Stochastic Economic Dispatch  [ :arrow_down: ](https://arxiv.org/pdf/2109.08195.pdf)
>  This letter proposes a data-driven sparse polynomial chaos expansion-based surrogate model for the stochastic economic dispatch problem considering uncertainty from wind power. The proposed method can provide accurate estimations for the statistical information (e.g., mean, variance, probability density function, and cumulative distribution function) for the stochastic economic dispatch solution efficiently without requiring the probability distributions of random inputs. Simulation studies on an integrated electricity and gas system (IEEE 118-bus system integrated with a 20-node gas system are presented, demonstrating the efficiency and accuracy of the proposed method compared to the Monte Carlo simulations.      
### 22.Fast-Slow Transformer for Visually Grounding Speech  [ :arrow_down: ](https://arxiv.org/pdf/2109.08186.pdf)
>  We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS. FaST-VGS is a Transformer-based model for learning the associations between raw speech waveforms and visual images. The model unifies dual-encoder and cross-attention architectures into a single model, reaping the superior retrieval speed of the former along with the accuracy of the latter. FaST-VGS achieves state-of-the-art speech-image retrieval accuracy on benchmark datasets, and its learned representations exhibit strong performance on the ZeroSpeech 2021 phonetic and semantic tasks.      
### 23.Version Age of Information in Clustered Gossip Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.08669.pdf)
>  We consider a network consisting of a single source and $n$ receiver nodes that are grouped into equal-sized clusters. We use cluster heads in each cluster to facilitate communication between the source and the nodes within that cluster. Inside clusters, nodes are connected to each other according to a given network topology. Based on the connectivity among the nodes, each node relays its current stored version of the source update to its neighboring nodes by $local$ $gossiping$. We use the $version$ $age$ metric to assess information freshness at the nodes. We consider disconnected, ring, and fully connected network topologies for each cluster. For each network topology, we characterize the average version age at each node and find the average version age scaling as a function of the network size $n$. Our results indicate that per node average version age scalings of $O(\sqrt{n})$, $O(n^{\frac{1}{3}})$, and $O(\log n)$ are achievable in disconnected, ring, and fully connected cluster models, respectively. Next, we increase connectivity in the network and allow gossiping among the cluster heads to improve version age at the nodes. With that, we show that when the cluster heads form a ring network among themselves, we obtain per node average version age scalings of $O(n^{\frac{1}{3}})$, $O(n^{\frac{1}{4}})$, and $O(\log n)$ in disconnected, ring, and fully connected cluster models, respectively. Next, focusing on a ring network topology in each cluster, we introduce hierarchy to the considered clustered gossip network model and show that when we employ two levels of hierarchy, we can achieve the same $O(n^{\frac{1}{4}})$ scaling without using dedicated cluster heads. We generalize this result for $h$ levels of hierarchy and show that per user average version age scaling of $O(n^{\frac{1}{2h}})$ is achievable in the case of a ring network in each cluster across all hierarchy levels.      
### 24.Predicting the effects of waning vaccine immunity against COVID-19 through high-resolution agent-based modeling  [ :arrow_down: ](https://arxiv.org/pdf/2109.08660.pdf)
>  The COVID-19 pandemic is yet again on the verge of escalating, despite a hopeful case decrease recorded during spring and summer 2021, due to successful vaccination roll-outs. Together with the emergence of new variants, the potential waning of the vaccination immunity could pose threats to public health. It is tenable that the timing of such a gradual drop in the immunity of most of the vaccinated population would synchronize with the near-complete restoration of normalcy. Should also testing be relaxed, we might witness a potentially disastrous COVID-19 wave in winter 2021/2022. In response to this risk, many countries, including the U.S., are opting for the administration of a third vaccine dose, the booster shot. Here, in a projected study with an outlook of six months, we explore the interplay between the rate at which boosters are distributed and the extent to which testing practices are implemented. Projections are based on a highly granular agent-based model that provides a close, one-to-one digital reproduction of a real, medium-sized U.S. town. Focusing on the dominant Delta variant, we contemplate the waning immunity provided by the locally available Johnson&amp;Johnson, Pfizer, and Moderna vaccines. Theoretical projections indicate that the administration of boosters at the rate at which the vaccine is currently administered could yield a severe resurgence of the pandemic, even worse than the first wave experienced in spring and summer 2020. Our projections suggest that the peak levels of mid spring 2021 in the vaccination rate may prevent the occurrence of such a scenario. Our study highlights the importance of testing, especially to detect infection of asymptomatic individuals in the very near future, as the release of the booster reaches full speed.      
### 25.Graph Learning for Cognitive Digital Twins in Manufacturing Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.08632.pdf)
>  Future manufacturing requires complex systems that connect simulation platforms and virtualization with physical data from industrial processes. Digital twins incorporate a physical twin, a digital twin, and the connection between the two. Benefits of using digital twins, especially in manufacturing, are abundant as they can increase efficiency across an entire manufacturing life-cycle. The digital twin concept has become increasingly sophisticated and capable over time, enabled by rises in many technologies. In this paper, we detail the cognitive digital twin as the next stage of advancement of a digital twin that will help realize the vision of Industry 4.0. Cognitive digital twins will allow enterprises to creatively, effectively, and efficiently exploit implicit knowledge drawn from the experience of existing manufacturing systems. They also enable more autonomous decisions and control, while improving the performance across the enterprise (at scale). This paper presents graph learning as one potential pathway towards enabling cognitive functionalities in manufacturing digital twins. A novel approach to realize cognitive digital twins in the product design stage of manufacturing that utilizes graph learning is presented.      
### 26.Scheduling in Parallel Finite Buffer Systems: Optimal Decisions under Delayed Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2109.08548.pdf)
>  Scheduling decisions in parallel queuing systems arise as a fundamental problem, underlying the dimensioning and operation of many computing and communication systems, such as job routing in data center clusters, multipath communication, and Big Data systems. In essence, the scheduler maps each arriving job to one of the possibly heterogeneous servers while aiming at an optimization goal such as load balancing, low average delay or low loss rate. One main difficulty in finding optimal scheduling decisions here is that the scheduler only partially observes the impact of its decisions, e.g., through the delayed acknowledgements of the served jobs. In this paper, we provide a partially observable (PO) model that captures the scheduling decisions in parallel queuing systems under limited information of delayed acknowledgements. We present a simulation model for this PO system to find a near-optimal scheduling policy in real-time using a scalable Monte Carlo tree search algorithm. We numerically show that the resulting policy outperforms other limited information scheduling strategies such as variants of Join-the-Most-Observations and has comparable performance to full information strategies like: Join-the-Shortest-Queue, Join-the- Shortest-Queue(d) and Shortest-Expected-Delay. Finally, we show how our approach can optimise the real-time parallel processing by using network data provided by Kaggle.      
### 27.Carl-Lead: Lidar-based End-to-End Autonomous Driving with Contrastive Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.08473.pdf)
>  Autonomous driving in urban crowds at unregulated intersections is challenging, where dynamic occlusions and uncertain behaviors of other vehicles should be carefully considered. Traditional methods are heuristic and based on hand-engineered rules and parameters, but scale poorly in new situations. Therefore, they require high labor cost to design and maintain rules in all foreseeable scenarios. Recently, deep reinforcement learning (DRL) has shown promising results in urban driving scenarios. However, DRL is known to be sample inefficient, and most previous works assume perfect observations such as ground-truth locations and motions of vehicles without considering noises and occlusions, which might be a too strong assumption for policy deployment. In this work, we use DRL to train lidar-based end-to-end driving policies that naturally consider imperfect partial observations. We further use unsupervised contrastive representation learning as an auxiliary task to improve the sample efficiency. The comparative evaluation results reveal that our method achieves higher success rates than the state-of-the-art (SOTA) lidar-based end-to-end driving network, better trades off safety and efficiency than the carefully tuned rule-based method, and generalizes better to new scenarios than the baselines. Demo videos are available at <a class="link-external link-https" href="https://caipeide.github.io/carl-lead/" rel="external noopener nofollow">this https URL</a>.      
### 28.OTFS-superimposed PRACH-aided Localization for UAV Safety Applications  [ :arrow_down: ](https://arxiv.org/pdf/2109.08429.pdf)
>  The adoption of Unmanned Aerial Vehicles (UAVs) for public safety applications has skyrocketed in the last years. Leveraging on Physical Random Access Channel (PRACH) preambles, in this paper we pioneer a novel localization technique for UAVs equipped with cellular base stations used in emergency scenarios. We exploit the new concept of Orthogonal Time Frequency Space (OTFS) modulation (tolerant to channel Doppler spread caused by UAVs motion) to build a fully standards-compliant OTFS-modulated PRACH transmission and reception scheme able to perform time-of-arrival (ToA) measurements. First, we analyze such novel ToA ranging technique, both analytically and numerically, to accurately and iteratively derive the distance between localized users and the points traversed by the UAV along its trajectory. Then, we determine the optimal UAV speed as a trade-off between the accuracy of the ranging technique and the power needed by the UAV to reach and keep its speed during emergency operations. Finally, we demonstrate that our solution outperforms standard PRACH-based localization techniques in terms of Root Mean Square Error (RMSE) by about 20% in quasi-static conditions and up to 80% in high-mobility conditions.      
### 29.100% renewable electricity in Japan  [ :arrow_down: ](https://arxiv.org/pdf/2109.08363.pdf)
>  Japan has committed to carbon neutrality by 2050. Emissions from the electricity sector amount to 42% of the total. Solar photovoltaics (PV) and wind comprise three quarters of global net capacity additions because of low and falling prices. This provides an opportunity for Japan to make large reductions in emissions while also reducing its dependence on energy imports. This study shows that Japan has 14 times more solar and offshore wind resources than needed to supply 100% renewable electricity. A 40 year hourly energy balance model is presented of Japan's electricity system using historical data. Pumped hydro energy storage, high voltage interconnection and dispatchable capacity (hydro, biomass and hydrogen energy) are included to balance variable generation and demand. Differential evolution is used to find the least-cost solution under various constraints. The levelized cost of electricity is found to be USD 86 per MWh for a PV-dominated system, and USD 110 per MWh for a wind-dominated system. These costs can be compared with the average system prices on the spot market in Japan of USD 102 per MWh. In summary, Japan can be self-sufficient for electricity supply at competitive costs.      
### 30.Observation of coherent perfect absorption at an exceptional point  [ :arrow_down: ](https://arxiv.org/pdf/2109.08353.pdf)
>  The past few years have witnessed growing interests in exceptional points (EPs) in various domains, including photonics, acoustics and electronics. However, EPs have mainly been realized based on the degeneracy of resonances of physical systems; distinct degeneracies occur relating to the absorption properties of waves, with distinct physical manifestations. Here we demonstrate this physically different kind of exceptional point, by engineering degeneracies in the absorption spectrum of optical microcavities with dissipation. We experimentally distinguish the conditions to realize a resonant EP and an absorbing EP. Furthermore, when the optical loss is optimized to achieve perfect absorption at such an EP, we observe an anomalously broadened lineshape in the absorption spectra, as predicted by theory. The distinct scattering properties enabled by this type of EP creates new opportunities for both the fundamental study and applications of non-Hermitian singularities.      
### 31.Robust Control Under Uncertainty via Bounded Rationality and Differential Privacy  [ :arrow_down: ](https://arxiv.org/pdf/2109.08262.pdf)
>  The rapid development of affordable and compact high-fidelity sensors (e.g., cameras and LIDAR) allows robots to construct detailed estimates of their states and environments. However, the availability of such rich sensor information introduces two technical challenges: (i) the lack of analytic sensing models, which makes it difficult to design controllers that are robust to sensor failures, and (ii) the computational expense of processing the high-dimensional sensor information in real time. This paper addresses these challenges using the theory of differential privacy, which allows us to (i) design controllers with bounded sensitivity to errors in state estimates, and (ii) bound the amount of state information used for control (i.e., to impose bounded rationality). The resulting framework approximates the separation principle and allows us to derive an upper-bound on the cost incurred with a faulty state estimator in terms of three quantities: the cost incurred using a perfect state estimator, the magnitude of state estimation errors, and the level of differential privacy. We demonstrate the efficacy of our framework numerically on different robotics problems, including nonlinear system stabilization and motion planning.      
### 32.Optimal Partitioning of Non-Convex Environments for Minimum Turn Coverage Planning  [ :arrow_down: ](https://arxiv.org/pdf/2109.08185.pdf)
>  In this paper, we tackle the problem of generating a turn-minimizing coverage plan for a robot operating in an indoor environment. In coverage planning, the number of turns in the generated path affects the time to cover the environment and the quality of coverage, e.g. tools like cameras and cleaning attachments commonly have poor performance around turns. In many existing turn-minimizing coverage methods, the environment is partitioned into the least number of ranks, which are non-intersecting rectangles of width equal to the robot's tool width. This partitioning problem is typically solved using heuristics that do not guarantee optimality. In this work, we propose a linear programming (LP) approach to partition the environment into the least number of axis-parallel (horizontal and vertical) ranks with the goal of minimizing the number of turns taken by the robot. We prove that our LP method solves this problem optimally and in polynomial time. We then generate coverage plans for a set of indoor environments using the proposed LP method and compare the results against that of a state-of-the-art coverage approach.      
### 33.Single-target mineral detection with site-specific endmember extraction for survey points identification: A case study of Jaffna, Sri Lanka  [ :arrow_down: ](https://arxiv.org/pdf/2109.08143.pdf)
>  As field surveys used for manual lithological mapping are costly and time-consuming, digital lithological mapping (DLM) that utilizes remotely sensed spectral imaging provides a viable and economical alternative. Generally, DLM has been performed using spectral imaging with the use of laboratory-generated generic endmember signatures. To that end, this paper proposes generating a single-target abundance mineral map for DLM, where the generated map can further be used as a guide for the selection or avoidance of a field survey. For that, a stochastic cancellation-based methodology was used to generate a site-specific endemic signature for the mineral in concern to reduce the inclusive nature otherwise present in DLM. Furthermore, a soil pixel alignment strategy to visualize the relative purity level of the target mineral has been introduced in the proposed work. Then, for the method validation, mapping of limestone deposits in the Jaffna peninsula of Sri Lanka was conducted as the case study using satellite-based spectral imaging as the input. It was observed that despite the low signal-to-noise ratio of the input hyperspectral data the proposed methodology was able to robustly extract the rich information contained in the input data. Further, a field survey was conducted to collect soil samples of four sites chosen by the proposed DLM from the Jaffna peninsula as an algorithm validation and to demonstrate the application of the proposed solution. The proposed abundance threshold of 0.1 coincided with the industrial standard X-ray diffraction (XRD) threshold of 5% for the mineral presence. The results of the XRD test validated the use of the algorithm in the selection of sites to be surveyed, hence could avoid conducting a costly field survey on the assumption of the existence of a mineral.      
