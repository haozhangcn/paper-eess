# ArXiv eess --Thu, 23 Sep 2021
### 1.Nonlinear Attitude Estimation Using Intermittent Linear Velocity and Vector Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2109.10872.pdf)
>  This paper investigates the problem of continuous attitude estimation on $SO(3)$ using continuous angular velocity and linear acceleration measurements as well as intermittent linear velocity and inertial vector measurements. First, we propose a nonlinear observer for the case where all the measurements are continuous and almost global asymptotic stability (AGAS) is shown using the notion of almost global input-to-state stability (ISS) on manifolds. Thereafter, a hybrid attitude observer, with AGAS guarantees, is proposed in terms of intermittent linear velocity and vector measurements. Numerical simulation results are presented to illustrate the performance of the proposed hybrid observer.      
### 2.Reliable Linearized Phase Retrieval for Near-Field Antenna Measurements with Truncated Measurement Surfaces  [ :arrow_down: ](https://arxiv.org/pdf/2109.10864.pdf)
>  Most methods tackling the phase retrieval problem of magnitude-only antenna measurements suffer from unrealistic sampling requirements, from unfeasible computational complexities, and, most severely, from the lacking reliability of nonlinear and nonconvex formulations. As an alternative, we propose a partially coherent (PC) multi-probe measurement technique and an associated linear reconstruction method which mitigate all these issues. Hence, reliable and accurate phase retrieval can be achieved in near-field far-field transformations (NFFFTs). In particular, we resolve the issues related to open measurement surfaces (as they may emerge in drone-based measurement setups) and we highlight the importance of considering the measurement setup and the phaseless NFFFT simultaneously. Specifically, the influence of special multi-probe arrangements on the reconstruction quality of PC solvers is shown.      
### 3.DVC-P: Deep Video Compression with Perceptual Optimizations  [ :arrow_down: ](https://arxiv.org/pdf/2109.10849.pdf)
>  Recent years have witnessed the significant development of learning-based video compression methods, which aim at optimizing objective or perceptual quality and bit rates. In this paper, we introduce deep video compression with perceptual optimizations (DVC-P), which aims at increasing perceptual quality of decoded videos. Our proposed DVC-P is based on Deep Video Compression (DVC) network, but improves it with perceptual optimizations. Specifically, a discriminator network and a mixed loss are employed to help our network trade off among distortion, perception and rate. Furthermore, nearest-neighbor interpolation is used to eliminate checkerboard artifacts which can appear in sequences encoded with DVC frameworks. Thanks to these two improvements, the perceptual quality of decoded sequences is improved. Experimental results demonstrate that, compared with the baseline DVC, our proposed method can generate videos with higher perceptual quality achieving 12.27% reduction in a perceptual BD-rate equivalent, on average.      
### 4.Formulations and Approximations of the Branch Flow Model for Mesh Power Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.10819.pdf)
>  The formulations and approximations of the branch flow model for mesh power networks (Mesh-BranchFlow) are given in this paper. Using different sets of the power flow equations, six formats of the exact Mesh-BranchFlow model are listed. These six formats are mathematically equivalent with each other. Linear approximation and second-order cone programming (SOCP) are then used to derive the six formats of the convex Mesh-BranchFlow model. The branch ampacity constraints considering the shunt conductance and capacitance of the transmission line $\Pi$-model are derived. The key foundation of deriving the ampacity constraints is the correct interpretation of the physical meaning of the transmission line $\Pi$-model. An exact linear expression of the ampacity constraints of the power loss variable is derived. The applications of the Mesh-BranchFlow model in deriving twelve formats of the exact optimal power flow (OPF) model and twelve formats of the approximate OPF model are formulated and analyzed. Using the Julia programming language, the extensive numerical investigations of all formats of the OPF models show the accuracy and computational efficiency of the Mesh-BranchFlow model. A penalty function based approximation gap reduction method is finally proposed and numerically validated to improve the AC-feasibility of the approximate Mesh-BranchFlow model.      
### 5.Stability Assessment for Multi-Infeed Grid-Connected VSCs Modeled in the Admittance Matrix Form  [ :arrow_down: ](https://arxiv.org/pdf/2109.10780.pdf)
>  The increasing use of power electronics converters to integrate renewable energy sources has been subject of concern due to the resonance oscillatory phenomena caused by their interaction with poorly damped AC networks. Early studies are focused to assess the controller influence of a single converter connected to simple networks, and they are no longer representative for existing systems. Lately, studies of multi-infeed grid-connected converters are of particular interest, and their main aim is to apply traditional criteria and identify their difficulties in the stability assessment. An extension of traditional criteria is commonly proposed as a result of these analysis, but they can be burdensome for large and complex power systems. The present work addresses this issue by proposing a simple criterion to assess the stability of large power systems with high-penetration of power converters. The criterion has its origin in the mode analysis and positive-net damping stability criteria, and it addresses the stability in the frequency domain by studying the eigenvalues magnitude and real component of dynamic models in the admittance matrix form. Its effectiveness is tested in two case studies developed in Matlab/Simulink which compare it with traditionally criteria, proving its simplicity.      
### 6.Stigmergy-based collision-avoidance algorithm for self-organising swarms  [ :arrow_down: ](https://arxiv.org/pdf/2109.10761.pdf)
>  Real-time multi-agent collision-avoidance algorithms comprise a key enabling technology for the practical use of self-organising swarms of drones. This paper proposes a decentralised reciprocal collision-avoidance algorithm, which is based on stigmergy and scalable. The algorithm is computationally inexpensive, based on the gradient of the locally measured dynamic cumulative signal strength field which results from the signals emitted by the swarm. The signal strength acts as a repulsor on each drone, which then tends to steer away from the noisiest regions (cluttered environment), thus avoiding collisions. The magnitudes of these repulsive forces can be tuned to control the relative importance assigned to collision avoidance with respect to the other phenomena affecting the agent's dynamics. We carried out numerical experiments on a self-organising swarm of drones aimed at fighting wildfires autonomously. As expected, it has been found that the collision rate can be reduced either by decreasing the cruise speed of the agents and/or by increasing the sampling frequency of the global signal strength field. A convenient by-product of the proposed collision-avoidance algorithm is that it helps maintain diversity in the swarm, thus enhancing exploration.      
### 7.Optimal Operation of a Hydrogen-based Building Multi-Energy System Based on Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.10754.pdf)
>  Since hydrogen has many advantages (e.g., free pollution, extensive sources, convenient storage and transportation), hydrogen-based multi-energy systems (HMESs) have received wide attention. However, existing works on the optimal operation of HMESs neglect building thermal dynamics, which means that the flexibility of building thermal loads can not be utilized for reducing system operation cost. In this paper, we investigate an optimal operation problem of an HMES with the consideration of building thermal dynamics. Specifically, we first formulate an expected operational cost minimization problem related to an HMES. Due to the existence of uncertain parameters, inexplicit building thermal dynamics models, temporally coupled operational constraints related to three kinds of energy storage systems and indoor temperatures, as well as the coupling between electric energy subsystems and thermal energy subsystems, it is challenging to solve the formulated problem. To overcome the challenge, we reformulate the problem as a Markov game and propose an energy management algorithm to solve it based on multi-agent discrete actor-critic with rules (MADACR). Note that the proposed algorithm does not require any prior knowledge of uncertain parameters, parameter prediction, and explicit building thermal dynamics model. Simulation results based on real-world traces show the effectiveness of the proposed algorithm.      
### 8.Automatic Plane Adjustment of Orthopedic Intra-operative Flat Panel Detector CT-Volumes  [ :arrow_down: ](https://arxiv.org/pdf/2109.10731.pdf)
>  Purpose <br>3D acquisitions are often acquired to assess the result in orthopedic trauma surgery. With a mobile C-Arm system, these acquisitions can be performed intra-operatively. That reduces the number of required revision surgeries. However, due to the operation room setup, the acquisitions typically cannot be performed such that the acquired volumes are aligned to the anatomical regions. Thus, the multiplanar reconstructed (MPR) planes need to be adjusted manually during the review of the volume. In this paper, we present a detailed study of multi-task learning (MTL) regression networks to estimate the parameters of the MPR planes. <br>Approach <br>First, various mathematical descriptions for rotation, including Euler angle, quaternion, and matrix representation, are revised. Then, three different MTL network architectures based on the PoseNet are compared with a single task learning network. <br>Results <br>Using a matrix description rather than the Euler angle description, the accuracy of the regressed normals improves from $7.7^{\circ}$ to $7.3^{\circ}$ in the mean value for single anatomies. The multi-head approach improves the regression of the plane position from $7.4mm$ to $6.1mm$, while the orientation does not benefit from this approach. <br>Conclusions <br>The results show that a multi-head approach can lead to slightly better results than the individual tasks networks. The most important benefit of the MTL approach is that it is a single network for standard plane regression for all body regions with a reduced number of stored parameters.      
### 9.Input-Output History Feedback Controller for Encrypted Control with Leveled Fully Homomorphic Encryption  [ :arrow_down: ](https://arxiv.org/pdf/2109.10718.pdf)
>  Protecting the parameters, states, and input/output signals of a dynamic controller is essential for securely outsourcing its computation to an untrusted third party. Although a fully homomorphic encryption scheme allows the evaluation of controller operations with encrypted data, an encrypted dynamic controller with the encryption scheme destabilizes a closed-loop system or degrades the control performance due to overflow. This paper presents a novel controller representation based on input-output history data to implement an encrypted dynamic controller that operates without destabilization and performance degradation. An algorithm for efficient encrypted control computation is also proposed using single instruction/multiple data operations based on a batching technique. Furthermore, this study analyzes the stability and performance degradation of a closed-loop system caused by the effects of controller encryption. A numerical simulation demonstrates the feasibility of the proposed encrypted control scheme, which inherits the control performance of the original controller at a sufficient level.      
### 10.A generic fixed-point iteration-based hierarchical control design: Application to a cryogenic process  [ :arrow_down: ](https://arxiv.org/pdf/2109.10717.pdf)
>  This paper presents an extension of a recently proposed hierarchical control framework applied to a cryogenic system. While in the previous work, each sub-system in the decomposition needed to show at least one component of the control input, in the present contribution, this condition is removed enabling a higher flexibility in the definition of the decomposition graph. The impact of this extended flexibility on the computation time is shown using the same cryogenic station where a decomposition in four sub-system is made possible (instead of two in the previous setting).      
### 11.Self-Training Based Unsupervised Cross-Modality Domain Adaptation for Vestibular Schwannoma and Cochlea Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.10674.pdf)
>  With the advances of deep learning, many medical image segmentation studies achieve human-level performance when in fully supervised condition. However, it is extremely expensive to acquire annotation on every data in medical fields, especially on magnetic resonance images (MRI) that comprise many different contrasts. Unsupervised methods can alleviate this problem; however, the performance drop is inevitable compared to fully supervised methods. In this work, we propose a self-training based unsupervised-learning framework that performs automatic segmentation of Vestibular Schwannoma (VS) and cochlea on high-resolution T2 scans. Our method consists of 4 main stages: 1) VS-preserving contrast conversion from contrast-enhanced T1 scan to high-resolution T2 scan, 2) training segmentation on generated T2 scans with annotations on T1 scans, and 3) Inferring pseudo-labels on non-annotated real T2 scans, and 4) boosting the generalizability of VS and cochlea segmentation by training with combined data (i.e., real T2 scans with pseudo-labels and generated T2 scans with true annotations). Our method showed mean Dice score and Average Symmetric Surface Distance (ASSD) of 0.8570 (0.0705) and 0.4970 (0.3391) for VS, 0.8446 (0.0211) and 0.1513 (0.0314) for Cochlea on CrossMoDA2021 challenge validation phase leaderboard, outperforming most other approaches.      
### 12.Deep Learning Model for Demodulation Reference Signal based Channel Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2109.10667.pdf)
>  In this paper, we propose a deep learning model for Demodulation Reference Signal (DMRS) based channel estimation task. Specifically, a novel Denoise, Linear interpolation and Refine (DLR) pipeline is proposed to mitigate the noise propagation problem during channel information interpolation and to restore the nonlinear variation of wireless channel over time. At the same time, the Small-norm Sample Cost-sensitive (SSC) learning method is proposed to equalize the qualities of channel estimation under different kinds of wireless environments and improve the channel estimation reliability. The effectiveness of the propose DLR-SSC model is verified on WAIC Dataset. Compared with the well know ChannelNet channel estimation model, our DLR-SSC model reduced normalized mean square error (NMSE) by 27.2dB, 22.4dB and 16.8dB respectively at 0dB, 10dB, and 20dB SNR. The proposed model has won the second place in the 2nd Wireless Communication Artificial Intelligence Competition (WAIC). The code is about to open source.      
### 13.Uncertainty-Aware Training for Cardiac Resynchronisation Therapy Response Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2109.10641.pdf)
>  Evaluation of predictive deep learning (DL) models beyond conventional performance metrics has become increasingly important for applications in sensitive environments like healthcare. Such models might have the capability to encode and analyse large sets of data but they often lack comprehensive interpretability methods, preventing clinical trust in predictive outcomes. Quantifying uncertainty of a prediction is one way to provide such interpretability and promote trust. However, relatively little attention has been paid to how to include such requirements into the training of the model. In this paper we: (i) quantify the data (aleatoric) and model (epistemic) uncertainty of a DL model for Cardiac Resynchronisation Therapy response prediction from cardiac magnetic resonance images, and (ii) propose and perform a preliminary investigation of an uncertainty-aware loss function that can be used to retrain an existing DL image-based classification model to encourage confidence in correct predictions and reduce confidence in incorrect predictions. Our initial results are promising, showing a significant increase in the (epistemic) confidence of true positive predictions, with some evidence of a reduction in false negative confidence.      
### 14.Polarimetric Room Electromagnetics  [ :arrow_down: ](https://arxiv.org/pdf/2109.10625.pdf)
>  A polarimetric model for the power delay spectrum for inroom communication is proposed. The proposed model describes the gradual depolarization of the signal with delay. The model is based on the theory of room electromagnetics, specifically the mirror source approach, which is straightforwardly generalized to the polarimetric case. Compared to the previously known unipolarized room electromagnetic models, which are contained as a special case, the new model holds one additional parameter describing the polarization leakage per wall bounce. The proposed model is fitted to polarimetric mm-wave measurement data.      
### 15.Model Reference Adaptive Control with Linear-like Closed-loop Behavior  [ :arrow_down: ](https://arxiv.org/pdf/2109.10611.pdf)
>  It is typically proven in adaptive control that asymptotic stabilization and tracking holds, and that at best a bounded-noise bounded-state property is proven. Recently, it has been shown in both the pole-placement control and the $d$-step ahead control settings that if, as part of the adaptive controller, a parameter estimator based on the original projection algorithm is used and the parameter estimates are restricted to a convex set, then the closed-loop system experiences linear-like behavior: exponential stability, a bounded gain on the noise in every $p$-norm, and a convolution bound on the exogenous inputs; this can be leveraged to provide tolerance to unmodelled dynamics and plant parameter time-variation. In this paper, we extend the approach to the more general Model Reference Adaptive Control (MRAC) problem and demonstrate that we achieve the same desirable linear-like closed-loop properties.      
### 16.Efficient Context-Aware Network for Abdominal Multi-organ Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.10601.pdf)
>  The contextual information, presented in abdominal CT scan, is relative consistent. In order to make full use of the overall 3D context, we develop a whole-volumebased coarse-to-fine framework for efficient and effective abdominal multi-organ segmentation. We propose a new efficientSegNet network, which is composed of encoder, decoder and context block. For the decoder module, anisotropic convolution with a k*k*1 intra-slice convolution and a 1*1*k inter-slice convolution, is designed to reduce the computation burden. For the context block, we propose strip pooling module to capture anisotropic and long-range contextual information, which exists in abdominal scene. Quantitative evaluation on the FLARE2021 validation cases, this method achieves the average dice similarity coefficient (DSC) of 0.895 and average normalized surface distance (NSD) of 0.775. The average running time is 9.8 s per case in inference phase, and maximum used GPU memory is 1017 MB.      
### 17.Deep Augmented MUSIC Algorithm for Data-Driven DoA Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2109.10581.pdf)
>  Direction of arrival (DoA) estimation is a crucial task in sensor array signal processing, giving rise to various successful model-based (MB) algorithms as well as recently developed data-driven (DD) methods. This paper introduces a new hybrid MB/DD DoA estimation architecture, based on the classical multiple signal classification (MUSIC) algorithm. Our approach augments crucial aspects of the original MUSIC structure with specifically designed neural architectures, allowing it to overcome certain limitations of the purely MB method, such as its inability to successfully localize coherent sources. The deep augmented MUSIC algorithm is shown to outperform its unaltered version with a superior resolution.      
### 18.Event-triggered observer design for linear systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.10576.pdf)
>  We present an event-triggered observer design for linear time-invariant systems, where the measured output is sent to the observer only when a triggering condition is satisfied. We proceed by emulation and we first construct a continuous-time Luenberger observer. We then propose a dynamic rule to trigger transmissions, which only depends on the plant output and an auxiliary scalar state variable. The overall system is modeled as a hybrid system, for which a jump corresponds to an output transmission. We show that the proposed event-triggered observer guarantees global practical asymptotic stability for the estimation error dynamics. Moreover, under mild boundedness conditions on the plant state and its input, we prove that there exists a uniform strictly positive minimum inter-event time between any two consecutive transmissions, guaranteeing that the system does not exhibit Zeno solutions. Finally, the proposed approach is applied to a numerical case study of a lithium-ion battery.      
### 19.On the Comparison of Single-Carrier vs. Digital Multi-Carrier Signaling for Long-Haul Transmission of Probabilistically Shaped Constellation Formats  [ :arrow_down: ](https://arxiv.org/pdf/2109.10553.pdf)
>  We report on theoretical and experimental investigations of the nonlinear tolerance of single carrier and digital multicarrier approaches with probabilistically shaped constellations. Experimental transmission of PCS16QAM is assessed at 120 GBd over an ultra-long-haul distance.      
### 20.Incorporating Data Uncertainty in Object Tracking Algorithms  [ :arrow_down: ](https://arxiv.org/pdf/2109.10521.pdf)
>  Methodologies for incorporating the uncertainties characteristic of data-driven object detectors into object tracking algorithms are explored. Object tracking methods rely on measurement error models, typically in the form of measurement noise, false positive rates, and missed detection rates. Each of these quantities, in general, can be dependent on object or measurement location. However, for detections generated from neural-network processed camera inputs, these measurement error statistics are not sufficient to represent the primary source of errors, namely a dissimilarity between run-time sensor input and the training data upon which the detector was trained. To this end, we investigate incorporating data uncertainty into object tracking methods such as to improve the ability to track objects, and particularly those which out-of-distribution w.r.t. training data. The proposed methodologies are validated on an object tracking benchmark as well on experiments with a real autonomous aircraft.      
### 21.Sensor-Based Satellite IoT for Early Wildfire Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.10505.pdf)
>  Frequent and severe wildfires have been observed lately on a global scale. Wildfires not only threaten lives and properties, but also pose negative environmental impacts that transcend national boundaries (e.g., greenhouse gas emission and global warming). Thus, early wildfire detection with timely feedback is much needed. We propose to use the emerging beyond fifth-generation (B5G) and sixth-generation (6G) satellite Internet of Things (IoT) communication technology to enable massive sensor deployment for wildfire detection. We propose wildfire and carbon emission models that take into account real environmental data including wind speed, soil wetness, and biomass, to simulate the fire spreading process and quantify the fire burning areas, carbon emissions, and economical benefits of the proposed system against the backdrop of recent California wildfires. We also conduct a satellite IoT feasibility check by analyzing the satellite link budget. Future research directions to further illustrate the promise of the proposed system are discussed.      
### 22.Joint Optical Neuroimaging Denoising with Semantic Tasks  [ :arrow_down: ](https://arxiv.org/pdf/2109.10499.pdf)
>  Optical neuroimaging is a vital tool for understanding the brain structure and the connection between regions and nuclei. However, the image noise introduced in the sample preparation and the imaging system hinders the extraction of the possible knowlege from the dataset, thus denoising for the optical neuroimaging is usually necessary. The supervised denoisng methods often outperform the unsupervised ones, but the training of the supervised denoising models needs the corresponding clean labels, which is not always avaiable due to the high labeling cost. On the other hand, those semantic labels, such as the located soma positions, the reconstructed neuronal fibers, and the nuclei segmentation result, are generally available and accumulated from everyday neuroscience research. This work connects a supervised denoising and a semantic segmentation model together to form a end-to-end model, which can make use of the semantic labels while still provides a denoised image as an intermediate product. We use both the supervised and the self-supervised models for the denoising and introduce a new cost term for the joint denoising and the segmentation setup. We test the proposed approach on both the synthetic data and the real-world data, including the optical neuroimaing dataset and the electron microscope dataset. The result shows that the joint denoising result outperforms the one using the denoising method alone and the joint model benefits the segmentation and other downstream task as well.      
### 23.Optimal excitation and measurement pattern for cascade networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.10459.pdf)
>  This work deals with accuracy analysis of dynamical systems interconnected in a cascade structure. For a cascade network there are a number of experimental settings for which the dynamic systems within the network can be identified. We study the problem of choosing which excitation and measurement pattern delivers the most accurate parameter estimates for the whole network. The optimal experiment is based on the accuracy assessed through the asymptotic covariance matrix of the prediction error method, while the cost criterion is the number of excitations and measurements. We develop theoretical results under the assumptions that all dynamic systems are equal and with equal signal-to-noise ratio throughout the network. We show that there are experimental settings which result in equal overall precision and that there is an excitation and measurement pattern that yields more accurate results than others. From these results a guideline based on the topology of the network emerges for the choice of the experimental setting. We provide numerical results which attest that the principles behind this guideline are also valid for more general situations.      
### 24.Towards cyber-physical systems robust to communication delays: A differential game approach  [ :arrow_down: ](https://arxiv.org/pdf/2109.10450.pdf)
>  Collaboration between interconnected cyber-physical systems is becoming increasingly pervasive. Time-delays in communication channels between such systems are known to induce catastrophic failure modes, like high frequency oscillations in robotic manipulators in bilateral teleoperation or string instability in platoons of autonomous vehicles. This paper considers nonlinear time-delay systems representing coupled robotic agents, and proposes controllers that are robust to time-varying communication delays. We introduce approximations that allow the delays to be considered as implicit control inputs themselves, and formulate the problem as a zero-sum differential game between the stabilizing controllers and the delays acting adversarially. The ensuing optimal control law is finally compared to known results from Lyapunov-Krasovskii based approaches via numerical experiments.      
### 25.ICEV dismantling or recycling on a challenging environment  [ :arrow_down: ](https://arxiv.org/pdf/2109.10435.pdf)
>  Nowadays Sustainability is a huge issue. Sustainability deals with the need for the protection of the natural environment and ecosystems health and requires innovation and commitment with the future. This manuscript uses the infinite servers with Poisson arrivals queue system, modelling Internal Combustion Engine Vehicles (ICEV), normally cars but not only, which turn idle when conventional energy becomes scarce, or a new status quo is required. In such a case, they are recycled, becoming either EV-Electric Vehicles or HEV-Hybrid Electric Vehicles or FCEV-Fuel Cell Electric Vehicles, or are dismantled (DV-Dismantled Vehicles). Our model shows that when the rhythm ICEV become EV, HEV, FCEV and DV is greater than the rate at which they get idle the system tends to balance. In a cost-benefit analysis perspective, there are minimum benefits above which, both dismantling and recycling, are interesting. Additionally, the most interesting is the one for which the minimum benefit is the least.      
### 26.Non-intrusive Balancing Transformation of Highly Stiff Systems with Lightly-damped Impulse Response  [ :arrow_down: ](https://arxiv.org/pdf/2109.10408.pdf)
>  Balanced truncation (BT) is a model reduction method that utilizes a coordinate transformation to retain eigen-directions that are highly observable and reachable. To address realizability and scalability of BT applied to highly stiff and lightly-damped systems, a non-intrusive data-driven method is developed for balancing discrete-time systems via the eigensystem realization algorithm (ERA). The advantage of ERA for balancing transformation makes full-state outputs tractable. Further, ERA enables balancing despite stiffness, by eliminating computation of balancing modes and adjoint simulations. As a demonstrative example, we create balanced ROMs for a one-dimensional reactive flow with pressure forcing, where the stiffness introduced by the chemical source term is extreme (condition number $10^{13}$), preventing analytical implementation of BT. We investigate the performance of ROMs in prediction of dynamics with unseen forcing inputs and demonstrate stability and accuracy of balanced ROMs in truly predictive scenarios whereas without ERA, POD-Galerkin and Least-squares Petrov-Galerkin projections fail to represent the true dynamics. We show that after the initial transients under unit impulse forcing, the system undergoes lightly-damped oscillations, which magnifies the influence of sampling properties on predictive performance of the balanced ROMs. The importance of proper sampling is established via sensitivity analysis in a predictive setting.      
### 27.Digital Signal Processing Using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.10404.pdf)
>  Currently there is great interest in the utility of deep neural networks (DNNs) for the physical layer of radio frequency (RF) communications. In this manuscript, we describe a custom DNN specially designed to solve problems in the RF domain. Our model leverages the mechanisms of feature extraction and attention through the combination of an autoencoder convolutional network with a transformer network, to accomplish several important communications network and digital signals processing (DSP) tasks. We also present a new open dataset and physical data augmentation model that enables training of DNNs that can perform automatic modulation classification, infer and correct transmission channel effects, and directly demodulate baseband RF signals.      
### 28.Frequency-based Ultrasonic Backscatter Modulation for Passive Sensing Applications  [ :arrow_down: ](https://arxiv.org/pdf/2109.10398.pdf)
>  Ultrasonic backscatter communication has gained popularity in recent years where piezoceramic resonators are used as acoustic antennas. Currently, backscatter communication revolves around the amplitude modulation of the echo signal by the sensor, which can be modeled as a variable shunt impedance. Amplitude based sensing is prone to high levels of noise, motion artifacts, and has low efficiency with respect to power usage and bandwidth. To overcome these shortcomings, we present here, a frequency-based sensing method for ultrasonic backscatter communication. This system exhibits a frequency shift in the ultrasonic sensor's parallel resonance when its shunt capacitance is varied, similar to an inductive near field link. The concept is simulated using an equivalent end-to-end model in SPICE that matches well with the experimental observation. To monitor the resonance frequency shift in the experiments a variety of methods are employed including ringdown measurements, chirp spectroscopy, bode analysis and phase locked loop. All the methods provde substantial proof for feasibility of frequency-based sensing in ultrasonic backscatter communication enabling passive sensor motes for sensing applications.      
### 29.An Ultra-Fast Method for Simulation of Realistic Ultrasound Images  [ :arrow_down: ](https://arxiv.org/pdf/2109.10353.pdf)
>  Convolutional neural networks (CNNs) have attracted a rapidly growing interest in a variety of different processing tasks in the medical ultrasound community. However, the performance of CNNs is highly reliant on both the amount and fidelity of the training data. Therefore, scarce data is almost always a concern, particularly in the medical field, where clinical data is not easily accessible. The utilization of synthetic data is a popular approach to address this challenge. However, but simulating a large number of images using packages such as Field II is time-consuming, and the distribution of simulated images is far from that of the real images. Herein, we introduce a novel ultra-fast ultrasound image simulation method based on the Fourier transform and evaluate its performance in a lesion segmentation task. We demonstrate that data augmentation using the images generated by the proposed method substantially outperforms Field II in terms of Dice similarity coefficient, while the simulation is almost 36000 times faster (both on CPU).      
### 30.Enabling variable high spatial resolution retrieval from a long pulse BOTDA sensor  [ :arrow_down: ](https://arxiv.org/pdf/2109.10349.pdf)
>  In the field of Internet of Things, there is an urgent need for sensors with large-scale sensing capability for scenarios such as intelligent monitoring of production lines and urban infrastructure. Brillouin optical time domain analysis (BOTDA) sensors, which can monitor thousands of continuous points simultaneously, show great advantages in these applications. We propose a convolutional neural network (CNN) to process the data of conventional Brillouin optical time domain analysis (BOTDA) sensors, which achieves unprecedented performance improvement that allows to directly retrieve higher spatial resolution (SR) from the sensing system that use long pump pulses. By using the simulated Brillouin gain spectrums (BGSs) as the CNN input and the corresponding high SR BFS as the output target, the trained CNN is able to obtain a SR higher than the theoretical value determined by the pump pulse width. In the experiment, the CNN accurately retrieves 0.5-m hotspots from the measured BGS with pump pulses from 20 to 50 ns, and the acquired BFS is in great agreement with 45/40 ns differential pulse-width pair (DPP) measurement results. Compared with the DPP technique, the proposed CNN demonstrates a 2-fold improvement in BFS uncertainty with only half the measurement time. In addition, by changing the training datasets, the proposed CNN can obtain tunable high SR retrieval based on conventional BOTDA sensors that use long pulses without any requirement of hardware modifications. The proposed data post-processing approach paves the way to enable novel high spatial resolution BOTDA sensors, which brings substantial improvement over the state-of-the-art techniques in terms of system complexity, measurement time and reliability, etc.      
### 31.ProvLet: A Provenance Management Service for Long Tail Microscopy Data  [ :arrow_down: ](https://arxiv.org/pdf/2109.10897.pdf)
>  Provenance management must be present to enhance the overall security and reliability of long-tail microscopy (LTM) data management systems. However, there are challenges in provenance for domains with LTM data. The provenance data need to be collected more frequently, which increases system overheads (in terms of computation and storage) and results in scalability issues. Moreover, in most scientific application domains a provenance solution must consider network-related events as well. Therefore, provenance data in LTM data management systems are highly diverse and must be organized and processed carefully. In this paper, we introduce a novel provenance service, called ProvLet, to collect, distribute, analyze, and visualize provenance data in LTM data management systems. This means (1) we address how to filter and store the desired transactions on disk; (2) we consider a data organization model at higher level data abstractions, suitable for step-by-step scientific experiments, such as datasets and collections, and develop provenance algorithms over these data abstractions, rather than solutions considering low-level abstractions such as files and folders. (3) We utilize ProvLet's log files and visualize provenance information for further forensics explorations. The validation of ProvLet with actual long tail microscopy data, collected over a period of six years, shows a provenance service that yields a low system overhead and enables scalability.      
### 32.A Transportation Digital-Twin Approach for Adaptive Traffic Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.10863.pdf)
>  A transportation digital twin represents a digital version of a transportation physical object or process, such as a traffic signal controller, and thereby a two-way real-time data exchange between the physical twin and digital twin. This paper introduces a digital twin approach for adaptive traffic signal control (ATSC) to improve a traveler's driving experience by reducing and redistributing waiting time at an intersection. While an ATSC combined with a connected vehicle concept can reduce waiting time at an intersection and improve travel time in a signalized corridor, it is nearly impossible to reduce traffic delay for congested traffic conditions. To remedy this defect of the traditional ATCS with connected vehicle data, we have developed a digital twin-based ATSC (DT-based ATSC) that considers the waiting time of approaching vehicles towards a subject intersection along with the waiting time of those vehicles at the immediate upstream intersection. We conducted a case study using a microscopic traffic simulation, Simulation of Urban Mobility (SUMO), by developing a digital replica of a roadway network with signalized intersections in an urban setting where vehicle and traffic signal data were collected in real-time. Our analyses reveal that the DT-based ATSC outperforms the connected vehicle-based baseline ATSC in terms of average cumulative waiting time, distribution of drivers' waiting time, and level of services for each approach for different traffic demands and therefore demonstrates our method's superior efficacy.      
### 33.Deep Variational Clustering Framework for Self-labeling of Large-scale Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2109.10777.pdf)
>  We propose a Deep Variational Clustering (DVC) framework for unsupervised representation learning and clustering of large-scale medical images. DVC simultaneously learns the multivariate Gaussian posterior through the probabilistic convolutional encoder and the likelihood distribution with the probabilistic convolutional decoder; and optimizes cluster labels assignment. Here, the learned multivariate Gaussian posterior captures the latent distribution of a large set of unlabeled images. Then, we perform unsupervised clustering on top of the variational latent space using a clustering loss. In this approach, the probabilistic decoder helps to prevent the distortion of data points in the latent space and to preserve the local structure of data generating distribution. The training process can be considered as a self-training process to refine the latent space and simultaneously optimizing cluster assignments iteratively. We evaluated our proposed framework on three public datasets that represented different medical imaging modalities. Our experimental results show that our proposed framework generalizes better across different datasets. It achieves compelling results on several medical imaging benchmarks. Thus, our approach offers potential advantages over conventional deep unsupervised learning in real-world applications. The source code of the method and all the experiments are available publicly at: <a class="link-external link-https" href="https://github.com/csfarzin/DVC" rel="external noopener nofollow">this https URL</a>      
### 34.A Deep Learning Perspective on Connected Automated Vehicle (CAV) Cybersecurity and Threat Intelligence  [ :arrow_down: ](https://arxiv.org/pdf/2109.10763.pdf)
>  The automation and connectivity of CAV inherit most of the cyber-physical vulnerabilities of incumbent technologies such as evolving network architectures, wireless communications, and AI-based automation. This book chapter entails the cyber-physical vulnerabilities and risks that originated in IT, OT, and the physical domains of the CAV ecosystem, eclectic threat landscapes, and threat intelligence. To deal with the security threats in high-speed, high dimensional, multimodal data and assets from eccentric stakeholders of the CAV ecosystem, this chapter presents and analyzes some of the state of art deep learning-based threat intelligence for attack detection. The frontiers in deep learning, namely Meta-Learning and Federated Learning, along with their challenges have been included in the chapter. We have proposed, trained, and tested the deep CNN-LSTM architecture for CAV threat intelligence; assessed and compared the performance of the proposed model against other deep learning algorithms such as DNN, CNN, LSTM. Our results indicate the superiority of the proposed model although DNN and 1d-CNN also achieved more than 99% of accuracy, precision, recall, f1-score, and AUC on the CAV-KDD dataset. The good performance of deep CNN-LSTM comes with the increased model complexity and cumbersome hyperparameters tuning. Still, there are open challenges on deep learning adoption in the CAV cybersecurity paradigm due to lack of properly developed protocols and policies, poorly defined privileges between stakeholders, costlier training, adversarial threats to the model, and poor generalizability of the model under out of data distributions.      
### 35.Constrained multi-agent ergodic area surveying control based on finite element approximation of the potential field  [ :arrow_down: ](https://arxiv.org/pdf/2109.10756.pdf)
>  Heat Equation Driven Area Coverage (HEDAC) is a state-of-the-art multi-agent ergodic motion control guided by a gradient of a potential field. A finite element method is hereby implemented to obtain a solution of Helmholtz partial differential equation, which models the potential field for surveying motion control. This allows us to survey arbitrarily shaped domains and to include obstacles in an elegant and robust manner intrinsic to HEDAC's fundamental idea. For a simple kinematic motion, the obstacles and boundary avoidance constraints are successfully handled by directing the agent motion with the gradient of the potential. However, including additional constraints, such as the minimal clearance dsitance from stationary and moving obstacles and the minimal path curvature radius, requires further alternations of the control algorithm. We introduce a relatively simple yet robust approach for handling these constraints by formulating a straightforward optimization problem based on collision-free escapes route maneuvers. This approach provides a guaranteed collision avoidance mechanism, while being computationally inexpensive as a result of the optimization problem partitioning. The proposed motion control is evaluated in three realistic surveying scenarios simulations, showing the effectiveness of the surveying and the robustness of the control algorithm. Furthermore, potential maneuvering difficulties due to improperly defined surveying scenarios are highlighted and we provide guidelines on how to overpass them. The results are promising and indiacate real-world applicability of proposed constrained multi-agent motion control for autonomous surveying and potentially other HEDAC utilizations.      
### 36.Low-Latency Incremental Text-to-Speech Synthesis with Distilled Context Prediction Network  [ :arrow_down: ](https://arxiv.org/pdf/2109.10724.pdf)
>  Incremental text-to-speech (TTS) synthesis generates utterances in small linguistic units for the sake of real-time and low-latency applications. We previously proposed an incremental TTS method that leverages a large pre-trained language model to take unobserved future context into account without waiting for the subsequent segment. Although this method achieves comparable speech quality to that of a method that waits for the future context, it entails a huge amount of processing for sampling from the language model at each time step. In this paper, we propose an incremental TTS method that directly predicts the unobserved future context with a lightweight model, instead of sampling words from the large-scale language model. We perform knowledge distillation from a GPT2-based context prediction network into a simple recurrent model by minimizing a teacher-student loss defined between the context embedding vectors of those models. Experimental results show that the proposed method requires about ten times less inference time to achieve comparable synthetic speech quality to that of our previous method, and it can perform incremental synthesis much faster than the average speaking speed of human English speakers, demonstrating the availability of our method to real-time applications.      
### 37.Autonomous Blimp Control using Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.10719.pdf)
>  Aerial robot solutions are becoming ubiquitous for an increasing number of tasks. Among the various types of aerial robots, blimps are very well suited to perform long-duration tasks while being energy efficient, relatively silent and safe. To address the blimp navigation and control task, in our recent work, we have developed a software-in-the-loop simulation and a PID-based controller for large blimps in the presence of wind disturbance. However, blimps have a deformable structure and their dynamics are inherently non-linear and time-delayed, often resulting in large trajectory tracking errors. Moreover, the buoyancy of a blimp is constantly changing due to changes in the ambient temperature and pressure. In the present paper, we explore a deep reinforcement learning (DRL) approach to address these issues. We train only in simulation, while keeping conditions as close as possible to the real-world scenario. We derive a compact state representation to reduce the training time and a discrete action space to enforce control smoothness. Our initial results in simulation show a significant potential of DRL in solving the blimp control task and robustness against moderate wind and parameter uncertainty. Extensive experiments are presented to study the robustness of our approach. We also openly provide the source code of our approach.      
### 38.A Quantitative Comparison of Epistemic Uncertainty Maps Applied to Multi-Class Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.10702.pdf)
>  Uncertainty assessment has gained rapid interest in medical image analysis. A popular technique to compute epistemic uncertainty is the Monte-Carlo (MC) dropout technique. From a network with MC dropout and a single input, multiple outputs can be sampled. Various methods can be used to obtain epistemic uncertainty maps from those multiple outputs. In the case of multi-class segmentation, the number of methods is even larger as epistemic uncertainty can be computed voxelwise per class or voxelwise per image. This paper highlights a systematic approach to define and quantitatively compare those methods in two different contexts: class-specific epistemic uncertainty maps (one value per image, voxel and class) and combined epistemic uncertainty maps (one value per image and voxel). We applied this quantitative analysis to a multi-class segmentation of the carotid artery lumen and vessel wall, on a multi-center, multi-scanner, multi-sequence dataset of (MR) images. We validated our analysis over 144 sets of hyperparameters of a model. Our main analysis considers the relationship between the order of the voxels sorted according to their epistemic uncertainty values and the misclassification of the prediction. Under this consideration, the comparison of combined uncertainty maps reveals that the multi-class entropy and the multi-class mutual information statistically out-perform the other combined uncertainty maps under study. In a class-specific scenario, the one-versus-all entropy statistically out-performs the class-wise entropy, the class-wise variance and the one versus all mutual information. The class-wise entropy statistically out-performs the other class-specific uncertainty maps in terms of calibration. We made a python package available to reproduce our analysis on different data and tasks.      
### 39.A Latent Restoring Force Approach to Nonlinear System Identification  [ :arrow_down: ](https://arxiv.org/pdf/2109.10681.pdf)
>  Identification of nonlinear dynamic systems remains a significant challenge across engineering. This work suggests an approach based on Bayesian filtering to extract and identify the contribution of an unknown nonlinear term in the system which can be seen as an alternative viewpoint on restoring force surface type approaches. To achieve this identification, the contribution which is the nonlinear restoring force is modelled, initially, as a Gaussian process in time. That Gaussian process is converted into a state-space model and combined with the linear dynamic component of the system. Then, by inference of the filtering and smoothing distributions, the internal states of the system and the nonlinear restoring force can be extracted. In possession of these states a nonlinear model can be constructed. The approach is demonstrated to be effective in both a simulated case study and on an experimental benchmark dataset.      
### 40.Optimal Control for Linear Networked Control Systems with Information Transmission Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2109.10666.pdf)
>  This paper addresses the problem of robust control of a linear discrete-time system subject to bounded disturbances and to measurement and control budget constraints. <br>Using Q-parameterization and a polytope containment method, we prove that the co-design of an affine feedback controller, a measurement schedule and a control schedule can be exactly formulated as a mixed integer linear program with 2 binary variables per time step. As a consequence, this problem can be solved efficiently, even when an exhaustive search for measurement and control times would have been impossible in a reasonable amount of time.      
### 41.TACTIC: Joint Rate-Distortion-Accuracy Optimisation for Low Bitrate Compression  [ :arrow_down: ](https://arxiv.org/pdf/2109.10658.pdf)
>  We present TACTIC: Task-Aware Compression Through Intelligent Coding. Our lossy compression model learns based on the rate-distortion-accuracy trade-off for a specific task. By considering what information is important for the follow-on problem, the system trades off visual fidelity for good task performance at a low bitrate. When compared against JPEG at the same bitrate, our approach is able to improve the accuracy of ImageNet subset classification by 4.5%. We also demonstrate the applicability of our approach to other problems, providing a 3.4% accuracy and 4.9% mean IoU improvements in performance over task-agnostic compression for semantic segmentation.      
### 42.Locality Matters: A Scalable Value Decomposition Approach for Cooperative Multi-Agent Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.10632.pdf)
>  Cooperative multi-agent reinforcement learning (MARL) faces significant scalability issues due to state and action spaces that are exponentially large in the number of agents. As environments grow in size, effective credit assignment becomes increasingly harder and often results in infeasible learning times. Still, in many real-world settings, there exist simplified underlying dynamics that can be leveraged for more scalable solutions. In this work, we exploit such locality structures effectively whilst maintaining global cooperation. We propose a novel, value-based multi-agent algorithm called LOMAQ, which incorporates local rewards in the Centralized Training Decentralized Execution paradigm. Additionally, we provide a direct reward decomposition method for finding these local rewards when only a global signal is provided. We test our method empirically, showing it scales well compared to other methods, significantly improving performance and convergence speed.      
### 43.Noisy-to-Noisy Voice Conversion Framework with Denoising Model  [ :arrow_down: ](https://arxiv.org/pdf/2109.10608.pdf)
>  In a conventional voice conversion (VC) framework, a VC model is often trained with a clean dataset consisting of speech data carefully recorded and selected by minimizing background interference. However, collecting such a high-quality dataset is expensive and time-consuming. Leveraging crowd-sourced speech data in training is more economical. Moreover, for some real-world VC scenarios such as VC in video and VC-based data augmentation for speech recognition systems, the background sounds themselves are also informative and need to be maintained. In this paper, to explore VC with the flexibility of handling background sounds, we propose a noisy-to-noisy (N2N) VC framework composed of a denoising module and a VC module. With the proposed framework, we can convert the speaker's identity while preserving the background sounds. Both objective and subjective evaluations are conducted, and the results reveal the effectiveness of the proposed framework.      
### 44.Diarisation using Location tracking with agglomerative clustering  [ :arrow_down: ](https://arxiv.org/pdf/2109.10598.pdf)
>  Previous works have shown that spatial location information can be complementary to speaker embeddings for a speaker diarisation task. However, the models used often assume that speakers are fairly stationary throughout a meeting. This paper proposes to relax this assumption, by explicitly modelling the movements of speakers within an Agglomerative Hierarchical Clustering (AHC) diarisation framework. Kalman filters, which track the locations of speakers, are used to compute log-likelihood ratios that contribute to the cluster affinity computations for the AHC merging and stopping decisions. Experiments show that the proposed approach is able to yield improvements on a Microsoft rich meeting transcription task, compared to methods that do not use location information or that make stationarity assumptions.      
### 45.Few-Shot Sound Source Distance Estimation Using Relation Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.10561.pdf)
>  In this paper, we study the performance of few-shot learning, specifically meta learning empowered few-shot relation networks, over classic supervised learning in the problem of sound source distance estimation(SSDE). In previous research on deep supervised SSDE, obtaining low accuracies due to the mismatch between the training data(sound from known environments) and the test data(sound from unknown environments) has almost always been the case. By performing comparative experiments on a sufficient amount of data, we show that the few-shot relation network outperform a classic CNN which is a supervised deep learning approach, and hence it is possible to calibrate a microphone-equipped system, with a few labeled examples of audio recorded in a particular unknown environment to adjust and generalize our classifier to the possible input data and gain higher accuracies.      
### 46.Cramér-Rao bound-informed training of neural networks for quantitative MRI  [ :arrow_down: ](https://arxiv.org/pdf/2109.10535.pdf)
>  Neural networks are increasingly used to estimate parameters in quantitative MRI, in particular in magnetic resonance fingerprinting. Their advantages over the gold standard non-linear least square fitting are their superior speed and their immunity to the non-convexity of many fitting problems. We find, however, that in heterogeneous parameter spaces, i.e. in spaces in which the variance of the estimated parameters varies considerably, good performance is hard to achieve and requires arduous tweaking of the loss function, hyper parameters, and the distribution of the training data in parameter space. Here, we address these issues with a theoretically well-founded loss function: the Cramér-Rao bound (CRB) provides a theoretical lower bound for the variance of an unbiased estimator and we propose to normalize the squared error with respective CRB. With this normalization, we balance the contributions of hard-to-estimate and not-so-hard-to-estimate parameters and areas in parameter space, and avoid a dominance of the former in the overall training loss. Further, the CRB-based loss function equals one for a maximally-efficient unbiased estimator, which we consider the ideal estimator. Hence, the proposed CRB-based loss function provides an absolute evaluation metric. We compare a network trained with the CRB-based loss with a network trained with the commonly used means squared error loss and demonstrate the advantages of the former in numerical, phantom, and in vivo experiments.      
### 47.Rotor Localization and Phase Mapping of Cardiac Excitation Waves using Deep Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.10472.pdf)
>  The analysis of electrical impulse phenomena in cardiac muscle tissue is important for the diagnosis of heart rhythm disorders and other cardiac pathophysiology. Cardiac mapping techniques acquire numerous local temporal measurements and combine them to visualize the spread of electrophysiological wave phenomena across the heart surface. However, low spatial resolutions, sparse measurement locations, noise and other artifacts make it challenging to accurately visualize spatio-temporal activity. For instance, electro-anatomical catheter mapping is severely limited by the sparsity of the measurements and optical mapping is prone to noise and motion artifacts. In the past, several approaches have been proposed to obtain more reliable maps from noisy or sparse mapping data. Here, we demonstrate that deep learning can be used to compute phase maps and detect phase singularities from both noisy and sparse electrical mapping data with high precision and efficiency. The self-supervised deep learning approach is fundamentally different from classical phase mapping techniques. Rather than encoding a phase signal from time-series data, the network instead learns to directly associate short spatio-temporal sequences of electrical data with phase maps and the positions of phase singularities. Using this method, we were able to accurately compute phase maps and locate rotor cores even from extremely sparse and noisy data, generated from both optical mapping experiments and computer simulations. Neural networks are a promising alternative to conventional phase mapping and rotor core localization methods, that could be used in optical mapping studies in basic cardiovascular research as well as in the clinical setting for the analysis of atrial fibrillation.      
### 48.The First Vision For Vitals (V4V) Challenge for Non-Contact Video-Based Physiological Estimation  [ :arrow_down: ](https://arxiv.org/pdf/2109.10471.pdf)
>  Telehealth has the potential to offset the high demand for help during public health emergencies, such as the COVID-19 pandemic. Remote Photoplethysmography (rPPG) - the problem of non-invasively estimating blood volume variations in the microvascular tissue from video - would be well suited for these situations. Over the past few years a number of research groups have made rapid advances in remote PPG methods for estimating heart rate from digital video and obtained impressive results. How these various methods compare in naturalistic conditions, where spontaneous behavior, facial expressions, and illumination changes are present, is relatively unknown. To enable comparisons among alternative methods, the 1st Vision for Vitals Challenge (V4V) presented a novel dataset containing high-resolution videos time-locked with varied physiological signals from a diverse population. In this paper, we outline the evaluation protocol, the data used, and the results. V4V is to be held in conjunction with the 2021 International Conference on Computer Vision.      
### 49.An Audio Synthesis Framework Derived from Industrial Process Control  [ :arrow_down: ](https://arxiv.org/pdf/2109.10455.pdf)
>  Since its conception, digital synthesis has significantly influenced the advancement of music, leading to new genres and production styles. Through existing synthesis techniques, one can recreate naturally occurring sounds as well as generate innovative artificial timbres. However, research in audio technology continues to pursue new methods of synthesizing sounds, keeping the transformation of music constant. This research attempts to formulate the framework of a new synthesis technique by redefining the popular Proportional-Integral-Derivative (PID) algorithm used in feedback-based process control. The framework is then implemented as a Python application to study the available control parameters and their effect on the synthesized output. Further, applications of this technique as an audio signal and LFO generator, including its potentiality as an alternative to FM and Wavetable synthesis techniques, are studied in detail. The research concludes by highlighting some of the imperfections in the current framework and the possible research directions to be considered to address them.      
### 50.Geometric Fabrics: Generalizing Classical Mechanics to Capture the Physics of Behavior  [ :arrow_down: ](https://arxiv.org/pdf/2109.10443.pdf)
>  Classical mechanical systems are central to controller design in energy shaping methods of geometric control. However, their expressivity is limited by position-only metrics and the intimate link between metric and geometry. Recent work on Riemannian Motion Policies (RMPs) has shown that shedding these restrictions results in powerful design tools, but at the expense of theoretical guarantees. In this work, we generalize classical mechanics to what we call geometric fabrics, whose expressivity and theory enable the design of systems that outperform RMPs in practice. Geometric fabrics strictly generalize classical mechanics forming a new physics of behavior by first generalizing them to Finsler geometries and then explicitly bending them to shape their behavior. We develop the theory of fabrics and present both a collection of controlled experiments examining their theoretical properties and a set of robot system experiments showing improved performance over a well-engineered and hardened implementation of RMPs, our current state-of-the-art in controller design.      
