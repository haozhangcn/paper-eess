# ArXiv eess --Tue, 28 Sep 2021
### 1.BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.13226.pdf)
>  We summarize the results of a host of efforts using giant automatic speech recognition (ASR) models pre-trained using large, diverse unlabeled datasets containing approximately a million hours of audio. We find that the combination of pre-training, self-training and scaling up model size greatly increases data efficiency, even for extremely large tasks with tens of thousands of hours of labeled data. In particular, on an ASR task with 34k hours of labeled data, by fine-tuning an 8 billion parameter pre-trained Conformer model we can match state-of-the-art (SoTA) performance with only 3% of the training data and significantly improve SoTA with the full training set. We also report on the universal benefits gained from using big pre-trained and self-trained models for a large set of downstream tasks that cover a wide range of speech domains and span multiple orders of magnitudes of dataset sizes, including obtaining SoTA performance on many public benchmarks. In addition, we utilize the learned representation of pre-trained networks to achieve SoTA results on non-ASR tasks.      
### 2.EEG based stress analysis using rhythm specific spectral feature for video gameplay  [ :arrow_down: ](https://arxiv.org/pdf/2109.13200.pdf)
>  For the emerging significance of mental stress, various research directives have been established over time to better understand the causes of stress and how to deal with it. In recent years, the rise of video gameplay is unprecedented, further triggered by the lockdown imposed due to the COVID-19 pandemic. This paper presents an end-to-end stress analysis for video gaming stimuli using EEG. The PSD value of the Alpha and Beta bands is computed to calculate the Beta-to-Alpha ratio (BAR). In this article, BAR is used to denote mental stress. Subjects are chosen based on various factors such as gender, gameplay experience, age, and BMI. EEG is recorded using Scan SynAmps2 Express equipment. There are three types of video gameplay: strategic, puzzle, and combinational. Relaxation is accomplished in this study by the use of music of various pitches. Two types of regression analysis are done to mathematically model stress and relaxation curve. Brain topography is rendered to indicate the stressed and relaxed region of the brain. In the relaxed state, the subjects have BAR 0.701, which is considered the baseline value. Non-gamer subjects have an average BAR of 2.403 for 1 hour of strategic video gameplay, whereas gamers have 2.218 BAR concurrently. After 12 minutes of listening to low-pitch music, gamers achieved 0.709 BAR, which is nearly the baseline value. In comparison to Quartic regression, the 4PL symmetrical sigmoid function performs regression analysis with fewer parameters and computational power.      
### 3.MIMO-OFDM Dual-Functional Radar-Communication Systems: Low-PAPR Waveform Design  [ :arrow_down: ](https://arxiv.org/pdf/2109.13148.pdf)
>  In this paper, we explore a dual-functional radar-communication (DFRC) system for achieving integrated sensing and communications (ISAC). The technique of orthogonal frequency division multiplexing (OFDM) is leveraged to overcome the frequency-selective fading of the wideband multiple-input multiple-output (MIMO) systems with one multi-antenna DFRC base station (BS) and multiple single-antenna user equipment (UEs). In order to restrain the high peak-to-average power ratio (PAPR) of OFDM signals, we aim to jointly design low-PAPR DFRC MIMO-OFDM waveforms. This is done by utilizing a weighted objective function on both communication and radar performance metrics under power and PAPR constraints. The formulated optimization problems can be equivalently transformed into standard semi-definite programming (SDP) and can be effectively solved by semi-definite relaxation (SDR) method, where we prove that globally optimal rank-1 solution can be obtained in general. We further develop a low-complexity method to solve the problems with much reduced overheads. Moreover, the practical scenario with oversampling on OFDM signals is further considered, which has a significant effect on the resulting PAPR levels. The feasibility, effectiveness, and flexibility of the proposed low- PAPR DFRC MIMO-OFDM waveform design methods are demonstrated by a range of simulations on communication sum rate, symbol error rate as well as radar beampattern and detection probability.      
### 4.Plane-Wave Ultrasound Beamforming: A Deep Learning Approach  [ :arrow_down: ](https://arxiv.org/pdf/2109.13119.pdf)
>  Medical ultrasound provides images which are the spatial map of the tissue echogenicity. Unfortunately, an ultrasound image is a low-quality version of the expected Tissue Reflectivity Function (TRF) mainly due to the non-ideal Point Spread Function (PSF) of the imaging system. This paper presents a novel beamforming approach based on deep learning to get closer to the ideal PSF in Plane-Wave Imaging (PWI). The proposed approach is designed to reconstruct the desired TRF from echo traces acquired by transducer elements using only a single plane-wave transmission. In this approach, first, an ideal model for the TRF is introduced by setting the imaging PSF as a sharp Gaussian function. Then, a mapping function between the pre-beamformed Radio-Frequency (RF) channel data and the proposed TRF is constructed using deep learning. Network architecture contains multi-resolution decomposition and reconstruction using wavelet transform for effective recovery of high-frequency content of the desired TRF. Inspired by curriculum learning, we exploit step by step training from coarse (mean square error) to fine ($\ell_{0.2}$) loss functions. The proposed method is trained on a large number of simulation ultrasound data with the ground-truth echogenicity map extracted from real photographic images. The performance of the trained network is evaluated on the publicly available simulation and \textit{in vivo} test data without any further fine-tuning. Simulation test results confirm that the proposed method reconstructs images with a high quality in terms of resolution and contrast, which are also visually similar to the proposed ground-truth image. Furthermore, \textit{in vivo} results show that the trained mapping function preserves its performance in the new domain. Therefore, the proposed approach maintains high resolution, contrast, and framerate simultaneously.      
### 5.Adaptive Model Predictive Safety Certification for Learning-based Control -- Extended Version  [ :arrow_down: ](https://arxiv.org/pdf/2109.13033.pdf)
>  We propose an adaptive Model Predictive Safety Certification (MPSC) scheme for learning-based control of linear systems with bounded disturbances and uncertain parameters where the true parameters are contained within an a priori known set of parameters. An MPSC is a modular framework which can be used in combination with any learning-based controller to ensure state and input constraint satisfaction of a dynamical system by solving an online optimisation problem. By continuously connecting the current system state with a safe terminal set using a robust tube, safety can be ensured. Thereby, the main sources of conservative safety interventions are model uncertainties and short planning horizons. We develop an adaptive mechanism to improve the system model, which leverages set-membership estimation to guarantee recursively feasible and non-decreasing safety performance improvements. In order to accommodate short prediction horizons, iterative safe set enlargements using previously computed robust backup plans are proposed. Finally, we illustrate the increase of the safety performance through the parameter and safe set adaptation for numerical examples with up to 16 state dimensions.      
### 6.ECG Beat Representation and Delineation by means of Variable Projection  [ :arrow_down: ](https://arxiv.org/pdf/2109.13022.pdf)
>  The electrocardiogram (ECG) follows a characteristic shape, which has led to the development of several mathematical models for extracting clinically important information. Our main objective is to resolve limitations of previous approaches, that means to simultaneously cope with various noise sources, perform exact beat segmentation, and to retain diagnostically important morphological information. Methods: We therefore propose a model that is based on Hermite and sigmoid functions combined with piecewise polynomial interpolation for exact segmentation and low-dimensional representation of individual ECG beat segments. Hermite and sigmoidal functions enable reliable extraction of important ECG waveform information while the piecewise polynomial interpolation captures noisy signal features like the BLW. For that we use variable projection, which allows the separation of linear and nonlinear morphological variations of the according ECG waveforms. The resulting ECG model simultaneously performs BLW cancellation, beat segmentation, and low-dimensional waveform representation. Results: We demonstrate its BLW denoising and segmentation performance in two experiments, using synthetic and real data (Physionet QT database). Compared to state-of-the-art algorithms, the experiments showed less diagnostic distortion in case of denoising and a more robust delineation for the P and T wave. Conclusion: This work suggests a novel concept for ECG beat representation, easily adaptable to other biomedical signals with similar shape characteristics, such as blood pressure and evoked potentials. Significance: Our method is able to capture linear and nonlinear wave shape changes. Therefore, it provides a novel methodology to understand the origin of morphological variations caused, for instance, by respiration, medication, and abnormalities.      
### 7.Social Shaping for Transactive Energy Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.12967.pdf)
>  This paper considers the problem of shaping agent utility functions in a transactive energy system to ensure the optimal energy price at a competitive equilibrium is always socially acceptable, that is, below a prescribed threshold. Agents in a distributed energy system aim to maximize their individual payoffs, as a combination of the utility of energy consumption and the income/expenditure from energy exchange. The utility function of each agent is parameterized by individual preference vectors, with the overall system operating at competitive equilibriums. We show the social shaping problem of the proposed transactive energy system is conceptually captured by a set decision problem. The set of agent preferences that guarantees a socially acceptable price is characterized by an implicit algebraic equation for strictly concave and continuously differentiable utility functions. We also present two analytical solutions where tight ranges for the coefficients of linear-quadratic utilities and piece-wise linear utilities are established under which optimal pricing is proven to be always socially acceptable.      
### 8.Optimized Automated Cardiac MR Scar Quantification with GAN-Based Data Augmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.12940.pdf)
>  Background: The clinical utility of late gadolinium enhancement (LGE) cardiac MRI is limited by the lack of standardization, and time-consuming postprocessing. In this work, we tested the hypothesis that a cascaded deep learning pipeline trained with augmentation by synthetically generated data would improve model accuracy and robustness for automated scar quantification. <br>Methods: A cascaded pipeline consisting of three consecutive neural networks is proposed, starting with a bounding box regression network to identify a region of interest around the left ventricular (LV) myocardium. Two further nnU-Net models are then used to segment the myocardium and, if present, scar. The models were trained on the data from the EMIDEC challenge, supplemented with an extensive synthetic dataset generated with a conditional GAN. <br>Results: The cascaded pipeline significantly outperformed a single nnU-Net directly segmenting both the myocardium (mean Dice similarity coefficient (DSC) (standard deviation (SD)): 0.84 (0.09) vs 0.63 (0.20), p &lt; 0.01) and scar (DSC: 0.72 (0.34) vs 0.46 (0.39), p &lt; 0.01) on a per-slice level. The inclusion of the synthetic data as data augmentation during training improved the scar segmentation DSC by 0.06 (p &lt; 0.01). The mean DSC per-subject on the challenge test set, for the cascaded pipeline augmented by synthetic generated data, was 0.86 (0.03) and 0.67 (0.29) for myocardium and scar, respectively. <br>Conclusion: A cascaded deep learning-based pipeline trained with augmentation by synthetically generated data leads to myocardium and scar segmentations that are similar to the manual operator, and outperforms direct segmentation without the synthetic images.      
### 9.Exploiting Wind Turbine-Mounted Base Stations to Enhance Rural Connectivity  [ :arrow_down: ](https://arxiv.org/pdf/2109.12877.pdf)
>  Although global connectivity is one of the main requirements for future generations of wireless networks driven by the United Nation's Sustainable Development Goals (SDGs), telecommunication (telecom) providers are economically discouraged from investing in sparsely populated areas, such as rural and remote ones. Novel affordable and sustainable paradigms are thus indispensable to enhance the cellular infrastructure in such areas and bridge the digital divide when compared with urban ones. We investigate the use of wind turbine-mounted base stations (WTBSs) as a cost-effective solution for regions with high wind energy potential, since it could replace or even outperform current solutions requiring additional cell towers (CTs), satellites, or aerial base stations (ABSs). Indeed, conveniently installing base station (BS) equipment on wind generators would allow the transceivers to reach sufficient altitudes and easily establish line-of-sight (LoS) channels within large areas. We also propose insightful simulation results for realistic case studies based on data sets for wind speed and population densities as well as wind turbines' (WTs') and CTs' locations within specific French, Argentine, and Ethiopian exurban regions. By doing this, we hope to prove the feasibility and the effectiveness of this solution and stimulate its implementation.      
### 10.Multi-Static UWB Radar-based Passive Human Tracking Using COTS Devices  [ :arrow_down: ](https://arxiv.org/pdf/2109.12856.pdf)
>  Due to its high delay resolution, the ultra-wideband (UWB) technique has been widely adopted for fine-grained indoor localization. Instead of active positioning, multi-static UWB radar-based passive human tracking is explored using commercial off-the-shelf (COTS) devices. To extract the time-of-flight (ToF) reflected by the moving person, channel impulse responses (CIR) and the corresponding variances are used to train the convolutional neural networks (CNN) model. Particle filter algorithm is adopted to track the moving person based on the extracted ToFs of all pairs of links. Experimental results show that the proposed CIR- and variance-based CNN models achieve 30.12-cm and 29.04-cm root-mean-square errors (RMSEs), respectively. Especially, the variance-based CNN model is robust to the scenario changing and promising for practical applications.      
### 11.Fast-MD: Fast Multi-Decoder End-to-End Speech Translation with Non-Autoregressive Hidden Intermediates  [ :arrow_down: ](https://arxiv.org/pdf/2109.12804.pdf)
>  The multi-decoder (MD) end-to-end speech translation model has demonstrated high translation quality by searching for better intermediate automatic speech recognition (ASR) decoder states as hidden intermediates (HI). It is a two-pass decoding model decomposing the overall task into ASR and machine translation sub-tasks. However, the decoding speed is not fast enough for real-world applications because it conducts beam search for both sub-tasks during inference. We propose Fast-MD, a fast MD model that generates HI by non-autoregressive (NAR) decoding based on connectionist temporal classification (CTC) outputs followed by an ASR decoder. We investigated two types of NAR HI: (1) parallel HI by using an autoregressive Transformer ASR decoder and (2) masked HI by using Mask-CTC, which combines CTC and the conditional masked language model. To reduce a mismatch in the ASR decoder between teacher-forcing during training and conditioning on CTC outputs during testing, we also propose sampling CTC outputs during training. Experimental evaluations on three corpora show that Fast-MD achieved about 2x and 4x faster decoding speed than that of the naïve MD model on GPU and CPU with comparable translation quality. Adopting the Conformer encoder and intermediate CTC loss further boosts its quality without sacrificing decoding speed.      
### 12.An Adaptive PID Autotuner for Multicopters with Experimental Results  [ :arrow_down: ](https://arxiv.org/pdf/2109.12797.pdf)
>  This paper develops an adaptive PID autotuner for multicopters, and presents simulation and experimental results. The autotuner consists of adaptive digital control laws based on retrospective cost adaptive control implemented in the PX4 flight stack. A learning trajectory is used to optimize the autopilot during a single flight. The autotuned autopilot is then compared with the default PX4 autopilot by flying a test trajectory constructed using the second-order Hilbert curve. In order to investigate the sensitivity of the autotuner to the quadcopter dynamics, the mass of the quadcopter is varied, and the performance of the autotuned and default autopilot is compared. It is observed that the autotuned autopilot outperforms the default autopilot.      
### 13.Leveraging Multiple CNNs for Triaging Medical Workflow  [ :arrow_down: ](https://arxiv.org/pdf/2109.12783.pdf)
>  High hospitalization rates due to the global spread of Covid-19 bring about a need for improvements to classical triaging workflows. To this end, convolutional neural networks (CNNs) can effectively differentiate critical from non-critical images so that critical cases may be addressed quickly, so long as there exists some representative image for the illness. Presented is a conglomerate neural network system consisting of multiple VGG16 CNNs; the system trains on weighted skin disease images re-labelled as critical or non-critical, to then attach to input images a critical index between 0 and 10. A critical index offers a more comprehensive rating system compared to binary critical/non-critical labels. Results for batches of input images run through the trained network are promising. A batch is shown being re-ordered by the proposed architecture from most critical to least critical roughly accurately.      
### 14.Self-Excited Dynamics of Discrete-Time Lur'e Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.12732.pdf)
>  Self-excited systems arise in numerous applications, such as biochemical systems, fluid-structure interaction, and combustion. This paper analyzes a discrete-time Lur'e system with a piecewise-linear saturation feedback nonlinearity. The main result provides sufficient conditions under which the Lur'e system is self-excited in the sense that its response is bounded and nonconvergent.      
### 15.Backscatter Sensors Communication for 6G Low-powered NOMA-enabled IoT Networks under Imperfect SIC  [ :arrow_down: ](https://arxiv.org/pdf/2109.12711.pdf)
>  The combination of non-orthogonal multiple access (NOMA) using power-domain with backscatter sensor communication (BSC) is expected to connect a large-scale Internet of things (IoT) devices in future sixth-generation (6G) era. In this paper, we introduce a BSC in multi-cell IoT network, where a source in each cell transmits superimposed signal to its associated IoT devices using NOMA. The backscatter sensor tag (BST) also transmit data towards IoT devices by reflecting and modulating the superimposed signal of the source. A new optimization framework is provided that simultaneously optimizes the total power of each source, power allocation coefficient of IoT devices and reflection coefficient of BST under imperfect successive interference cancellation decoding. The objective of this work is to maximize the total energy efficiency of IoT network subject to quality of services of each IoT device. The problem is first transformed using the Dinkelbach method and then decoupled into two subproblems. The Karush-Kuhn-Tucker conditions and Lagrangian dual method are employed to obtain the efficient solutions. In addition, we also present the conventional NOMA network without BSC as a benchmark framework. Simulation results unveil the advantage of our considered NOMA BSC networks over the conventional NOMA network.      
### 16.Performance Analysis of IRS-Assisted Cell-Free Communication  [ :arrow_down: ](https://arxiv.org/pdf/2109.12650.pdf)
>  In this paper, the feasibility of adopting an intelligent reflective surface (IRS) in a cell-free wireless communication system is studied. The received signal-to-noise ratio (SNR) for this IRS-enabled cell-free set-up is optimized by adjusting phase-shifts of the passive reflective elements. Then, tight approximations for the probability density function and the cumulative distribution function for this optimal SNR are derived for Rayleigh fading. To investigate the performance of this system model, tight bounds/approximations for the achievable rate and outage probability are derived in closed form. The impact of discrete phase-shifts is modeled, and the corresponding detrimental effects are investigated by deriving an upper bound for the achievable rate in the presence of phase-shift quantization errors. Monte-Carlo simulations are used to validate our statistical characterization of the optimal SNR, and the corresponding analysis is used to investigate the performance gains of the proposed system model. We reveal that IRS-assisted communications can boost the performance of cell-free wireless architectures.      
### 17.A Novel Hybrid Convolutional Neural Network for Accurate Organ Segmentation in 3D Head and Neck CT Images  [ :arrow_down: ](https://arxiv.org/pdf/2109.12634.pdf)
>  Radiation therapy (RT) is widely employed in the clinic for the treatment of head and neck (HaN) cancers. An essential step of RT planning is the accurate segmentation of various organs-at-risks (OARs) in HaN CT images. Nevertheless, segmenting OARs manually is time-consuming, tedious, and error-prone considering that typical HaN CT images contain tens to hundreds of slices. Automated segmentation algorithms are urgently required. Recently, convolutional neural networks (CNNs) have been extensively investigated on this task. Particularly, 3D CNNs are frequently adopted to process 3D HaN CT images. There are two issues with naïve 3D CNNs. First, the depth resolution of 3D CT images is usually several times lower than the in-plane resolution. Direct employment of 3D CNNs without distinguishing this difference can lead to the extraction of distorted image features and influence the final segmentation performance. Second, a severe class imbalance problem exists, and large organs can be orders of times larger than small organs. It is difficult to simultaneously achieve accurate segmentation for all the organs. To address these issues, we propose a novel hybrid CNN that fuses 2D and 3D convolutions to combat the different spatial resolutions and extract effective edge and semantic features from 3D HaN CT images. To accommodate large and small organs, our final model, named OrganNet2.5D, consists of only two instead of the classic four downsampling operations, and hybrid dilated convolutions are introduced to maintain the respective field. Experiments on the MICCAI 2015 challenge dataset demonstrate that OrganNet2.5D achieves promising performance compared to state-of-the-art methods.      
### 18.Group Shift Pointwise Convolution for Volumetric Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.12629.pdf)
>  Recent studies have witnessed the effectiveness of 3D convolutions on segmenting volumetric medical images. Compared with the 2D counterparts, 3D convolutions can capture the spatial context in three dimensions. Nevertheless, models employing 3D convolutions introduce more trainable parameters and are more computationally complex, which may lead easily to model overfitting especially for medical applications with limited available training data. This paper aims to improve the effectiveness and efficiency of 3D convolutions by introducing a novel Group Shift Pointwise Convolution (GSP-Conv). GSP-Conv simplifies 3D convolutions into pointwise ones with 1x1x1 kernels, which dramatically reduces the number of model parameters and FLOPs (e.g. 27x fewer than 3D convolutions with 3x3x3 kernels). Naïve pointwise convolutions with limited receptive fields cannot make full use of the spatial image context. To address this problem, we propose a parameter-free operation, Group Shift (GS), which shifts the feature maps along with different spatial directions in an elegant way. With GS, pointwise convolutions can access features from different spatial locations, and the limited receptive fields of pointwise convolutions can be compensated. We evaluate the proposed methods on two datasets, PROMISE12 and BraTS18. Results show that our method, with substantially decreased model complexity, achieves comparable or even better performance than models employing 3D convolutions.      
### 19.Structure-aware scale-adaptive networks for cancer segmentation in whole-slide images  [ :arrow_down: ](https://arxiv.org/pdf/2109.12617.pdf)
>  Cancer segmentation in whole-slide images is a fundamental step for viable tumour burden estimation, which is of great value for cancer assessment. However, factors like vague boundaries or small regions dissociated from viable tumour areas make it a challenging task. Considering the usefulness of multi-scale features in various vision-related tasks, we present a structure-aware scale-adaptive feature selection method for efficient and accurate cancer segmentation. Based on a segmentation network with a popular encoder-decoder architecture, a scale-adaptive module is proposed for selecting more robust features to represent the vague, non-rigid boundaries. Furthermore, a structural similarity metric is proposed for better tissue structure awareness to deal with small region segmentation. In addition, advanced designs including several attention mechanisms and the selective-kernel convolutions are applied to the baseline network for comparative study purposes. Extensive experimental results show that the proposed structure-aware scale-adaptive networks achieve outstanding performance on liver cancer segmentation when compared to top ten submitted results in the challenge of PAIP 2019. Further evaluation on colorectal cancer segmentation shows that the scale-adaptive module improves the baseline network or outperforms the other excellent designs of attention mechanisms when considering the tradeoff between efficiency and accuracy.      
### 20.Recovery of Graph Signals from Sign Measurements  [ :arrow_down: ](https://arxiv.org/pdf/2109.12576.pdf)
>  Sampling and interpolation have been extensively studied, in order to reconstruct or estimate the entire graph signal from the signal values on a subset of vertexes, of which most achievements are about continuous signals. While in a lot of signal processing tasks, signals are not fully observed, and only the signs of signals are available, for example a rating system may only provide several simple options. In this paper, the reconstruction of band-limited graph signals based on sign sampling is discussed and a greedy sampling strategy is proposed. The simulation experiments are presented, and the greedy sampling algorithm is compared with random sampling algorithm, which verify the validity of the proposed approach.      
### 21.Deep Reinforcement Learning for Wireless Scheduling in Distributed Networked Control  [ :arrow_down: ](https://arxiv.org/pdf/2109.12562.pdf)
>  In the literature of transmission scheduling in wireless networked control systems (WNCSs) over shared wireless resources, most research works have focused on partially distributed settings, i.e., where either the controller and actuator, or the sensor and controller are co-located. To overcome this limitation, the present work considers a fully distributed WNCS with distributed plants, sensors, actuators and a controller, sharing a limited number of frequency channels. To overcome communication limitations, the controller schedules the transmissions and generates sequential predictive commands for control. Using elements of stochastic systems theory, we derive a sufficient stability condition of the WNCS, which is stated in terms of both the control and communication system parameters. Once the condition is satisfied, there exists at least one stationary and deterministic scheduling policy that can stabilize all plants of the WNCS. By analyzing and representing the per-step cost function of the WNCS in terms of a finite-length countable vector state, we formulate the optimal transmission scheduling problem into a Markov decision process problem and develop a deep-reinforcement-learning-based algorithm for solving it. Numerical results show that the proposed algorithm significantly outperforms the benchmark policies.      
### 22.Neural Augmentation of Kalman Filter with Hypernetwork for Channel Tracking  [ :arrow_down: ](https://arxiv.org/pdf/2109.12561.pdf)
>  We propose Hypernetwork Kalman Filter (HKF) for tracking applications with multiple different dynamics. The HKF combines generalization power of Kalman filters with expressive power of neural networks. Instead of keeping a bank of Kalman filters and choosing one based on approximating the actual dynamics, HKF adapts itself to each dynamics based on the observed sequence. Through extensive experiments on CDL-B channel model, we show that the HKF can be used for tracking the channel over a wide range of Doppler values, matching Kalman filter performance with genie Doppler information. At high Doppler values, it achieves around 2dB gain over genie Kalman filter. The HKF generalizes well to unseen Doppler, SNR values and pilot patterns unlike LSTM, which suffers from severe performance degradation.      
### 23.Self-loop Compensation in Signed Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.12555.pdf)
>  Stability of multi-agent systems on signed networks is intricate. To some extent, this is due to the associated signed Laplacian may lose its diagonal dominance property. This paper proposes a distributed self-loop compensation approach to rebuild the diagonal dominance of signed Laplacian, and subsequently, examine the stability and cluster consensus of the resultant compensated signed networks. Quantitative connections between the magnitude of self-loop compensation and the steady-state of the compensated signed network are analytically established, depending on the structural balance of signed networks. Some necessary and sufficient conditions for cluster consensus of compensated signed networks are provided as well as the explicit characterization of their steady-states. It turns out that structurally imbalanced networks need less self-loop compensation to be stable compared with the structurally balanced ones. The optimality of compensation magnitude is discussed. Both undirected and directed signed networks are examined. Simulation examples are provided to demonstrate the theoretical results.      
### 24.SDN-based Resource Allocation in Edge and Cloud Computing Systems: An Evolutionary Stackelberg Differential Game Approach  [ :arrow_down: ](https://arxiv.org/pdf/2109.12543.pdf)
>  Recently, the boosting growth of computation-heavy applications raises great challenges for the Fifth Generation (5G) and future wireless networks. As responding, the hybrid edge and cloud computing (ECC) system has been expected as a promising solution to handle the increasing computational applications with low-latency and on-demand services of computation offloading, which requires new computing resource sharing and access control technology paradigms. This work establishes a software-defined networking (SDN) based architecture for edge/cloud computing services in 5G heterogeneous networks (HetNets), which can support efficient and on-demand computing resource management to optimize resource utilization and satisfy the time-varying computational tasks uploaded by user devices. In addition, resulting from the information incompleteness, we design an evolutionary game based service selection for users, which can model the replicator dynamics of service subscription. Based on this dynamic access model, a Stackelberg differential game based cloud computing resource sharing mechanism is proposed to facilitate the resource trading between the cloud computing service provider (CCP) and different edge computing service providers (ECPs). Then we derive the optimal pricing and allocation strategies of cloud computing resource based on the replicator dynamics of users' service selection. These strategies can promise the maximum integral utilities to all computing service providers (CPs), meanwhile the user distribution can reach the evolutionary stable state at this Stackelberg equilibrium. Furthermore, simulation results validate the performance of the designed resource sharing mechanism, and reveal the convergence and equilibrium states of user selection, and computing resource pricing and allocation.      
### 25.On the Scheduling Policy for Multi-process WNCS under Edge Computing  [ :arrow_down: ](https://arxiv.org/pdf/2109.12535.pdf)
>  This paper considers a multi-process and multi-controller wireless networked control system (WNCS). There are $N$ independent linear time-invariant processes in the system plant which represent different kinds of physical processes. By considering the edge computing, the controllers are played by edge server and cloud server. Each process is measured by a sensor, and the status updates is sent to controller to generate the control command. The link delay of cloud server is longer than that of edge server. The processing time of status update depends on the characteristic of servers and processes. By taking into account such conditions, we mainly investigate how to choose the destination of status updates to minimize the system's average Mean Square Error (MSE), edge server or cloud server? To address this issue, we formulate an infinite horizon average cost Markov Decision Process (MDP) problem and obtain the optimal scheduling policy. The monotonicity of the value function in MDP is characterized and then used to show the threshold structure properties of the optimal scheduling policy. To overcome the curse of dimensionality, we propose a low-complexity suboptimal policy by using additive separable structure of value function. Furthermore, the processing preemption mechanism is considered to handle the status updates more flexible, and the consistency property is proved. On this basis, a numerical example is provided. The simulation results illustrate that the controller selection is related to the timeliness of process and show the performance of the suboptimal policy. We also find that the optimal policy will become a static policy in which the destination of status update is fixed when the wireless channel is error free.      
### 26.Approaching the Stability Boundary of a Power System: Theory and Applications  [ :arrow_down: ](https://arxiv.org/pdf/2109.12514.pdf)
>  Estimating the stability boundary is a fundamental and challenging problem in transient stability studies. It is known that a proper level set of a Lyapunov function or an energy function can provide an inner approximation of the stability boundary, and the estimation can be expanded by trajectory reversing methods. In this paper, we streamline the theoretical foundation of the expansion methodology, and generalize it by relaxing the request that the initial guess should be a subset of the stability region. We investigate topological characteristics of the expanded boundary, showing how an initial guess can approach the exact stability boundary locally or globally. We apply the theory to transient stability assessment, and propose expansion algorithms to improve the well-known Potential Energy Boundary Surface (PEBS) and Boundary of stability region based Controlling Unstable equilibrium point (BCU) methods. Case studies on the IEEE 39-bus system well verify our results and demonstrate that estimations of the stability boundary and the critical clearing time can be significantly improved with modest computational cost.      
### 27.Venc Design and Velocity Estimation for Phase Contrast MRI  [ :arrow_down: ](https://arxiv.org/pdf/2109.12481.pdf)
>  In phase-contrast magnetic resonance imaging (PC-MRI), the velocity of spins at a voxel is encoded in the image phase. The strength of the velocity encoding (venc) gradient offers a trade-off between the velocity-to-noise ratio (VNR) and the extent of phase aliasing. In the three-point encoding employed in traditional dual-venc acquisition, two velocity-encoded acquisitions are acquired along with a third velocity-compensated measurement; their phase differences result in an unaliased high-venc measurement used to unwrap the less noisy low-venc measurement. Alternatively, the velocity may be more accurately estimated by jointly processing all three potentially wrapped phase differences. We present a fast, grid-free approximate maximum likelihood estimator, Phase Recovery from Multiple Wrapped Measurements (PRoM), for solving a noisy set of congruence equations with correlated noise. PRoM is applied to three-point acquisition for estimating velocity. The proposed approach can significantly expand the range of correctly unwrapped velocities compared to the traditional dual-venc method, while also providing improvement in velocity-to-noise ratio. Moreover, its closed-form expressions for the probability distribution of the estimated velocity enable the optimized design of acquisition.      
### 28.EllipseNet: Anchor-Free Ellipse Detection for Automatic Cardiac Biometrics in Fetal Echocardiography  [ :arrow_down: ](https://arxiv.org/pdf/2109.12474.pdf)
>  As an important scan plane, four chamber view is routinely performed in both second trimester perinatal screening and fetal echocardiographic examinations. The biometrics in this plane including cardio-thoracic ratio (CTR) and cardiac axis are usually measured by sonographers for diagnosing congenital heart disease. However, due to the commonly existing artifacts like acoustic shadowing, the traditional manual measurements not only suffer from the low efficiency, but also with the inconsistent results depending on the operators' skills. In this paper, we present an anchor-free ellipse detection network, namely EllipseNet, which detects the cardiac and thoracic regions in ellipse and automatically calculates the CTR and cardiac axis for fetal cardiac biometrics in 4-chamber view. In particular, we formulate the network that detects the center of each object as points and regresses the ellipses' parameters simultaneously. We define an intersection-over-union loss to further regulate the regression procedure. We evaluate EllipseNet on clinical echocardiogram dataset with more than 2000 subjects. Experimental results show that the proposed framework outperforms several state-of-the-art methods. Source code will be available at <a class="link-external link-https" href="https://git.openi.org.cn/capepoint/EllipseNet" rel="external noopener nofollow">this https URL</a> .      
### 29.System Identification in Multi-Actuator Hard Disk Drives with Colored Noises using Observer/Kalman Filter Identification (OKID) Framework  [ :arrow_down: ](https://arxiv.org/pdf/2109.12460.pdf)
>  Multi Actuator Technology in Hard Disk drives (HDDs) equips drives with two dual stage actuators (DSA) each comprising of a voice coil motor (VCM) actuator and a piezoelectric micro actuator (MA) operating on the same pivot point. Each DSA is responsible for controlling half of the drive's arms. As both the DSAs operate independently on the same pivot timber, the control forces and torques generated by one affect the operation of the other. The feedback controllers might not completely reject these transferred disturbances and a need to design feedforward controllers arises, which require a good model of the disturbance process. The usual system identification techniques produce a biased estimate because of the presence of the runout which is a colored noise. In this paper, we use the OKID framework to estimate this disturbance cross transfer function from the VCM control input of one DSA to the output of the other DSA from the collected time series data corrupted by colored noise.      
### 30.Classification of COVID-19 from CXR Images in a 15-class Scenario: an Attempt to Avoid Bias in the System  [ :arrow_down: ](https://arxiv.org/pdf/2109.12453.pdf)
>  As of June 2021, the World Health Organization (WHO) has reported 171.7 million confirmed cases including 3,698,621 deaths from COVID-19. Detecting COVID-19 and other lung diseases from Chest X-Ray (CXR) images can be very effective for emergency diagnosis and treatment as CXR is fast and cheap. The objective of this study is to develop a system capable of detecting COVID-19 along with 14 other lung diseases from CXRs in a fair and unbiased manner. The proposed system consists of a CXR image selection technique and a deep learning based model to classify 15 diseases including COVID-19. The proposed CXR selection technique aims to retain the maximum variation uniformly and eliminate poor quality CXRs with the goal of reducing the training dataset size without compromising classifier accuracy. More importantly, it reduces the often hidden bias and unfairness in decision making. The proposed solution exhibits a promising COVID-19 detection scheme in a more realistic situation than most existing studies as it deals with 15 lung diseases together. We hope the proposed method will have wider adoption in medical image classification and other related fields.      
### 31.Optimal Precoder Design for MIMO-OFDM-based Joint Automotive Radar-Communication Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.12452.pdf)
>  Large-scale deployment of connected vehicles with cooperative awareness technologies increases the demand for vehicle-to-everything (V2X) communication spectrum in 5.9 GHz that is mainly allocated for the exchange of safety messages. To supplement V2X communication and support the high data rates needed by broadband applications, the millimeter-wave (mmWave) automotive radar spectrum at 76-81 GHz can be utilized. For this purpose, joint radar-communication systems have been proposed in the literature to perform both functions using the same waveform and hardware. While multiple-input and multiple-output (MIMO) communication with multiple users enables independent data streaming for high throughput, MIMO radar processing provides high-resolution imaging that is crucial for safety-critical systems. However, employing conventional precoding methods designed for communication generates directional beams that impair MIMO radar imaging and target tracking capabilities during data streaming. In this paper, we propose a MIMO joint automotive radar-communication (JARC) framework based on orthogonal frequency division multiplexing (OFDM) waveform. First, we show that the MIMO-OFDM preamble can be exploited for both MIMO radar processing and estimation of the communication channel. Then, we propose an optimal precoder design method that enables high accuracy target tracking while transmitting independent data streams to multiple receivers. The proposed methods provide high-resolution radar imaging and high throughput capabilities for MIMO JARC networks. Finally, we evaluate the efficacy of the proposed methods through numerical simulations.      
### 32.ReCal-Net: Joint Region-Channel-Wise Calibrated Network for Semantic Segmentation in Cataract Surgery Videos  [ :arrow_down: ](https://arxiv.org/pdf/2109.12448.pdf)
>  Semantic segmentation in surgical videos is a prerequisite for a broad range of applications towards improving surgical outcomes and surgical video analysis. However, semantic segmentation in surgical videos involves many challenges. In particular, in cataract surgery, various features of the relevant objects such as blunt edges, color and context variation, reflection, transparency, and motion blur pose a challenge for semantic segmentation. In this paper, we propose a novel convolutional module termed as \textit{ReCal} module, which can calibrate the feature maps by employing region intra-and-inter-dependencies and channel-region cross-dependencies. This calibration strategy can effectively enhance semantic representation by correlating different representations of the same semantic label, considering a multi-angle local view centering around each pixel. Thus the proposed module can deal with distant visual characteristics of unique objects as well as cross-similarities in the visual characteristics of different objects. Moreover, we propose a novel network architecture based on the proposed module termed as ReCal-Net. Experimental results confirm the superiority of ReCal-Net compared to rival state-of-the-art approaches for all relevant objects in cataract surgery. Moreover, ablation studies reveal the effectiveness of the ReCal module in boosting semantic segmentation accuracy.      
### 33.Accelerated consensus in multi-agent networks via memory of local averages  [ :arrow_down: ](https://arxiv.org/pdf/2109.12441.pdf)
>  Classical mathematical models of information sharing and updating in multi-agent networks use linear operators. In the paradigmatic DeGroot model, agents update their states with linear combinations of their neighbors' current states. In prior work, an accelerated averaging model employing the use of memory has been suggested to accelerate convergence to a consensus state for undirected networks. There, the DeGroot update on the current states is followed by a linear combination with the previous states. We propose a modification where the DeGroot update is applied to the current and previous states and is then followed by a linear combination step. We show that this simple modification applied to undirected networks permits convergence even for periodic networks. Further, it allows for faster convergence than the DeGroot and accelerated averaging models for suitable networks and model parameters.      
### 34.Smart Home Energy Management: Sequence-to-Sequence Load Forecasting and Q-Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.12440.pdf)
>  A smart home energy management system (HEMS) can contribute towards reducing the energy costs of customers; however, HEMS suffers from uncertainty in both energy generation and consumption patterns. In this paper, we propose a sequence to sequence (Seq2Seq) learning-based supply and load prediction along with reinforcement learning-based HEMS control. We investigate how the prediction method affects the HEMS operation. First, we use Seq2Seq learning to predict photovoltaic (PV) power and home devices' load. We then apply Q-learning for offline optimization of HEMS based on the prediction results. Finally, we test the online performance of the trained Q-learning scheme with actual PV and load data. The Seq2Seq learning is compared with VARMA, SVR, and LSTM in both prediction and operation levels. The simulation results show that Seq2Seq performs better with a lower prediction error and online operation performance.      
### 35.Channel State Information Based Localization with Deep Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.12398.pdf)
>  Localization is one of the most important problems in various fields such as robotics and wireless communications. For instance, Unmanned Aerial Vehicles (UAVs) require the information of the position precisely for an adequate control strategy. This problem is handled very efficiently with integrated GPS units for outdoor applications. However, indoor applications require special treatment due to the unavailability of GPS signals. Another aspect of mobile robots such as UAVs is that there is constant wireless communication between the mobile robot and a computational unit. This communication is mainly done for obtaining telemetry information or computation of control actions directly. The responsible integrated units for this transmission are commercial wireless communication chipsets. These units on the receiver side are responsible for getting rid of the diverse effects of the communication channel with various mathematical techniques. These techniques mainly require the Channel State Information (CSI) of the current channel to compensate the channel itself. After the compensation, the chipset has nothing to do with CSI. However, the locations of both the transmitter and receiver have a direct impact on CSI. Even though CSI contains such rich information about the environment, the accessibility of these data is blocked by the commercial wireless chipsets since they are manufactured to provide only the processed information data bits to the user. However, with the IEEE 802.11n standardization, certain chipsets provide access to CSI. Therefore, CSI data became processible and integrable to localization schemes. In this project, a test environment was constructed for the localization task. Two routers with proper chipsets were assigned as transmitter and receiver. They were operationalized for the CSI data collection. Lastly, these data were processed with various deep learning models.      
### 36.Automated Multi-domain Engineering Design through Linear Graph and Genetic Programming  [ :arrow_down: ](https://arxiv.org/pdf/2109.12388.pdf)
>  This paper proposes a methodology of integrating the Linear Graph (LG) approach with Genetic Programming (GP) for generating an automated multi-domain engineering design approach by using the in-house developed LG MATLAB toolbox and the GP toolbox in MATLAB. The necessary background for the development are presented, and the methodology used in this work to facilitate the construction and evaluation of filter circuits, using LG models, is described. Designing electronic filter circuits through an evolution from electronic components to the completed circuits is demonstrated. The topology and component values of three types of filter circuits: low pass, high pass, and band pass, are designed through this evolutionary approach, for various cut-off frequencies. Furthermore, the paper demonstrates through examples of these evolved filter circuits, the combined GP and LG approach is successful in constructing high order filter circuits that are capable of attenuating undesired frequencies while maintaining desirable ones. The work presented in the paper is a key step towards the integration of LG modeling, through the use of the LGtheory MATLAB Toolbox, with machine learning techniques for the automated design of dynamic multi-domain mechatronic systems.      
### 37.Joint Progressive and Coarse-to-fine Registration of Brain MRI via Deformation Field Integration and Non-Rigid Feature Fusion  [ :arrow_down: ](https://arxiv.org/pdf/2109.12384.pdf)
>  Registration of brain MRI images requires to solve a deformation field, which is extremely difficult in aligning intricate brain tissues, e.g., subcortical nuclei, etc. Existing efforts resort to decomposing the target deformation field into intermediate sub-fields with either tiny motions, i.e., progressive registration stage by stage, or lower resolutions, i.e., coarse-to-fine estimation of the full-size deformation field. In this paper, we argue that those efforts are not mutually exclusive, and propose a unified framework for robust brain MRI registration in both progressive and coarse-to-fine manners simultaneously. Specifically, building on a dual-encoder U-Net, the fixed-moving MRI pair is encoded and decoded into multi-scale deformation sub-fields from coarse to fine. Each decoding block contains two proposed novel modules: i) in Deformation Field Integration (DFI), a single integrated sub-field is calculated, warping by which is equivalent to warping progressively by sub-fields from all previous decoding blocks, and ii) in Non-rigid Feature Fusion (NFF), features of the fixed-moving pair are aligned by DFI-integrated sub-field, and then fused to predict a finer sub-field. Leveraging both DFI and NFF, the target deformation field is factorized into multi-scale sub-fields, where the coarser fields alleviate the estimate of a finer one and the finer field learns to make up those misalignments insolvable by previous coarser ones. The extensive and comprehensive experimental results on both private and public datasets demonstrate a superior registration performance of brain MRI images over progressive registration only and coarse-to-fine estimation only, with an increase by at most 10% in the average Dice.      
### 38.Automaton-based Implicit Controlled Invariant Set Computation for Discrete-Time Linear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.12378.pdf)
>  In this paper, we derive closed-form expressions for implicit controlled invariant sets for discrete-time controllable linear systems with measurable disturbances. In particular, a disturbance-reactive (or disturbance feedback) controller in the form of a parameterized finite automaton is considered. We show that, for a class of automata, the robust positively invariant sets of the corresponding closed-loop systems can be expressed by a set of linear inequality constraints in the joint space of system states and controller parameters. This leads to an implicit representation of the invariant set in a lifted space. We further show how the same parameterization can be used to compute invariant sets when the disturbance is not available for measurement.      
### 39.Suboptimal nonlinear model predictive control with input move-blocking  [ :arrow_down: ](https://arxiv.org/pdf/2109.12355.pdf)
>  This paper deals with the integration of input move-blocking into the framework of suboptimal model predictive control. The blocked input parameterization is explicitly considered as a source of suboptimality. A straightforward integration approach is to hold back a manually generated stabilizing fallback solution in some buffer for the case that the optimizer does not find a better input move-blocked solution. An extended approach superimposes the manually generated stabilizing warm-start by the move-blocked control sequence and enables a stepwise improvement of the control performance. In addition, this contribution provides a detailed review of the literature on input move-blocked model predictive control and combines important results with the findings of suboptimal model predictive control. A numerical example supports the theoretical results and shows the effectiveness of the proposed approach.      
### 40.Prediction of MGMT Methylation Status of Glioblastoma using Radiomics and Latent Space Shape Features  [ :arrow_down: ](https://arxiv.org/pdf/2109.12339.pdf)
>  In this paper we propose a method for predicting the status of MGMT promoter methylation in high-grade gliomas. From the available MR images, we segment the tumor using deep convolutional neural networks and extract both radiomic features and shape features learned by a variational autoencoder. We implemented a standard machine learning workflow to obtain predictions, consisting of feature selection followed by training of a random forest classification model. We trained and evaluated our method on the RSNA-ASNR-MICCAI BraTS 2021 challenge dataset and submitted our predictions to the challenge.      
### 41.Predicting survival of glioblastoma from automatic whole-brain and tumor segmentation of MR images  [ :arrow_down: ](https://arxiv.org/pdf/2109.12334.pdf)
>  Survival prediction models can potentially be used to guide treatment of glioblastoma patients. However, currently available MR imaging biomarkers holding prognostic information are often challenging to interpret, have difficulties generalizing across data acquisitions, or are only applicable to pre-operative MR data. In this paper we aim to address these issues by introducing novel imaging features that can be automatically computed from MR images and fed into machine learning models to predict patient survival. The features we propose have a direct biological interpretation: They measure the deformation caused by the tumor on the surrounding brain structures, comparing the shape of various structures in the patient's brain to their expected shape in healthy individuals. To obtain the required segmentations, we use an automatic method that is contrast-adaptive and robust to missing modalities, making the features generalizable across scanners and imaging protocols. Since the features we propose do not depend on characteristics of the tumor region itself, they are also applicable to post-operative images, which have been much less studied in the context of survival prediction. Using experiments involving both pre- and post-operative data, we show that the proposed features carry prognostic value in terms of overall- and progression-free survival, over and above that of conventional non-imaging features.      
### 42.A Primal Decomposition Approach to Globally Coupled Aggregative Optimization over Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.12297.pdf)
>  We consider a class of multi-agent optimization problems, where each agent has a local objective function that depends on its own decision variables and the aggregate of others, and is willing to cooperate with other agents to minimize the sum of the local objectives. After associating each agent with an auxiliary variable and the related local estimates, we conduct primal decomposition to the globally coupled problem and reformulate it so that it can be solved distributedly. Based on the Douglas-Rachford method, an algorithm is proposed which ensures the exact convergence to a solution of the original problem. The proposed method enjoys desirable scalability by only requiring each agent to keep local estimates that grow linearly with the number of its neighbors. We illustrate our proposed algorithm by numerical simulations on a commodity distribution problem over a transport network.      
### 43.Distributed Computation of Stochastic GNE with Partial Information: An Augmented Best-Response Scheme  [ :arrow_down: ](https://arxiv.org/pdf/2109.12290.pdf)
>  In this paper, we focus on the stochastic generalized Nash equilibrium problem (SGNEP) which is an important and widely-used model in many different fields. In this model, subject to certain global resource constraints, a set of self-interested players aim to optimize their local objectives that depend on their own decisions and the decisions of others and are influenced by some random factors. We propose a distributed stochastic generalized Nash equilibrium seeking algorithm in a partial-decision information setting based on the Douglas-Rachford operator splitting scheme, which significantly relaxes assumptions on co-coercivity and contractiveness in the existing literature. The proposed algorithm updates players' local decisions through box-constrained augmented best-response schemes and subsequent projections onto the local feasible sets, which occupy most of the computational workload. The projected stochastic subgradient method is applied to provide approximate solutions to the augmented best-response subproblems for each player. The Robbins-Siegmund theorem is leveraged to establish the main convergence results to a true Nash equilibrium and sufficient conditions to be satisfied by the inexact solver used. Finally, we illustrate the validity of the proposed algorithm through two numerical examples, i.e., a stochastic Nash-Cournot distribution game and a multi-product assembly problem with the two-stage model.      
### 44.BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.12271.pdf)
>  Convolutional neural networks (CNNs) have recently achieved remarkable success in automatically identifying organs or lesions on 3D medical images. Meanwhile, vision transformer networks have exhibited exceptional performance in 2D image classification tasks. Compared with CNNs, transformer networks have an obvious advantage of extracting long-range features due to their self-attention algorithm. Therefore, in this paper we present a CNN-Transformer combined model called BiTr-Unet for brain tumor segmentation on multi-modal MRI scans. The proposed BiTr-Unet achieves good performance on the BraTS 2021 validation dataset with mean Dice score 0.9076, 0.8392 and 0.8231, and mean Hausdorff distance 4.5322, 13.4592 and 14.9963 for the whole tumor, tumor core, and enhancing tumor, respectively.      
### 45.Constrained Attack-Resilient Estimation of Stochastic Cyber-Physical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.12255.pdf)
>  In this paper, a constrained attack-resilient estimation algorithm (CARE) is developed for stochastic cyber-physical systems. The proposed CARE has improved estimation accuracy and detection performance when physical constraints and operational limitations are available. In particular, CARE is designed for simultaneous input and state estimation that provides minimum-variance unbiased estimates, and these estimates are projected onto the constrained space restricted by inequality constraints subsequently. We prove that the estimation errors and their covariances from CARE are less than those from unconstrained algorithms, and confirm that this property can further reduce the false negative rate in attack detection. We show that estimation errors of CARE are practically exponentially stable in mean square. Finally, an illustrative example of attacks on a vehicle is given to demonstrate the improved estimation accuracy and detection performance compared to an existing unconstrained algorithm.      
### 46.Development of Safety Monitoring System of Connected and Automated Vehicles considering the Trade-off between Communication Efficiency and Data Reliability  [ :arrow_down: ](https://arxiv.org/pdf/2109.12253.pdf)
>  The safety of urban transportation systems is considered a public health issue worldwide, and many researchers have contributed to improving it. Connected automated vehicles (CAVs) and cooperative intelligent transportation systems (C-ITSs) are considered solutions to ensure urban transportation systems' safety using various sensors and communication devices. However, it is found difficult to deploy the C-ITS framework in South Korea, because CAVs produce a massive amount of data every minute but it cannot be transmitted via existing communication network. Thus, raw data must be sampled to reduce the size of the data over communication network and transmitted to the server for further processing. On the other hand, the sampled data must be highly accurate to ensure the safety of different agents in C-ITS. Thus, in this study, we designed and developed a C-ITS architecture and data flow, including messages and protocols for the safety monitoring system of CAVs, and determined the optimal sampling interval for data transmission while considering the trade-off between communication efficiency and reliability of safety performance indicators. Three safety performance indicators were introduced: severe deceleration, lateral position variance, and inverse time to collision. A field test is conducted to collect data from various sensors installed in the CAV, determining the optimal sampling interval. Kolmogorov-Smirnov test is conducted to ensure statistical consistency between sampled and raw datasets. The effects of the sampling interval on message delay, data accuracy, and communication efficiency were analyzed.      
### 47.Data-driven control via Petersen's lemma  [ :arrow_down: ](https://arxiv.org/pdf/2109.12175.pdf)
>  We address the problem of designing a stabilizing closed-loop control law directly from input and state measurements collected in an open-loop experiment. In the presence of noise in data, we have that a set of dynamics could have generated the collected data and we need the designed controller to stabilize such set of data-consistent dynamics robustly. For this problem of data-driven control with noisy data, we advocate the use of a popular tool from robust control, Petersen's lemma. In the cases of data generated by linear and polynomial systems, we conveniently express the uncertainty captured in the set of data-consistent dynamics through a matrix ellipsoid, and we show that a specific form of this matrix ellipsoid makes it possible to apply Petersen's lemma to all of the mentioned cases. In this way, we obtain necessary and sufficient conditions for data-driven stabilization of linear systems through a linear matrix inequality. The matrix ellipsoid representation enables insights and interpretations of the designed control laws. In the same way, we also obtain sufficient conditions for data-driven stabilization of polynomial systems through (convex) sum-of-squares programs. The findings are illustrated numerically.      
### 48.Unsupervised Cross-Modality Domain Adaptation for Segmenting Vestibular Schwannoma and Cochlea with Data Augmentation and Model Ensemble  [ :arrow_down: ](https://arxiv.org/pdf/2109.12169.pdf)
>  Magnetic resonance images (MRIs) are widely used to quantify vestibular schwannoma and the cochlea. Recently, deep learning methods have shown state-of-the-art performance for segmenting these structures. However, training segmentation models may require manual labels in target domain, which is expensive and time-consuming. To overcome this problem, domain adaptation is an effective way to leverage information from source domain to obtain accurate segmentations without requiring manual labels in target domain. In this paper, we propose an unsupervised learning framework to segment the VS and cochlea. Our framework leverages information from contrast-enhanced T1-weighted (ceT1-w) MRIs and its labels, and produces segmentations for T2-weighted MRIs without any labels in the target domain. We first applied a generator to achieve image-to-image translation. Next, we ensemble outputs from an ensemble of different models to obtain final segmentations. To cope with MRIs from different sites/scanners, we applied various 'online' augmentations during training to better capture the geometric variability and the variability in image appearance and quality. Our method is easy to build and produces promising segmentations, with a mean Dice score of 0.7930 and 0.7432 for VS and cochlea respectively in the validation set.      
### 49.Deep Neural Networks for Blind Image Quality Assessment: Addressing the Data Challenge  [ :arrow_down: ](https://arxiv.org/pdf/2109.12161.pdf)
>  The enormous space and diversity of natural images is usually represented by a few small-scale human-rated image quality assessment (IQA) datasets. This casts great challenges to deep neural network (DNN) based blind IQA (BIQA), which requires large-scale training data that is representative of the natural image distribution. It is extremely difficult to create human-rated IQA datasets composed of millions of images due to constraints of subjective testing. While a number of efforts have focused on design innovations to enhance the performance of DNN based BIQA, attempts to address the scarcity of labeled IQA data remain surprisingly missing. To address this data challenge, we construct so far the largest IQA database, namely Waterloo Exploration-II, which contains 3,570 pristine reference and around 3.45 million singly and multiply distorted images. Since subjective testing for such a large dataset is nearly impossible, we develop a novel mechanism that synthetically assigns perceptual quality labels to the distorted images. We construct a DNN-based BIQA model called EONSS, train it on Waterloo Exploration-II, and test it on nine subject-rated IQA datasets, without any retraining or fine-tuning. The results show that with a straightforward DNN architecture, EONSS is able to outperform the very state-of-the-art in BIQA, both in terms of quality prediction performance and execution speed. This study strongly supports the view that the quantity and quality of meaningfully annotated training data, rather than a sophisticated network architecture or training strategy, is the dominating factor that determines the performance of DNN-based BIQA models. (Note: Since this is an ongoing project, the final versions of Waterloo Exploration-II database, quality annotations, and EONSS, will be made publicly available in the future when it culminates.)      
### 50.Use of the Deep Learning Approach to Measure Alveolar Bone Level  [ :arrow_down: ](https://arxiv.org/pdf/2109.12115.pdf)
>  Abstract: <br>Aim: The goal was to use a Deep Convolutional Neural Network to measure the radiographic alveolar bone level to aid periodontal diagnosis. <br>Material and methods: A Deep Learning (DL) model was developed by integrating three segmentation networks (bone area, tooth, cementoenamel junction) and image analysis to measure the radiographic bone level and assign radiographic bone loss (RBL) stages. The percentage of RBL was calculated to determine the stage of RBL for each tooth. A provisional periodontal diagnosis was assigned using the 2018 periodontitis classification. RBL percentage, staging, and presumptive diagnosis were compared to the measurements and diagnoses made by the independent examiners. <br>Results: The average Dice Similarity Coefficient (DSC) for segmentation was over 0.91. There was no significant difference in RBL percentage measurements determined by DL and examiners (p=0.65). The Area Under the Receiver Operating Characteristics Curve of RBL stage assignment for stage I, II and III was 0.89, 0.90 and 0.90, respectively. The accuracy of the case diagnosis was 0.85. <br>Conclusion: The proposed DL model provides reliable RBL measurements and image-based periodontal diagnosis using periapical radiographic images. However, this model has to be further optimized and validated by a larger number of images to facilitate its application.      
### 51.Identifying Women with Mammographically-Occult Breast Cancer Leveraging GAN-Simulated Mammograms  [ :arrow_down: ](https://arxiv.org/pdf/2109.12113.pdf)
>  Our objective is to show the feasibility of using simulated mammograms to detect mammographically-occult (MO) cancer in women with dense breasts and a normal screening mammogram who could be triaged for additional screening with magnetic resonance imaging (MRI) or ultrasound. We developed a Conditional Generative Adversarial Network (CGAN) to simulate a mammogram with normal appearance using the opposite mammogram as the condition. We used a Convolutional Neural Network (CNN) trained on Radon Cumulative Distribution Transform (RCDT) processed mammograms to detect MO cancer. For training CGAN, we used screening mammograms of 1366 women. For MO cancer detection, we used screening mammograms of 333 women (97 MO cancer) with dense breasts. We simulated the right mammogram for normal controls and the cancer side for MO cancer cases. We created two RCDT images, one from a real mammogram pair and another from a real-simulated mammogram pair. We finetuned a VGG16 on resulting RCDT images to classify the women with MO cancer. We compared the classification performance of the CNN trained on fused RCDT images, CNN_{Fused} to that of trained only on real RCDT images, CNN_{Real}, and to that of trained only on simulated RCDT images, CNN_{Simulated}. The test AUC for CNN_{Fused} was 0.77 with a 95% confidence interval (95CI) of [0.71, 0.83], which was statistically better (p-value &lt; 0.02) than the CNN_{Real} AUC of 0.70 with a 95CI of [0.64, 0.77] and CNN_{Simulated} AUC of 0.68 with a 95CI of [0.62, 0.75]. It showed that CGAN simulated mammograms can help MO cancer detection.      
### 52.Challenges and Opportunities of Speech Recognition for Bengali Language  [ :arrow_down: ](https://arxiv.org/pdf/2109.13217.pdf)
>  Speech recognition is a fascinating process that offers the opportunity to interact and command the machine in the field of human-computer interactions. Speech recognition is a language-dependent system constructed directly based on the linguistic and textual properties of any language. Automatic Speech Recognition (ASR) systems are currently being used to translate speech to text flawlessly. Although ASR systems are being strongly executed in international languages, ASR systems' implementation in the Bengali language has not reached an acceptable state. In this research work, we sedulously disclose the current status of the Bengali ASR system's research endeavors. In what follows, we acquaint the challenges that are mostly encountered while constructing a Bengali ASR system. We split the challenges into language-dependent and language-independent challenges and guide how the particular complications may be overhauled. Following a rigorous investigation and highlighting the challenges, we conclude that Bengali ASR systems require specific construction of ASR architectures based on the Bengali language's grammatical and phonetic structure.      
### 53.Faster and More Reliable Quantum SWAPs via Native Gates  [ :arrow_down: ](https://arxiv.org/pdf/2109.13199.pdf)
>  Due to the sparse connectivity of superconducting quantum computers, qubit communication via SWAP gates accounts for the vast majority of overhead in quantum programs. We introduce a method for improving the speed and reliability of SWAPs at the level of the superconducting hardware's native gateset. Our method relies on four techniques: 1) SWAP Orientation, 2) Cross-Gate Pulse Cancellation, 3) Commutation through Cross-Resonance, and 4) Cross-Resonance Polarity. Importantly, our Optimized SWAP is bootstrapped from the pre-calibrated gates, and therefore incurs zero calibration overhead. We experimentally evaluate our optimizations with Qiskit Pulse on IBM hardware. Our Optimized SWAP is 11% faster and 13% more reliable than the Standard SWAP. We also experimentally validate our optimizations on application-level benchmarks. Due to (a) the multiplicatively compounding gains from improved SWAPs and (b) the frequency of SWAPs, we observe typical improvements in success probability of 10-40%. The Optimized SWAP is available through the SuperstaQ platform.      
### 54.On the Synthesis of Bellman Inequalities for Data-Driven Optimal Control  [ :arrow_down: ](https://arxiv.org/pdf/2109.13193.pdf)
>  In the context of the linear programming (LP) approach to data-driven control, one assumes that the dynamical system is unknown but can be observed indirectly through data on its evolution. Both theoretical and empirical evidence suggest that a desired suboptimality gap is often only achieved with massive exploration of the state-space. In case of linear systems, we discuss how a relatively small but sufficiently rich dataset can be exploited to generate new constraints offline and without observing the corresponding transitions. Moreover, we show how to reconstruct the associated unknown stage-costs and, when the system is stochastic, we offer insights on the related problem of estimating the expected value in the Bellman operator without re-initializing the dynamics in the same state-input pairs.      
### 55.Dynamic Allocation of Visual Attention for Vision-based Autonomous Navigation under Data Rate Constraints  [ :arrow_down: ](https://arxiv.org/pdf/2109.13146.pdf)
>  This paper considers the problem of task-dependent (top-down) attention allocation for vision-based autonomous navigation using known landmarks. Unlike the existing paradigm in which landmark selection is formulated as a combinatorial optimization problem, we model it as a resource allocation problem where the decision-maker (DM) is granted extra freedom to control the degree of attention to each landmark. The total resource available to DM is expressed in terms of the capacity limit of the in-take information flow, which is quantified by the directed information from the state of the environment to the DM's observation. We consider a receding horizon implementation of such a controlled sensing scheme in the Linear-Quadratic-Gaussian (LQG) regime. The convex-concave procedure is applied in each time step, whose time complexity is shown to be linear in the horizon length if the alternating direction method of multipliers (ADMM) is used. Numerical studies show that the proposed formulation is sparsity-promoting in the sense that it tends to allocate zero data rate to uninformative landmarks.      
### 56.Optimization Landscape of Gradient Descent for Discrete-time Static Output Feedback  [ :arrow_down: ](https://arxiv.org/pdf/2109.13132.pdf)
>  In this paper, we analyze the optimization landscape of the gradient descent method for static output feedback (SOF) control of discrete-time linear time-invariant systems with quadratic cost. The SOF setting can be quite common, for example, when there are unmodeled hidden states in the underlying process. We first identify several important properties of the SOF cost function, including coercivity, L-smoothness, and M-Lipschitz continuous Hessian. Based on these results, we show that when the observation matrix has full column rank, gradient descent is able to converge to the global optimal controller at a linear rate. For the partially observed case, convergence to stationary points is obtained in general and the corresponding convergence rate is characterized. In the latter more challenging case, we further prove that under some mild conditions, gradient descent converges linearly to a local minimum if the starting point is close to one. These results not only characterize the performance of gradient descent in optimizing the SOF problem, but also shed light on the efficiency of general policy gradient methods in reinforcement learning.      
### 57.Inferring Facing Direction from Voice Signals  [ :arrow_down: ](https://arxiv.org/pdf/2109.13094.pdf)
>  Consider a home or office where multiple devices are running voice assistants (e.g., TVs, lights, ovens, refrigerators, etc.). A human user turns to a particular device and gives a voice command, such as ``Alexa, can you ...''. This paper focuses on the problem of detecting which device the user was facing, and therefore, enabling only that device to respond to the command. Our core intuition emerges from the fact that human voice exhibits a directional radiation pattern, and the orientation of this pattern should influence the signal received at each device. Unfortunately, indoor multipath, unknown user location, and unknown voice signals pose as critical hurdles. Through a new algorithm that estimates the line-of-sight (LoS) power from a given signal, and combined with beamforming and triangulation, we design a functional solution called CoDIR. Results from $500+$ configurations, across $5$ rooms and $9$ different users, are encouraging. While improvements are necessary, we believe this is an important step forward in a challenging but urgent problem space.      
### 58.Estimating Angle of Arrival (AoA) of multiple Echoes in a Steering Vector Space  [ :arrow_down: ](https://arxiv.org/pdf/2109.13072.pdf)
>  Consider a microphone array, such as those present in Amazon Echos, conference phones, or self-driving cars. One of the goals of these arrays is to decode the angles in which acoustic signals arrive at them. This paper considers the problem of estimating K angle of arrivals (AoA), i.e., the direct path's AoA and the AoA of subsequent echoes. Significant progress has been made on this problem, however, solutions remain elusive when the source signal is unknown (such as human voice) and the channel is strongly correlated (such as in multipath settings). Today's algorithms reliably estimate the direct-path-AoA, but the subsequent AoAs diverge in noisy real-world conditions. <br>We design SubAoA, an algorithm that improves on the current body of work. Our core idea models signal in a new AoA sub-space, and employs a cancellation approach that successively cancels each AoA to decode the next. We explain the behavior and complexity of the algorithm from the first principles, simulate the performance across a range of parameters, and present results from real-world experiments. Comparison against multiple existing algorithms like GCC-PHAT, MUSIC, and VoLoc shows increasing gains for the latter AoAs, while our computation complexity allows real-time operation. We believe progress in multi-AoA estimation is a fundamental building block to various acoustic and RF applications, including human or vehicle localization, multi-user separation, and even (blind) channel estimation.      
### 59.Channel Customization for Joint Tx-RISs-Rx Design in Hybrid mmWave systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.13058.pdf)
>  In strong line-of-sight millimeter-wave (mmWave) wireless systems, the rank-deficient channel severely hampers spatial multiplexing. To address this inherent deficiency, distributed reconfigurable intelligent surfaces (RISs) are introduced in this study to customize the wireless channel. Capitalizing on the ability of the RIS to reshape electromagnetic waves, we theoretically show that a favorable channel with an arbitrary tunable rank and a minimized truncated condition number can be established by elaborately designing the placement and reflection matrix of RISs. Different from existing works on distributed RISs, the number of elements needed for each RIS to combat the path loss and the limited phase control is also considered in this research. On the basis of the proposed channel customization, a joint transmitter-RISs-receiver (Tx-RISs-Rx) design under a hybrid mmWave system is investigated to maximize the downlink spectral efficiency. Using the proposed scheme, the optimal singular value decomposition-based hybrid beamforming at the Tx and Rx can be easily obtained without matrix decomposition for the digital and analog beamforming. The bottoms of the sub-channel mode in the water-filling power allocation algorithm, which are conventionally uncontrollable when the noise power is fixed, are proven to be independently adjustable by RISs. Moreover, the transmit power required for realizing multi-stream transmission is derived. Numerical results are presented to verify our theoretical analysis and exhibit substantial gains over systems without RISs.      
### 60.Efficiently Training On-Policy Actor-Critic Networks in Robotic Deep Reinforcement Learning with Demonstration-like Sampled Exploration  [ :arrow_down: ](https://arxiv.org/pdf/2109.13005.pdf)
>  In complex environments with high dimension, training a reinforcement learning (RL) model from scratch often suffers from lengthy and tedious collection of agent-environment interactions. Instead, leveraging expert demonstration to guide RL agent can boost sample efficiency and improve final convergence. In order to better integrate expert prior with on-policy RL models, we propose a generic framework for Learning from Demonstration (LfD) based on actor-critic algorithms. Technically, we first employ K-Means clustering to evaluate the similarity of sampled exploration with demonstration data. Then we increase the likelihood of actions in similar frames by modifying the gradient update strategy to leverage demonstration. We conduct experiments on 4 standard benchmark environments in Mujoco and 2 self-designed robotic environments. Results show that, under certain condition, our algorithm can improve sample efficiency by 20% ~ 40%. By combining our framework with on-policy algorithms, RL models can accelerate convergence and obtain better final mean episode rewards especially in complex robotic context where interactions are expensive.      
### 61.High-Rate Uninterrupted Internet-of-Vehicle Communications in Highways: Dynamic Blockage Avoidance and CSIT Acquisition  [ :arrow_down: ](https://arxiv.org/pdf/2109.12878.pdf)
>  In future wireless networks, one of the use-cases of interest is Internet-of-vehicles (IoV). Here, IoV refers to two different functionalities, namely, serving the in-vehicle users and supporting the connected-vehicle functionalities, where both can be well provided by the transceivers installed on top of vehicles. Such dual functionality of on-vehicle transceivers, however, implies strict rate and reliability requirements, for which one may need to utilize large bandwidths/beamforming, acquire up-to-date channel state information (CSI) and avoid blockages. In this article, we incorporate the recently proposed concept of predictor antennas (PAs) into a \textit{large-scale cooperative PA (LSCPA)} setup where both temporal blockages and CSI out-dating are avoided via base stations (BSs)/vehicles cooperation. Summarizing the ongoing standardization progress enabling IoV communications, we present the potentials and challenges of the LSCPA setup, and compare the effect of cooperative and non-cooperative schemes on the performance of IoV links. As we show, the BSs cooperation and blockage/CSI prediction can boost the performance of IoV links remarkably.      
### 62.Physics-Coupled Neural Network Magnetic Resonance Electrical Property Tomography (MREPT) for Conductivity Reconstruction  [ :arrow_down: ](https://arxiv.org/pdf/2109.12873.pdf)
>  The electrical property (EP) of human tissues is a quantitative biomarker that facilitates early diagnosis of cancerous tissues. Magnetic resonance electrical properties tomography (MREPT) is an imaging modality that reconstructs EPs by the radio-frequency field in an MRI system. MREPT reconstructs EPs by solving analytic models numerically based on Maxwell's equations. Most MREPT methods suffer from artifacts caused by inaccuracy of the hypotheses behind the models, and/or numerical errors. These artifacts can be mitigated by adding coefficients to stabilize the models, however, the selection of such coefficient has been empirical, which limit its medical application. Alternatively, end-to-end Neural networks-based MREPT (NN-MREPT) learns to reconstruct the EPs from training samples, circumventing Maxwell's equations. However, due to its pattern-matching nature, it is difficult for NN-MREPT to produce accurate reconstructions for new samples. In this work, we proposed a physics-coupled NN for MREPT (PCNN-MREPT), in which an analytic model, cr-MREPT, works with diffusion and convection coefficients, learned by NNs from the difference between the reconstructed and ground-truth EPs to reduce artifacts. With two simulated datasets, three generalization experiments in which test samples deviate gradually from the training samples, and one noise-robustness experiment were conducted. The results show that the proposed PCNN-MREPT achieves higher accuracy than two representative analytic methods. Moreover, compared with an end-to-end NN-MREPT, the proposed method attained higher accuracy in two critical generalization tests. This is an important step to practical MREPT medical diagnoses.      
### 63.Improving Uncertainty of Deep Learning-based Object Classification on Radar Spectra using Label Smoothing  [ :arrow_down: ](https://arxiv.org/pdf/2109.12851.pdf)
>  Object type classification for automotive radar has greatly improved with recent deep learning (DL) solutions, however these developments have mostly focused on the classification accuracy. Before employing DL solutions in safety-critical applications, such as automated driving, an indispensable prerequisite is the accurate quantification of the classifiers' reliability. Unfortunately, DL classifiers are characterized as black-box systems which output severely over-confident predictions, leading downstream decision-making systems to false conclusions with possibly catastrophic consequences. We find that deep radar classifiers maintain high-confidences for ambiguous, difficult samples, e.g. small objects measured at large distances, under domain shift and signal corruptions, regardless of the correctness of the predictions. The focus of this article is to learn deep radar spectra classifiers which offer robust real-time uncertainty estimates using label smoothing during training. Label smoothing is a technique of refining, or softening, the hard labels typically available in classification datasets. In this article, we exploit radar-specific know-how to define soft labels which encourage the classifiers to learn to output high-quality calibrated uncertainty estimates, thereby partially resolving the problem of over-confidence. Our investigations show how simple radar knowledge can easily be combined with complex data-driven learning algorithms to yield safe automotive radar perception.      
### 64.Emotional Speech Synthesis for Companion Robot to Imitate Professional Caregiver Speech  [ :arrow_down: ](https://arxiv.org/pdf/2109.12787.pdf)
>  When people try to influence others to do something, they subconsciously adjust their speech to include appropriate emotional information. In order for a robot to influence people in the same way, the robot should be able to imitate the range of human emotions when speaking. To achieve this, we propose a speech synthesis method for imitating the emotional states in human speech. In contrast to previous methods, the advantage of our method is that it requires less manual effort to adjust the emotion of the synthesized speech. Our synthesizer receives an emotion vector to characterize the emotion of synthesized speech. The vector is automatically obtained from human utterances by using a speech emotion recognizer. We evaluated our method in a scenario when a robot tries to regulate an elderly person's circadian rhythm by speaking to the person using appropriate emotional states. For the target speech to imitate, we collected utterances from professional caregivers when they speak to elderly people at different times of the day. Then we conducted a subjective evaluation where the elderly participants listened to the speech samples generated by our method. The results showed that listening to the samples made the participants feel more active in the early morning and calmer in the middle of the night. This suggests that the robot may be able to adjust the participants' circadian rhythm and that the robot can potentially exert influence similarly to a person.      
### 65.High Frame Rate Video Quality Assessment using VMAF and Entropic Differences  [ :arrow_down: ](https://arxiv.org/pdf/2109.12785.pdf)
>  The popularity of streaming videos with live, high-action content has led to an increased interest in High Frame Rate (HFR) videos. In this work we address the problem of frame rate dependent Video Quality Assessment (VQA) when the videos to be compared have different frame rate and compression factor. The current VQA models such as VMAF have superior correlation with perceptual judgments when videos to be compared have same frame rates and contain conventional distortions such as compression, scaling etc. However this framework requires additional pre-processing step when videos with different frame rates need to be compared, which can potentially limit its overall performance. Recently, Generalized Entropic Difference (GREED) VQA model was proposed to account for artifacts that arise due to changes in frame rate, and showed superior performance on the LIVE-YT-HFR database which contains frame rate dependent artifacts such as judder, strobing etc. In this paper we propose a simple extension, where the features from VMAF and GREED are fused in order to exploit the advantages of both models. We show through various experiments that the proposed fusion framework results in more efficient features for predicting frame rate dependent video quality. We also evaluate the fused feature set on standard non-HFR VQA databases and obtain superior performance than both GREED and VMAF, indicating the combined feature set captures complimentary perceptual quality information.      
### 66.Improving the Thermal Infrared Monitoring of Volcanoes: A Deep Learning Approach for Intermittent Image Series  [ :arrow_down: ](https://arxiv.org/pdf/2109.12767.pdf)
>  Active volcanoes are globally distributed and pose societal risks at multiple geographic scales, ranging from local hazards to regional/international disruptions. Many volcanoes do not have continuous ground monitoring networks; meaning that satellite observations provide the only record of volcanic behavior and unrest. Among these remote sensing observations, thermal imagery is inspected daily by volcanic observatories for examining the early signs, onset, and evolution of eruptive activity. However, thermal scenes are often obstructed by clouds, meaning that forecasts must be made off image sequences whose scenes are only usable intermittently through time. Here, we explore forecasting this thermal data stream from a deep learning perspective using existing architectures that model sequences with varying spatiotemporal considerations. Additionally, we propose and evaluate new architectures that explicitly model intermittent image sequences. Using ASTER Kinetic Surface Temperature data for $9$ volcanoes between $1999$ and $2020$, we found that a proposed architecture (ConvLSTM + Time-LSTM + U-Net) forecasts volcanic temperature imagery with the lowest RMSE ($4.164^{\circ}$C, other methods: $4.217-5.291^{\circ}$C). Additionally, we examined performance on multiple time series derived from the thermal imagery and the effect of training with data from singular volcanoes. Ultimately, we found that models with the lowest RMSE on forecasting imagery did not possess the lowest RMSE on recreating time series derived from that imagery and that training with individual volcanoes generally worsened performance relative to a multi-volcano data set. This work highlights the potential of data-driven deep learning models for volcanic unrest forecasting while revealing the need for carefully constructed optimization targets.      
### 67.Equilibria and learning dynamics in mixed network coordination/anti-coordination games  [ :arrow_down: ](https://arxiv.org/pdf/2109.12692.pdf)
>  Whilst network coordination games and network anti-coordination games have received a considerable amount of attention in the literature, network games with coexisting coordinating and anti-coordinating players are known to exhibit more complex behaviors. In fact, depending on the network structure, such games may even fail to have pure-strategy Nash equilibria. An example is represented by the well-known matching pennies (discoordination) game. <br>In this work, we first provide graph-theoretic conditions for the existence of pure-strategy Nash equilibria in mixed network coordination/anti-coordination games of arbitrary size. For the case where such conditions are met, we then study the asymptotic behavior of best-response dynamics and provide sufficient conditions for finite-time convergence to the set of Nash equilibria. Our results build on an extension and refinement of the notion of network cohesiveness and on the formulation of the new concept of network indecomposibility.      
### 68.Soundata: A Python library for reproducible use of audio datasets  [ :arrow_down: ](https://arxiv.org/pdf/2109.12690.pdf)
>  Soundata is a Python library for loading and working with audio datasets in a standardized way, removing the need for writing custom loaders in every project, and improving reproducibility by providing tools to validate data against a canonical version. It speeds up research pipelines by allowing users to quickly download a dataset, load it into memory in a standardized and reproducible way, validate that the dataset is complete and correct, and more. Soundata is based and inspired on mirdata and design to complement mirdata by working with environmental sound, bioacoustic and speech datasets, among others. Soundata was created to be easy to use, easy to contribute to, and to increase reproducibility and standardize usage of sound datasets in a flexible way.      
### 69.Robust Coordination in Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.12685.pdf)
>  We study the robustness of binary-action heterogeneous network coordination games equipped with an external field modeling the different players' biases towards one action with respect to the other one. We prove necessary and sufficient conditions for global stability of consensus equilibria under best response type dynamics, robustly with respect to (constant or time-varying) values of the external field. We then apply these results to the analysis of mixed network coordination and anti-coordination games and find sufficient conditions for existence and global stability of pure strategy Nash equilibria. Our results apply to general weighted directed interaction networks and build on supermodularity properties of the coordination games in order to characterize conditions for the existence of a novel notion of robust improvement and best response paths.      
### 70.Joint magnitude estimation and phase recovery using Cyle-in-cycle GAN for non-parallel speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2109.12591.pdf)
>  For the lack of adequate paired noisy-clean speech corpus in many real scenarios, non-parallel training is a promising task for DNN-based speech enhancement methods. However, because of the severe mismatch between input and target speech, many previous studies only focus on magnitude spectrum estimation and remain the phase unaltered, resulting in the degraded speech quality under low signal-to-noise ratio conditions. To tackle this problem, we decouple the difficult target $\emph{w.r.t.}$ original spectrum optimization into spectral magnitude and phase, and propose a novel Cycle-in-cycle generative adversarial network (dubbed CinCGAN) to jointly estimate the spectral magnitude and phase information stage by stage. In the first stage, we pretrain a magnitude CycleGAN to coarsely denoise the spectral magnitude spectrum. In the second stage, we couple the pretrained CycleGAN with a complex-valued CycleGAN as a cycle-in-cycle structure to recover phase information and refine the spectral magnitude simultaneously. The experimental results on the VoiceBank + Demand show that the proposed approach significantly outperforms previous baselines under non-parallel training. Experiments on training the models with standard paired data also show that the proposed method can achieve remarkable performance.      
### 71.A Video Summarization Method Using Temporal Interest Detection and Key Frame Prediction  [ :arrow_down: ](https://arxiv.org/pdf/2109.12581.pdf)
>  In this paper, a Video Summarization Method using Temporal Interest Detection and Key Frame Prediction is proposed for supervised video summarization, where video summarization is formulated as a combination of sequence labeling and temporal interest detection problem. In our method, we firstly built a flexible universal network frame to simultaneously predicts frame-level importance scores and temporal interest segments, and then combine the two components with different weights to achieve a more detailed video summarization. Extensive experiments and analysis on two benchmark datasets prove the effectiveness of our method. Specifically, compared with other state-of-the-art methods, its performance is increased by at least 2.6% and 4.2% on TVSum and SumMe respectively.      
### 72.Effective Testing Policies for Controlling an Epidemic Outbreak  [ :arrow_down: ](https://arxiv.org/pdf/2109.12553.pdf)
>  Testing is a crucial control mechanism for an epidemic outbreak because it enables the health authority to detect and isolate the infected cases, thereby limiting the disease transmission to susceptible people, when no effective treatment or vaccine is available. In this paper, an epidemic model that incorporates the testing rate as a control input is presented. The proposed model distinguishes between the undetected infected and the detected infected cases with the latter assumed to be isolated from the disease spreading process in the population. Two testing policies, effective during the onset of an epidemic when no treatment or vaccine is available, are devised: (i) best-effort strategy for testing (BEST) and (ii) constant optimal strategy for testing (COST). The BEST is a suppression policy that provides a lower bound on the testing rate to stop the growth of the epidemic. The COST is a mitigation policy that minimizes the peak of the epidemic by providing a constant, optimal allocation of tests in a certain time interval when the total stockpile of tests is limited. Both testing policies are evaluated by their impact on the number of active intensive care unit (ICU) cases and the cumulative number of deaths due to COVID-19 in France.      
### 73.Self-Supervised Learning for MRI Reconstruction with a Parallel Network Training Framework  [ :arrow_down: ](https://arxiv.org/pdf/2109.12502.pdf)
>  Image reconstruction from undersampled k-space data plays an important role in accelerating the acquisition of MR data, and a lot of deep learning-based methods have been exploited recently. Despite the achieved inspiring results, the optimization of these methods commonly relies on the fully-sampled reference data, which are time-consuming and difficult to collect. To address this issue, we propose a novel self-supervised learning method. Specifically, during model optimization, two subsets are constructed by randomly selecting part of k-space data from the undersampled data and then fed into two parallel reconstruction networks to perform information recovery. Two reconstruction losses are defined on all the scanned data points to enhance the network's capability of recovering the frequency information. Meanwhile, to constrain the learned unscanned data points of the network, a difference loss is designed to enforce consistency between the two parallel networks. In this way, the reconstruction model can be properly trained with only the undersampled data. During the model evaluation, the undersampled data are treated as the inputs and either of the two trained networks is expected to reconstruct the high-quality results. The proposed method is flexible and can be employed in any existing deep learning-based method. The effectiveness of the method is evaluated on an open brain MRI dataset. Experimental results demonstrate that the proposed self-supervised method can achieve competitive reconstruction performance compared to the corresponding supervised learning method at high acceleration rates (4 and 8). The code is publicly available at \url{<a class="link-external link-https" href="https://github.com/chenhu96/Self-Supervised-MRI-Reconstruction" rel="external noopener nofollow">this https URL</a>}.      
### 74.General Theory of Music by Icosahedron 3: Musical invariant and Melakarta raga  [ :arrow_down: ](https://arxiv.org/pdf/2109.12475.pdf)
>  Raga is a central musical concept in South Asia, especially India, and we investigate connections between Western classical music and Melakarta raga that is a raga in Karnatak (south Indian) classical music, through musical icosahedron. In our previous study, we introduced some kinds of musical icosahedra connecting various musical concepts in Western music: chromatic/whole tone musical icosahedra, Pythagorean/whole tone musical icosahedra, and exceptional musical icosahedra. In this paper, first, we introduce kinds of musical icosahedra that connect the above musical icosahedra through two kinds of permutations of 12 tones: inter-permutations and intra-permutations, and we call them intermediate musical icosahedra. Next, we define a neighboring number as a number of pairs of neighboring two tones in a given scale that neighbor each other on a given musical icosahedron, and we also define a musical invariant as a linear combination of the neighboring numbers. We find there exists a pair of a musical invariant and scales that is constant for some musical icosahedra and analyze their mathematical structure. Last, we define an extension of a given scale by the inter-permutations of a given musical icosahedron: the permutation-extension. Then, we show that the permutation-extension of the C major scale by Melakarta raga musical icosahedra that are four of the intermediate musical icosahedra from the type 1 chromatic/whole tone musical icosahedron to the type 1' Pythagorean/whole tone musical icosahedron, is a set of all the scales included in Melakarta raga. There exists a musical invariant that is constant for all the musical icosahedra corresponding to the scales of Melakarta raga, and we obtained a diagram representation of those scales characterizing the musical invariant.      
### 75.Rendering Spatial Sound for Interoperable Experiences in the Audio Metaverse  [ :arrow_down: ](https://arxiv.org/pdf/2109.12471.pdf)
>  Interactive audio spatialization technology previously developed for video game authoring and rendering has evolved into an essential component of platforms enabling shared immersive virtual experiences for future co-presence, remote collaboration and entertainment applications. New wearable virtual and augmented reality displays employ real-time binaural audio computing engines rendering multiple digital objects and supporting the free navigation of networked participants or their avatars through a juxtaposition of environments, real and virtual, often referred to as the Metaverse. These applications require a parametric audio scene programming interface to facilitate the creation and deployment of shared, dynamic and realistic virtual 3D worlds on mobile computing platforms and remote servers. <br>We propose a practical approach for designing parametric 6-degree-of-freedom object-based interactive audio engines to deliver the perceptually relevant binaural cues necessary for audio/visual and virtual/real congruence in Metaverse experiences. We address the effects of room reverberation, acoustic reflectors, and obstacles in both the virtual and real environments, and discuss how such effects may be driven by combinations of pre-computed and real-time acoustic propagation solvers. We envision an open scene description model distilled to facilitate the development of interoperable applications distributed across multiple platforms, where each audio object represents, to the user, a natural sound source having controllable distance, size, orientation, and acoustic radiation properties.      
### 76.Emergent behavior and neural dynamics in artificial agents tracking turbulent plumes  [ :arrow_down: ](https://arxiv.org/pdf/2109.12434.pdf)
>  Tracking a turbulent plume to locate its source is a complex control problem because it requires multi-sensory integration and must be robust to intermittent odors, changing wind direction, and variable plume statistics. This task is routinely performed by flying insects, often over long distances, in pursuit of food or mates. Several aspects of this remarkable behavior have been studied in detail in many experimental studies. Here, we take a complementary in silico approach, using artificial agents trained with reinforcement learning to develop an integrated understanding of the behaviors and neural computations that support plume tracking. Specifically, we use deep reinforcement learning (DRL) to train recurrent neural network (RNN) agents to locate the source of simulated turbulent plumes. Interestingly, the agents' emergent behaviors resemble those of flying insects, and the RNNs learn to represent task-relevant variables, such as head direction and time since last odor encounter. Our analyses suggest an intriguing experimentally testable hypothesis for tracking plumes in changing wind direction -- that agents follow local plume shape rather than the current wind direction. While reflexive short-memory behaviors are sufficient for tracking plumes in constant wind, longer timescales of memory are essential for tracking plumes that switch direction. At the level of neural dynamics, the RNNs' population activity is low-dimensional and organized into distinct dynamical structures, with some correspondence to behavioral modules. Our in silico approach provides key intuitions for turbulent plume tracking strategies and motivates future targeted experimental and theoretical developments.      
### 77.Blind Interference Alignment in 6G Optical Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2109.12433.pdf)
>  In recent years, the demand for high speed wireless networking has increased considerably due to the enormous number of devices connected to the Internet. In this context, optical wireless communication (OWC) has received tremendous interest in the context of next generation wireless networks where OWC offers a huge unlicensed bandwidth using optical bands. OWC systems are directional and can naturally provide multiple-input and multiple-output (MIMO) configurations serving multiple users using a high number of transmitters in the indoor environment to ensure coverage. Therefore, multiuser interference must be managed efficiently to enhance the performance of OWC networks considering different metrics. A transmission scheme referred to as blind interference alignment (BIA) is proposed for OWC systems to maximize the multiplexing gain without the need for channel state information (CSI) at the transmitters, which is difficult to achieve in MIMO scenarios. However, standard BIA avoids the need for CSI at the cost of requiring channel coherence time large enough for transmitting the whole transmission block. Moreover, the methodology of BIA results in increased noise with increase in the number of transmitters and users. Therefore, various network topologies such as network centric (NC) and user centric (UC) designs are proposed to relax the limitations of BIA where these topologies divide the receiving area into multiple clusters. The results show a significant enhancement in the performance of topological BIA compared with standard BIA.      
### 78.Verification of Switched Stochastic Systems via Barrier Certificates  [ :arrow_down: ](https://arxiv.org/pdf/2109.12420.pdf)
>  The paper presents a methodology for temporal logic verification of continuous-time switched stochastic systems. Our goal is to find the lower bound on the probability that a complex temporal property is satisfied over a finite time horizon. The required temporal properties of the system are expressed using a fragment of linear temporal logic, called safe-LTL with respect to finite traces. Our approach combines automata-based verification and the use of barrier certificates. It relies on decomposing the automaton associated with the negation of specification into a sequence of simpler reachability tasks and compute upper bounds for these reachability probabilities by means of common or multiple barrier certificates. Theoretical results are illustrated by applying a counter-example guided inductive synthesis framework to find barrier certificates.      
### 79.Communication-Efficient Distributed Linear and Deep Generalized Canonical Correlation Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2109.12400.pdf)
>  Classic and deep learning-based generalized canonical correlation analysis (GCCA) algorithms seek low-dimensional common representations of data entities from multiple ``views'' (e.g., audio and image) using linear transformations and neural networks, respectively. When the views are acquired and stored at different locations, organizations and edge devices, computing GCCA in a distributed, parallel and efficient manner is well-motivated. However, existing distributed GCCA algorithms may incur prohitively high communication overhead. This work puts forth a communication-efficient distributed framework for both linear and deep GCCA under the maximum variance (MAX-VAR) paradigm. The overhead issue is addressed by aggressively compressing (via quantization) the exchanging information between the distributed computing agents and a central controller. Compared to the unquantized version, the proposed algorithm consistently reduces the communication overhead by about $90\%$ with virtually no loss in accuracy and convergence speed. Rigorous convergence analyses are also presented -- which is a nontrivial effort since no existing generic result from quantized distributed optimization covers the special problem structure of GCCA. Our result shows that the proposed algorithms for both linear and deep GCCA converge to critical points in a sublinear rate, even under heavy quantization and stochastic approximations. In addition, it is shown that in the linear MAX-VAR case, the quantized algorithm approaches a {\it global optimum} in a {\it geometric} rate -- if the computing agents' updates meet a certain accuracy level. Synthetic and real data experiments are used to showcase the effectiveness of the proposed approach.      
### 80.TEMGNet: Deep Transformer-based Decoding of Upperlimb sEMG for Hand Gestures Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.12379.pdf)
>  There has been a surge of recent interest in Machine Learning (ML), particularly Deep Neural Network (DNN)-based models, to decode muscle activities from surface Electromyography (sEMG) signals for myoelectric control of neurorobotic systems. DNN-based models, however, require large training sets and, typically, have high structural complexity, i.e., they depend on a large number of trainable parameters. To address these issues, we developed a framework based on the Transformer architecture for processing sEMG signals. We propose a novel Vision Transformer (ViT)-based neural network architecture (referred to as the TEMGNet) to classify and recognize upperlimb hand gestures from sEMG to be used for myocontrol of prostheses. The proposed TEMGNet architecture is trained with a small dataset without the need for pre-training or fine-tuning. To evaluate the efficacy, following the-recent literature, the second subset (exercise B) of the NinaPro DB2 dataset was utilized, where the proposed TEMGNet framework achieved a recognition accuracy of 82.93% and 82.05% for window sizes of 300ms and 200ms, respectively, outperforming its state-of-the-art counterparts. Moreover, the proposed TEMGNet framework is superior in terms of structural capacity while having seven times fewer trainable parameters. These characteristics and the high performance make DNN-based models promising approaches for myoelectric control of neurorobots.      
### 81.A Principled Approach to Failure Analysis and Model Repairment: Demonstration in Medical Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2109.12347.pdf)
>  Machine learning models commonly exhibit unexpected failures post-deployment due to either data shifts or uncommon situations in the training environment. Domain experts typically go through the tedious process of inspecting the failure cases manually, identifying failure modes and then attempting to fix the model. In this work, we aim to standardise and bring principles to this process through answering two critical questions: (i) how do we know that we have identified meaningful and distinct failure types?; (ii) how can we validate that a model has, indeed, been repaired? We suggest that the quality of the identified failure types can be validated through measuring the intra- and inter-type generalisation after fine-tuning and introduce metrics to compare different subtyping methods. Furthermore, we argue that a model can be considered repaired if it achieves high accuracy on the failure types while retaining performance on the previously correct data. We combine these two ideas into a principled framework for evaluating the quality of both the identified failure subtypes and model repairment. We evaluate its utility on a classification and an object detection tasks. Our code is available at <a class="link-external link-https" href="https://github.com/Rokken-lab6/Failure-Analysis-and-Model-Repairment" rel="external noopener nofollow">this https URL</a>      
### 82.Beyond Robustness: A Taxonomy of Approaches towards Resilient Multi-Robot Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.12343.pdf)
>  Robustness is key to engineering, automation, and science as a whole. However, the property of robustness is often underpinned by costly requirements such as over-provisioning, known uncertainty and predictive models, and known adversaries. These conditions are idealistic, and often not satisfiable. Resilience on the other hand is the capability to endure unexpected disruptions, to recover swiftly from negative events, and bounce back to normality. In this survey article, we analyze how resilience is achieved in networks of agents and multi-robot systems that are able to overcome adversity by leveraging system-wide complementarity, diversity, and redundancy - often involving a reconfiguration of robotic capabilities to provide some key ability that was not present in the system a priori. As society increasingly depends on connected automated systems to provide key infrastructure services (e.g., logistics, transport, and precision agriculture), providing the means to achieving resilient multi-robot systems is paramount. By enumerating the consequences of a system that is not resilient (fragile), we argue that resilience must become a central engineering design consideration. Towards this goal, the community needs to gain clarity on how it is defined, measured, and maintained. We address these questions across foundational robotics domains, spanning perception, control, planning, and learning. One of our key contributions is a formal taxonomy of approaches, which also helps us discuss the defining factors and stressors for a resilient system. Finally, this survey article gives insight as to how resilience may be achieved. Importantly, we highlight open problems that remain to be tackled in order to reap the benefits of resilient robotic systems.      
### 83.Distributed Online Optimization with Byzantine Adversarial Agents  [ :arrow_down: ](https://arxiv.org/pdf/2109.12340.pdf)
>  We study the problem of non-constrained, discrete-time, online distributed optimization in a multi-agent system where some of the agents do not follow the prescribed update rule either due to failures or malicious intentions. None of the agents have prior information about the identities of the faulty agents and any agent can communicate only with its immediate neighbours. At each time step, a Lipschitz strongly convex cost function is revealed locally to all the agents and the non-faulty agents update their states using their local information and the information obtained from their neighbours. We measure the performance of the online algorithm by comparing it to its offline version when the cost functions are known apriori. The difference between the same is termed as regret. Under sufficient conditions on the graph topology, the number and location of the adversaries, the defined regret grows sublinearly. We further conduct numerical experiments to validate our theoretical results.      
### 84.Performance Analyses of TAS/Alamouti-MRC NOMA in Dual-Hop Full-Duplex AF Relaying Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.12326.pdf)
>  In this paper, the performance of a power domain downlink multiple-input multiple-output non-orthogonal multiple access system in dual-hop full-duplex (FD) relaying networks is investigated over Nakagami-$m$ fading channels by considering the channel estimation error and feedback delay. Particularly, in the investigated system, the base station equipped with multiple antennas transmits information to all mobile users by applying conventional transmit antenna selection/Alamouti-space-time block coding scheme with the help of a dedicated FD amplify-and-forward relay. The received signals at mobile users are combined according to maximal-ratio combining technique to exploit benefits of receive diversity. In order to demonstrate the superiority of the proposed system, outage probability (OP) is investigated and tight lower bound expressions are derived for the obtained OP. Moreover, asymptotic analyses are also conducted for ideal and practical conditions to provide further insights about the outage behavior in the high signal-to-noise ratio region. Finally, theoretical analyses are validated via Monte Carlo simulations and software defined radio based test-bed implementation.      
### 85.A Fast Computational Optimization for Optimal Control and Trajectory Planning for Obstacle Avoidance between Polytopes  [ :arrow_down: ](https://arxiv.org/pdf/2109.12313.pdf)
>  Obstacle avoidance between polytopes is a challenging topic for optimal control and optimization-based trajectory planning problems. Existing work either solves this problem through mixed-integer optimization, relying on simplification of system dynamics, or through model predictive control with dual variables using distance constraints, requiring long horizons for obstacle avoidance. In either case, the solution can only be applied as an offline planning algorithm. In this paper, we exploit the property that a smaller horizon is sufficient for obstacle avoidance by using discrete-time control barrier function (DCBF) constraints and we propose a novel optimization formulation with dual variables based on DCBFs to generate a collision-free dynamically-feasible trajectory. The proposed optimization formulation has lower computational complexity compared to existing work and can be used as a fast online algorithm for control and planning for general nonlinear dynamical systems. We validate our algorithm on different robot shapes using numerical simulations with a kinematic bicycle model, resulting in successful navigation through maze environments with polytopic obstacles.      
### 86.Adaptive video transmission using QUBO method and Digital Annealer based on Ising machine  [ :arrow_down: ](https://arxiv.org/pdf/2109.12293.pdf)
>  With the dramatically increasing video streaming in the total network traffic, it is critical to develop effective algorithms to promote the content delivery service of high quality. Adaptive bitrate (ABR) control is the most essential technique which determines the proper bitrate to be chosen based on network conditions, thus realize high-quality video streaming. In this paper, a novel ABR strategy is proposed based on Ising machine by using the quadratic unconstrained binary optimization (QUBO) method and Digital Annealer (DA) for the first time. The proposed method is evaluated by simulation with the real-world measured throughput, and compared with other state-of-the-art methods. Experiment results show that the proposed QUBO-based method can outperform the existing methods, which demonstrating the superior of the proposed QUBO-based method.      
### 87.A Variational Bayesian Inference-Inspired Unrolled Deep Network for MIMO Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.12275.pdf)
>  The great success of deep learning (DL) has inspired researchers to develop more accurate and efficient symbol detectors for multi-input multi-output (MIMO) systems. Existing DL-based MIMO detectors, however, suffer several drawbacks. To address these issues, in this paper, we develop a modeldriven DL detector based on variational Bayesian inference. Specifically, the proposed unrolled DL architecture is inspired by an inverse-free variational Bayesian learning framework which circumvents matrix inversion via maximizing a relaxed evidence lower bound. Two networks are respectively developed for independent and identically distributed (i.i.d.) Gaussian channels and arbitrarily correlated channels. The proposed networks, referred to as VBINet, have only a few learnable parameters and thus can be efficiently trained with a moderate amount of training samples. The proposed VBINet-based detectors can work in both offline and online training modes. An important advantage of our proposed networks over state-of-the-art MIMO detection networks such as OAMPNet and MMNet is that the VBINet can automatically learn the noise variance from data, thus yielding a significant performance improvement over the OAMPNet and MMNet in the presence of noise variance uncertainty. Simulation results show that the proposed VBINet-based detectors achieve competitive performance for both i.i.d. Gaussian and realistic 3GPP MIMO channels.      
### 88.Reducing the LQG Cost with Minimal Communication  [ :arrow_down: ](https://arxiv.org/pdf/2109.12246.pdf)
>  We study the linear quadratic Gaussian (LQG) control problem, in which the controller's observation of the system state is such that a desired cost is unattainable. To achieve the desired LQG cost, we introduce a communication link from the observer (encoder) to the controller. We investigate the optimal trade-off between the improved LQG cost and the consumed communication (information) resources, measured with the conditional directed information, across all encoding-decoding policies. The main result is a semidefinite programming formulation for that optimization problem in the finite-horizon scenario, which applies to time-varying linear dynamical systems. This result extends a seminal work by Tanaka et al., where the only information the controller knows about the system state arrives via a communication channel, to the scenario where the controller has also access to a noisy observation of the system state. As part of our derivation to show the optimiality of an encoder that transmits a memoryless Gaussian measurement of the state, we show that the presence of the controller's observations at the encoder can not reduce the minimal directed information. For time-invariant systems, where the optimal policy may be time-varying, we show in the infinite-horizon scenario that the optimal policy is time-invariant and can be computed explicitly from a solution of a finite-dimensional semidefinite programming. The results are demonstrated via examples that show that even low-quality measurements can have a significant impact on the required communication resources.      
### 89.Fast Successive-Cancellation List Flip Decoding of Polar Codes  [ :arrow_down: ](https://arxiv.org/pdf/2109.12239.pdf)
>  This work presents a fast successive-cancellation list flip (Fast-SCLF) decoding algorithm for polar codes that addresses the high latency issue associated with the successive-cancellation list flip (SCLF) decoding algorithm. We first propose a bit-flipping strategy tailored to the state-of-the-art fast successive-cancellation list (FSCL) decoding that avoids tree-traversal in the binary tree representation of SCLF, thus reducing the latency of the decoding process. We then derive a parameterized path-selection error model to accurately estimate the bit index at which the correct decoding path is eliminated from the initial FSCL decoding. The trainable parameter is optimized online based on an efficient supervised learning framework. Simulation results show that for a polar code of length 512 with 256 information bits, with similar error-correction performance and memory consumption, the proposed Fast-SCLF decoder reduces up to $73.4\%$ of the average decoding latency of the SCLF decoder with the same list size at the frame error rate of $10^{-4}$, while incurring a maximum computational overhead of $36.2\%$. For the same polar code of length 512 with 256 information bits and at practical signal-to-noise ratios, the proposed decoder with list size 4 reduces $89.1\%$ and $43.7\%$ of the average complexity and decoding latency of the FSCL decoder with list size 32 (FSCL-32), respectively, while also reducing $83.3\%$ of the memory consumption of FSCL-32. The significant improvements of the proposed decoder come at the cost of only $0.07$ dB error-correction performance degradation compared with FSCL-32.      
### 90.On the Fairness of Swarm Learning in Skin Lesion Classification  [ :arrow_down: ](https://arxiv.org/pdf/2109.12176.pdf)
>  in healthcare. However, the existing AI model may be biased in its decision marking. The bias induced by data itself, such as collecting data in subgroups only, can be mitigated by including more diversified data. Distributed and collaborative learning is an approach to involve training models in massive, heterogeneous, and distributed data sources, also known as nodes. In this work, we target on examining the fairness issue in Swarm Learning (SL), a recent edge-computing based decentralized machine learning approach, which is designed for heterogeneous illnesses detection in precision medicine. SL has achieved high performance in clinical applications, but no attempt has been made to evaluate if SL can improve fairness. To address the problem, we present an empirical study by comparing the fairness among single (node) training, SL, centralized training. Specifically, we evaluate on large public available skin lesion dataset, which contains samples from various subgroups. The experiments demonstrate that SL does not exacerbate the fairness problem compared to centralized training and improves both performance and fairness compared to single training. However, there still exists biases in SL model and the implementation of SL is more complex than the alternative two strategies.      
