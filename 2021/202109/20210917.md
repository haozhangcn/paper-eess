# ArXiv eess --Fri, 17 Sep 2021
### 1.Adversarial Attacks against Deep Learning Based Power Control in Wireless Communications  [ :arrow_down: ](https://arxiv.org/pdf/2109.08139.pdf)
>  We consider adversarial machine learning based attacks on power allocation where the base station (BS) allocates its transmit power to multiple orthogonal subcarriers by using a deep neural network (DNN) to serve multiple user equipments (UEs). The DNN that corresponds to a regression model is trained with channel gains as the input and allocated transmit powers as the output. While the BS allocates the transmit power to the UEs to maximize rates for all UEs, there is an adversary that aims to minimize these rates. The adversary may be an external transmitter that aims to manipulate the inputs to the DNN by interfering with the pilot signals that are transmitted to measure the channel gain. Alternatively, the adversary may be a rogue UE that transmits fabricated channel estimates to the BS. In both cases, the adversary carefully crafts adversarial perturbations to manipulate the inputs to the DNN of the BS subject to an upper bound on the strengths of these perturbations. We consider the attacks targeted on a single UE or all UEs. We compare these attacks with a benchmark, where the adversary scales down the input to the DNN. We show that adversarial attacks are much more effective than the benchmark attack in terms of reducing the rate of communications. We also show that adversarial attacks are robust to the uncertainty at the adversary including the erroneous knowledge of channel gains and the potential errors in exercising the attacks exactly as specified.      
### 2.NORESQA -- A Framework for Speech Quality Assessment using Non-Matching References  [ :arrow_down: ](https://arxiv.org/pdf/2109.08125.pdf)
>  The perceptual task of speech quality assessment (SQA) is a challenging task for machines to do. Objective SQA methods that rely on the availability of the corresponding clean reference have been the primary go-to approaches for SQA. Clearly, these methods fail in real-world scenarios where the ground truth clean references are not available. In recent years, non-intrusive methods that train neural networks to predict ratings or scores have attracted much attention, but they suffer from several shortcomings such as lack of robustness, reliance on labeled data for training and so on. In this work, we propose a new direction for speech quality assessment. Inspired by human's innate ability to compare and assess the quality of speech signals even when they have non-matching contents, we propose a novel framework that predicts a subjective relative quality score for the given speech signal with respect to any provided reference without using any subjective data. We show that neural networks trained using our framework produce scores that correlate well with subjective mean opinion scores (MOS) and are also competitive to methods such as DNSMOS, which explicitly relies on MOS from humans for training networks. Moreover, our method also provides a natural way to embed quality-related information in neural networks, which we show is helpful for downstream tasks such as speech enhancement.      
### 3.Neural Étendue Expander for Ultra-Wide-Angle High-Fidelity Holographic Display  [ :arrow_down: ](https://arxiv.org/pdf/2109.08123.pdf)
>  Holographic displays can generate light fields by dynamically modulating the wavefront of a coherent beam of light using a spatial light modulator, promising rich virtual and augmented reality applications. However, the limited spatial resolution of existing dynamic spatial light modulators imposes a tight bound on the diffraction angle. As a result, today's holographic displays possess low étendue, which is the product of the display area and the maximum solid angle of diffracted light. The low étendue forces a sacrifice of either the field of view (FOV) or the display size. In this work, we lift this limitation by presenting neural étendue expanders. This new breed of optical elements, which is learned from a natural image dataset, enables higher diffraction angles for ultra-wide FOV while maintaining both a compact form factor and the fidelity of displayed contents to human viewers. With neural étendue expanders, we achieve 64$\times$ étendue expansion of natural images with reconstruction quality (measured in PSNR) over 29dB on simulated retinal-resolution images. As a result, the proposed approach with expansion factor 64$\times$ enables high-fidelity ultra-wide-angle holographic projection of natural images using an 8K-pixel SLM, resulting in a 18.5 mm eyebox size and 2.18 steradians FOV, covering 85\% of the human stereo FOV.      
### 4.Stabilization of physical systems via saturated controllers with only partial state measurements  [ :arrow_down: ](https://arxiv.org/pdf/2109.08111.pdf)
>  This paper provides a constructive passivity-based control approach to solve the set-point regulation problem for input-affine continuous nonlinear systems while considering saturation in the inputs. As customarily in passivity-based control, the methodology consists of two steps: energy shaping and damping injection. In terms of applicability, the proposed controllers have two advantages concerning other passivity-based control techniques: (i) the energy shaping is carried out without solving partial differential equations, and (ii) the damping injection is performed without measuring the passive output. The proposed methodology is suitable to control a broad range of physical systems, e.g., mechanical, electrical, and electro-mechanical systems. We illustrate the applicability of the technique by designing controllers for systems in different physical domains, where we validate the analytical results via simulations and experiments.      
### 5.Deep Learning based Model-free Robust Load Restoration to Enhance Bulk System Resilience with Wind Power Penetration  [ :arrow_down: ](https://arxiv.org/pdf/2109.08074.pdf)
>  This paper proposes a new deep learning (DL) based model-free robust method for bulk system on-line load restoration with high penetration of wind power. Inspired by the iterative calculation of the two-stage robust load restoration model, the deep neural network (DNN) and deep convolutional neural network (CNN) are respectively designed to find the worst-case system condition of a load pickup decision and evaluate the corresponding security. In order to find the optimal result within a limited number of checks, a load pickup checklist generation (LPCG) algorithm is developed to ensure the optimality. Then, the fast robust load restoration strategy acquisition is achieved based on the designed one-line strategy generation (OSG) algorithm. The proposed method finds the optimal result in a model-free way, holds the robustness to handle uncertainties, and provides real-time computation. It can completely replace conventional robust optimization and supports on-line robust load restoration which better satisfies the changeable restoration process. The effectiveness of the proposed method is validated using the IEEE 30-bus system and the IEEE 118-bus system, showing high computational efficiency and considerable accuracy.      
### 6.Eformer: Edge Enhancement based Transformer for Medical Image Denoising  [ :arrow_down: ](https://arxiv.org/pdf/2109.08044.pdf)
>  In this work, we present Eformer - Edge enhancement based transformer, a novel architecture that builds an encoder-decoder network using transformer blocks for medical image denoising. Non-overlapping window-based self-attention is used in the transformer block that reduces computational requirements. This work further incorporates learnable Sobel-Feldman operators to enhance edges in the image and propose an effective way to concatenate them in the intermediate layers of our architecture. The experimental analysis is conducted by comparing deterministic learning and residual learning for the task of medical image denoising. To defend the effectiveness of our approach, our model is evaluated on the AAPM-Mayo Clinic Low-Dose CT Grand Challenge Dataset and achieves state-of-the-art performance, $i.e.$, 43.487 PSNR, 0.0067 RMSE, and 0.9861 SSIM. We believe that our work will encourage more research in transformer-based architectures for medical image denoising using residual learning.      
### 7.Robust Stability Analysis of an Uncertain Aircraft Model with Scalar Parametric Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2109.08027.pdf)
>  A robust controller is specified, and the stability bounds of the uncertain closed-loop system are determined using the small gain, circle, positive real, and Popov criteria. A graphical approach is employed in order to demonstrate the ease with which the above robustness tests can be carried out on a problem of practical interest. A significant improvement in stability bounds is observed as the analysis moves from the small gain test to the circle, positive real, and Popov tests. In particular, small gain analysis results in the most conservative robust stability bounds, while Popov analysis yields significantly less conservative bounds. This is because traditional small gain type tests allow the uncertainty to be arbitrarily time-varying, whereas Popov analysis restricts the uncertainty to be constant, real parametric uncertainty. Therefore, the results reported here indicate the conservatism associated with small gain analysis, and the effectiveness of Popov analysis, in gauging robust stability in the presence of constant, real parametric uncertainty.      
### 8.Learning from Peers: Transfer Reinforcement Learning for Joint Radio and Cache Resource Allocation in 5G Network Slicing  [ :arrow_down: ](https://arxiv.org/pdf/2109.07999.pdf)
>  Radio access network (RAN) slicing is an important part of network slicing in 5G. The evolving network architecture requires the orchestration of multiple network resources such as radio and cache resources. In recent years, machine learning (ML) techniques have been widely applied for network slicing. However, most existing works do not take advantage of the knowledge transfer capability in ML. In this paper, we propose a transfer reinforcement learning (TRL) scheme for joint radio and cache resources allocation to serve 5G RAN slicing.We first define a hierarchical architecture for the joint resources allocation. Then we propose two TRL algorithms: Q-value transfer reinforcement learning (QTRL) and action selection transfer reinforcement learning (ASTRL). In the proposed schemes, learner agents utilize the expert agents' knowledge to improve their performance on target tasks. The proposed algorithms are compared with both the model-free Q-learning and the model-based priority proportional fairness and time-to-live (PPF-TTL) algorithms. Compared with Q-learning, QTRL and ASTRL present 23.9% lower delay for Ultra Reliable Low Latency Communications slice and 41.6% higher throughput for enhanced Mobile Broad Band slice, while achieving significantly faster convergence than Q-learning. Moreover, 40.3% lower URLLC delay and almost twice eMBB throughput are observed with respect to PPF-TTL.      
### 9.An extremum seeking algorithm for monotone Nash equilibrium problems  [ :arrow_down: ](https://arxiv.org/pdf/2109.07975.pdf)
>  In this paper we consider the problem of finding a Nash equilibrium (NE) via zeroth-order feedback information in games with merely monotone pseudogradient mapping. Based on hybrid system theory, we propose a novel extremum seeking algorithm which converges to the set of Nash equilibria in a semi-global practical sense. Finally, we present two simulation examples. The first shows that the standard extremum seeking algorithm fails, while ours succeeds in reaching NE. In the second, we simulate an allocation problem with fixed demand.      
### 10.Quality-aware Cine Cardiac MRI Reconstruction and Analysis from Undersampled k-space Data  [ :arrow_down: ](https://arxiv.org/pdf/2109.07955.pdf)
>  Cine cardiac MRI is routinely acquired for the assessment of cardiac health, but the imaging process is slow and typically requires several breath-holds to acquire sufficient k-space profiles to ensure good image quality. Several undersampling-based reconstruction techniques have been proposed during the last decades to speed up cine cardiac MRI acquisition. However, the undersampling factor is commonly fixed to conservative values before acquisition to ensure diagnostic image quality, potentially leading to unnecessarily long scan times. In this paper, we propose an end-to-end quality-aware cine short-axis cardiac MRI framework that combines image acquisition and reconstruction with downstream tasks such as segmentation, volume curve analysis and estimation of cardiac functional parameters. The goal is to reduce scan time by acquiring only a fraction of k-space data to enable the reconstruction of images that can pass quality control checks and produce reliable estimates of cardiac functional parameters. The framework consists of a deep learning model for the reconstruction of 2D+t cardiac cine MRI images from undersampled data, an image quality-control step to detect good quality reconstructions, followed by a deep learning model for bi-ventricular segmentation, a quality-control step to detect good quality segmentations and automated calculation of cardiac functional parameters. To demonstrate the feasibility of the proposed approach, we perform simulations using a cohort of selected participants from the UK Biobank (n=270), 200 healthy subjects and 70 patients with cardiomyopathies. Our results show that we can produce quality-controlled images in a scan time reduced from 12 to 4 seconds per slice, enabling reliable estimates of cardiac functional parameters such as ejection fraction within 5% mean absolute error.      
### 11.PDAugment: Data Augmentation by Pitch and Duration Adjustments for Automatic Lyrics Transcription  [ :arrow_down: ](https://arxiv.org/pdf/2109.07940.pdf)
>  Automatic lyrics transcription (ALT), which can be regarded as automatic speech recognition (ASR) on singing voice, is an interesting and practical topic in academia and industry. ALT has not been well developed mainly due to the dearth of paired singing voice and lyrics datasets for model training. Considering that there is a large amount of ASR training data, a straightforward method is to leverage ASR data to enhance ALT training. However, the improvement is marginal when training the ALT system directly with ASR data, because of the gap between the singing voice and standard speech data which is rooted in music-specific acoustic characteristics in singing voice. In this paper, we propose PDAugment, a data augmentation method that adjusts pitch and duration of speech at syllable level under the guidance of music scores to help ALT training. Specifically, we adjust the pitch and duration of each syllable in natural speech to those of the corresponding note extracted from music scores, so as to narrow the gap between natural speech and singing voice. Experiments on DSing30 and Dali corpus show that the ALT system equipped with our PDAugment outperforms previous state-of-the-art systems by 5.9% and 18.1% WERs respectively, demonstrating the effectiveness of PDAugment for ALT.      
### 12.DDS: A new device-degraded speech dataset for speech enhancement  [ :arrow_down: ](https://arxiv.org/pdf/2109.07931.pdf)
>  A large and growing amount of speech content in real-life scenarios is being recorded on common consumer devices in uncontrolled environments, resulting in degraded speech quality. Transforming such low-quality device-degraded speech into high-quality speech is a goal of speech enhancement (SE). This paper introduces a new speech dataset, DDS, to facilitate the research on SE. DDS provides aligned parallel recordings of high-quality speech (recorded in professional studios) and a number of versions of low-quality speech, producing approximately 2,000 hours speech data. The DDS dataset covers 27 realistic recording conditions by combining diverse acoustic environments and microphone devices, and each version of a condition consists of multiple recordings from six different microphone positions to simulate various signal-to-noise ratio (SNR) and reverberation levels. We also test several SE baseline systems on the DDS dataset and show the impact of recording diversity on performance.      
### 13.Behavior of Keyword Spotting Networks Under Noisy Conditions  [ :arrow_down: ](https://arxiv.org/pdf/2109.07930.pdf)
>  Keyword spotting (KWS) is becoming a ubiquitous need with the advancement in artificial intelligence and smart devices. Recent work in this field have focused on several different architectures to achieve good results on datasets with low to moderate noise. However, the performance of these models deteriorates under high noise conditions as shown by our experiments. In our paper, we present an extensive comparison between state-of-the-art KWS networks under various noisy conditions. We also suggest adaptive batch normalization as a technique to improve the performance of the networks when the noise files are unknown during the training phase. The results of such high noise characterization enable future work in developing models that perform better in the aforementioned conditions.      
### 14.FSER: Deep Convolutional Neural Networks for Speech Emotion Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.07916.pdf)
>  Using mel-spectrograms over conventional MFCCs features, we assess the abilities of convolutional neural networks to accurately recognize and classify emotions from speech data. We introduce FSER, a speech emotion recognition model trained on four valid speech databases, achieving a high-classification accuracy of 95,05\%, over 8 different emotion classes: anger, anxiety, calm, disgust, happiness, neutral, sadness, surprise. On each benchmark dataset, FSER outperforms the best models introduced so far, achieving a state-of-the-art performance. We show that FSER stays reliable, independently of the language, sex identity, and any other external factor. Additionally, we describe how FSER could potentially be used to improve mental and emotional health care and how our analysis and findings serve as guidelines and benchmarks for further works in the same direction.      
### 15.Automated risk classification of colon biopsies based on semantic segmentation of histopathology images  [ :arrow_down: ](https://arxiv.org/pdf/2109.07892.pdf)
>  Artificial Intelligence (AI) can potentially support histopathologists in the diagnosis of a broad spectrum of cancer types. In colorectal cancer (CRC), AI can alleviate the laborious task of characterization and reporting on resected biopsies, including polyps, the numbers of which are increasing as a result of CRC population screening programs, ongoing in many countries all around the globe. Here, we present an approach to address two major challenges in automated assessment of CRC histopathology whole-slide images. First, we present an AI-based method to segment multiple tissue compartments in the H\&amp;E-stained whole-slide image, which provides a different, more perceptible picture of tissue morphology and composition. We test and compare a panel of state-of-the-art loss functions available for segmentation models, and provide indications about their use in histopathology image segmentation, based on the analysis of a) a multi-centric cohort of CRC cases from five medical centers in the Netherlands and Germany, and b) two publicly available datasets on segmentation in CRC. Second, we use the best performing AI model as the basis for a computer-aided diagnosis system (CAD) that classifies colon biopsies into four main categories that are relevant pathologically. We report the performance of this system on an independent cohort of more than 1,000 patients. The results show the potential of such an AI-based system to assist pathologists in diagnosis of CRC in the context of population screening. We have made the segmentation model available for research use on <a class="link-external link-https" href="https://grand-challenge.org/algorithms/colon-tissue-segmentation/" rel="external noopener nofollow">this https URL</a>.      
### 16.Automatic maneuver detection and tracking of space objects in optical survey scenarios based on stochastic hybrid systems formulation  [ :arrow_down: ](https://arxiv.org/pdf/2109.07801.pdf)
>  The state space representation of active resident space objects can be posed in the form of a stochastic hybrid system. Satellite maneuvers may be accounted for according to control cost or heuristical considerations, yet it is possible to jointly consider a combination of both. In this work, Sequential Monte Carlo filtering techniques are applied to the maneuvering target tracking problem in an optical survey scenario, where the maneuver control inputs are characterized in a Bayesian inference process. Due to the scarcity of data inherent to space surveillance and tracking, model switching probabilities are not estimated but derived from the ability of the state representation to fit incoming measurements. A Markov Chain Monte Carlo sampling scheme is used to explore the region assumed accessible to the object in terms of the hypothesized post-maneuver observation and a novel and efficient control distance metric. Results are obtained for a simulated optical survey scenario, and comparisons are drawn with respect to a moving horizon least-squares estimator. The proposed framework is proved to allow for a capable implementation of an automated online maneuver detection algorithm, thus contributing to the reduction of uncertainty in the state of active space objects.      
### 17.Towards Non-Line-of-Sight Photography  [ :arrow_down: ](https://arxiv.org/pdf/2109.07783.pdf)
>  Non-line-of-sight (NLOS) imaging is based on capturing the multi-bounce indirect reflections from the hidden objects. Active NLOS imaging systems rely on the capture of the time of flight of light through the scene, and have shown great promise for the accurate and robust reconstruction of hidden scenes without the need for specialized scene setups and prior assumptions. Despite that existing methods can reconstruct 3D geometries of the hidden scene with excellent depth resolution, accurately recovering object textures and appearance with high lateral resolution remains an challenging problem. In this work, we propose a new problem formulation, called NLOS photography, to specifically address this deficiency. Rather than performing an intermediate estimate of the 3D scene geometry, our method follows a data-driven approach and directly reconstructs 2D images of a NLOS scene that closely resemble the pictures taken with a conventional camera from the location of the relay wall. This formulation largely simplifies the challenging reconstruction problem by bypassing the explicit modeling of 3D geometry, and enables the learning of a deep model with a relatively small training dataset. The results are NLOS reconstructions of unprecedented lateral resolution and image quality.      
### 18.Design and Evaluation of Reconfigurable Intelligent Surfaces in Real-World Environment  [ :arrow_down: ](https://arxiv.org/pdf/2109.07763.pdf)
>  Reconfigurable intelligent surfaces (RISs) have promising coverage and data rate gains for wireless communication systems in 5G and beyond. Prior work has mainly focused on analyzing the performance of these surfaces using computer simulations or lab-level prototypes. To draw accurate insights about the actual performance of these systems, this paper develops an RIS proof-of-concept prototype and extensively evaluates its potential gains in the field and under realistic wireless communication settings. In particular, a 160-element reconfigurable surface, operating at a 5.8GHz band, is first designed, fabricated, and accurately measured in the anechoic chamber. This surface is then integrated into a wireless communication system and the beamforming gains, path-loss, and coverage improvements are evaluated in realistic outdoor communication scenarios. When both the transmitter and receiver employ directional antennas and with 5m and 10m distances between the transmitter-RIS and RIS-receiver, the developed RIS achieves $15$-$20$dB gain in the signal-to-noise ratio (SNR) in a range of $\pm60^\circ$ beamforming angles. In terms of coverage, and considering a far-field experiment with a blockage between a base station and a grid of mobile users and with an average distance of $35m$ between base station (BS) and the user (through the RIS), the RIS provides an average SNR improvement of $6$dB (max $8$dB) within an area $&gt; 75$m$^2$. Thanks to the scalable RIS design, these SNR gains can be directly increased with larger RIS areas. For example, a 1,600-element RIS with the same design is expected to provide around $26$dB SNR gain for a similar deployment. These results, among others, draw useful insights into the design and performance of RIS systems and provide an important proof for their potential gains in real-world far-field wireless communication environments.      
### 19.Utterance-level neural confidence measure for end-to-end children speech recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.07750.pdf)
>  Confidence measure is a performance index of particular importance for automatic speech recognition (ASR) systems deployed in real-world scenarios. In the present study, utterance-level neural confidence measure (NCM) in end-to-end automatic speech recognition (E2E ASR) is investigated. The E2E system adopts the joint CTC-attention Transformer architecture. The prediction of NCM is formulated as a task of binary classification, i.e., accept/reject the input utterance, based on a set of predictor features acquired during the ASR decoding process. The investigation is focused on evaluating and comparing the efficacies of predictor features that are derived from different internal and external modules of the E2E system. Experiments are carried out on children speech, for which state-of-the-art ASR systems show less than satisfactory performance and robust confidence measure is particularly useful. It is noted that predictor features related to acoustic information of speech play a more important role in estimating confidence measure than those related to linguistic information. N-best score features show significantly better performance than single-best ones. It has also been shown that the metrics of EER and AUC are not appropriate to evaluate the NCM of a mismatched ASR with significant performance gap.      
### 20.Beyond 5G RIS mmWave Systems: Where Communication and Localization Meet  [ :arrow_down: ](https://arxiv.org/pdf/2109.07729.pdf)
>  Upcoming beyond fifth generation (5G) communications systems aim at further enhancing key performance indicators and fully supporting brand new use cases by embracing emerging techniques, e.g., reconfigurable intelligent surface (RIS), integrated communication, localization, and sensing, and mmWave/THz communications. The wireless intelligence empowered by state-of-the-art artificial intelligence techniques has been widely considered at the transceivers, and now the paradigm is deemed to be shifted to the smart control of radio propagation environment by virtue of RISs. In this article, we argue that to harness the full potential of RISs, localization and communication must be tightly coupled. This is in sharp contrast to 5G and earlier generations, where localization was a minor additional service. To support this, we first introduce the fundamentals of RIS mmWave channel modeling, followed by RIS channel state information acquisition and link establishment. Then, we deal with the connection between localization and communications, from a separate and joint perspective.      
### 21.Secure Transmission for Hierarchical Information Accessibility in Downlink MU-MIMO  [ :arrow_down: ](https://arxiv.org/pdf/2109.07727.pdf)
>  Physical layer security is a useful tool to prevent confidential information from wiretapping. In this paper, we consider a generalized model of conventional physical layer security, referred to as hierarchical information accessibility (HIA). A main feature of the HIA model is that a network has a hierarchy in information accessibility, wherein decoding feasibility is determined by a priority of users. Under this HIA model, we formulate a sum secrecy rate maximization problem with regard to precoding vectors. This problem is challenging since multiple non-smooth functions are involved into the secrecy rate to fulfill the HIA conditions and also the problem is non-convex. To address the challenges, we approximate the minimum function by using the LogSumExp technique, thereafter obtain the first-order optimality condition. One key observation is that the derived condition is cast as a functional eigenvalue problem, where the eigenvalue is equivalent to the approximated objective function of the formulated problem. Accordingly, we show that finding a principal eigenvector is equivalent to finding a local optimal solution. To this end, we develop a novel method called generalized power iteration for HIA (GPI-HIA). Simulations demonstrate that the GPI-HIA significantly outperforms other baseline methods in terms of the secrecy rate.      
### 22.DeepMTS: Deep Multi-task Learning for Survival Prediction in Patients with Advanced Nasopharyngeal Carcinoma using Pretreatment PET/CT  [ :arrow_down: ](https://arxiv.org/pdf/2109.07711.pdf)
>  Nasopharyngeal Carcinoma (NPC) is a worldwide malignant epithelial cancer. Survival prediction is a major concern for NPC patients, as it provides early prognostic information that is needed to guide treatments. Recently, deep learning, which leverages Deep Neural Networks (DNNs) to learn deep representations of image patterns, has been introduced to the survival prediction in various cancers including NPC. It has been reported that image-derived end-to-end deep survival models have the potential to outperform clinical prognostic indicators and traditional radiomics-based survival models in prognostic performance. However, deep survival models, especially 3D models, require large image training data to avoid overfitting. Unfortunately, medical image data is usually scarce, especially for Positron Emission Tomography/Computed Tomography (PET/CT) due to the high cost of PET/CT scanning. Compared to Magnetic Resonance Imaging (MRI) or Computed Tomography (CT) providing only anatomical information of tumors, PET/CT that provides both anatomical (from CT) and metabolic (from PET) information is promising to achieve more accurate survival prediction. However, we have not identified any 3D end-to-end deep survival model that applies to small PET/CT data of NPC patients. In this study, we introduced the concept of multi-task leaning into deep survival models to address the overfitting problem resulted from small data. Tumor segmentation was incorporated as an auxiliary task to enhance the model's efficiency of learning from scarce PET/CT data. Based on this idea, we proposed a 3D end-to-end Deep Multi-Task Survival model (DeepMTS) for joint survival prediction and tumor segmentation. Our DeepMTS can jointly learn survival prediction and tumor segmentation using PET/CT data of only 170 patients with advanced NPC.      
### 23.Basil: A Fast and Byzantine-Resilient Approach for Decentralized Training  [ :arrow_down: ](https://arxiv.org/pdf/2109.07706.pdf)
>  Detection and mitigation of Byzantine behaviors in a decentralized learning setting is a daunting task, especially when the data distribution at the users is heterogeneous. As our main contribution, we propose Basil, a fast and computationally efficient Byzantine robust algorithm for decentralized training systems, which leverages a novel sequential, memory assisted and performance-based criteria for training over a logical ring while filtering the Byzantine users. In the IID dataset distribution setting, we provide the theoretical convergence guarantees of Basil, demonstrating its linear convergence rate. Furthermore, for the IID setting, we experimentally demonstrate that Basil is robust to various Byzantine attacks, including the strong Hidden attack, while providing up to ${\sim}16 \%$ higher test accuracy over the state-of-the-art Byzantine-resilient decentralized learning approach. Additionally, we generalize Basil to the non-IID dataset distribution setting by proposing Anonymous Cyclic Data Sharing (ACDS), a technique that allows each node to anonymously share a random fraction of its local non-sensitive dataset (e.g., landmarks images) with all other nodes. We demonstrate that Basil alongside ACDS with only $5\%$ data sharing provides effective toleration of Byzantine nodes, unlike the state-of-the-art Byzantine robust algorithm that completely fails in the heterogeneous data setting. Finally, to reduce the overall latency of Basil resulting from its sequential implementation over the logical ring, we propose Basil+. In particular, Basil+ provides scalability by enabling Byzantine-robust parallel training across groups of logical rings, and at the same time, it retains the performance gains of Basil due to sequential training within each group. Furthermore, we experimentally demonstrate the scalability gains of Basil+ through different sets of experiments.      
### 24.A Multi-Task Cross-Task Learning Architecture for Ad-hoc Uncertainty Estimation in 3D Cardiac MRI Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.07702.pdf)
>  Medical image segmentation has significantly benefitted thanks to deep learning architectures. Furthermore, semi-supervised learning (SSL) has recently been a growing trend for improving a model's overall performance by leveraging abundant unlabeled data. Moreover, learning multiple tasks within the same model further improves model generalizability. To generate smoother and accurate segmentation masks from 3D cardiac MR images, we present a Multi-task Cross-task learning consistency approach to enforce the correlation between the pixel-level (segmentation) and the geometric-level (distance map) tasks. Our extensive experimentation with varied quantities of labeled data in the training sets justifies the effectiveness of our model for the segmentation of the left atrial cavity from Gadolinium-enhanced magnetic resonance (GE-MR) images. With the incorporation of uncertainty estimates to detect failures in the segmentation masks generated by CNNs, our study further showcases the potential of our model to flag low-quality segmentation from a given model.      
### 25.Back to the Future: Efficient, Time-Consistent Solutions in Reach-Avoid Games  [ :arrow_down: ](https://arxiv.org/pdf/2109.07673.pdf)
>  We study the class of reach-avoid dynamic games in which multiple agents interact noncooperatively, and each wishes to satisfy a distinct target condition while avoiding a failure condition. Reach-avoid games are commonly used to express safety-critical optimal control problems found in mobile robot motion planning. While a wide variety of approaches exist for these motion planning problems, we focus on finding time-consistent solutions, in which planned future motion is still optimal despite prior suboptimal actions. Though abstract, time consistency encapsulates an extremely desirable property: namely, time-consistent motion plans remain optimal even when a robot's motion diverges from the plan early on due to, e.g., intrinsic dynamic uncertainty or extrinsic environment disturbances. Our main contribution is a computationally-efficient algorithm for multi-agent reach-avoid games which renders time-consistent solutions. We demonstrate our approach in a simulated driving scenario, where we construct a two-player adversarial game to model a range of defensive driving behaviors.      
### 26.Reachability of Linear Uncertain Systems: Sampling Based Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2109.07638.pdf)
>  In this work, we perform safety analysis of linear dynamical systems with uncertainties. Instead of computing a conservative overapproximation of the reachable set, our approach involves computing a statistical approximate reachable set. As a result, the guarantees provided by our method are probabilistic in nature. In this paper, we provide two different techniques to compute statistical approximate reachable set. We have implemented our algorithms in a python based prototype and demonstrate the applicability of our approaches on various case studies. We also provide an empirical comparison between the two proposed methods and with Flow*.      
### 27.Robustness of Safety for Linear Dynamical Systems: Symbolic and Numerical Approaches  [ :arrow_down: ](https://arxiv.org/pdf/2109.07632.pdf)
>  In this paper, we study the robustness of safety properties of a linear dynamical system with respect to model uncertainties. Our paper involves three parts. In the first part, we provide symbolic (analytical) and numerical (representation based) techniques for computing the reachable set of uncertain linear systems. We further prove a relationship between the reachable set of a linear uncertain system and the maximum singular value of the uncertain dynamics matrix. Finally, we propose two heuristics to compute the robustness threshold of the system -- the maximum uncertainty that can be introduced to the system without violating the safety property. We evaluate the reachable set computation techniques, effects of singular values, and estimation of robustness threshold on two case studies from varied domains, illustrating the applicability, practicality and scalability of the artifacts, proposed in this paper, on real-world examples. We further evaluate our artifacts on several linear dynamical system benchmarks. To the best of the authors' knowledge, this is the first work to: (i) extend perturbation theory to compute reachable sets of linear uncertain systems, (ii) leverage the relationship between the reachable set of a linear system and the maximum singular values to determine the effect of uncertainties and (3) estimate the threshold of robustness that can be tolerated by the system while remaining safe.      
### 28.Adaptive Control of Quadratic Costs in Linear Stochastic Differential Equations  [ :arrow_down: ](https://arxiv.org/pdf/2109.07630.pdf)
>  We study a canonical problem in adaptive control; design and analysis of policies for minimizing quadratic costs in unknown continuous-time linear dynamical systems. We address important challenges including accuracy of learning the unknown parameters of the underlying stochastic differential equation, as well as full analyses of performance degradation due to sub-optimal actions (i.e., regret). Then, an easy-to-implement algorithm for balancing exploration versus exploitation is proposed, followed by theoretical guarantees showing a square-root of time regret bound. Further, we present tight results for assuring system stability and for specifying fundamental limits for regret. To establish the presented results, multiple novel technical frameworks are developed, which can be of independent interests.      
### 29.Virtual Inertia Control of the Virtual Synchronous Generator: A Review  [ :arrow_down: ](https://arxiv.org/pdf/2109.07590.pdf)
>  With the increasing impact of low inertia due to the high penetration of distributed generation, virtual synchronous generator (VSG) technology has been proposed to improve the stability of the inverter-interfaced distributed generator by providing "virtual inertia". This paper presents a recent review of virtual inertia control covering significance, features, design principles, and state-of-art inertia strategies from both physical and mathematical perspectives to facilitate the wide application of the VSG. The definition and source of virtual inertia are given to help researchers to establish the concept of "virtual inertia". Then, this paper covers influencing mechanism studies of virtual inertia to reveal its functions. Also, a design framework of the virtual inertia is established by considering both the characteristics of the control system and the limitation of energy storage systems and renewable energy resources. Finally, several novel adaptive inertia control strategies are reviewed, and some aspects of potential future research are recommended.      
### 30.A Pathology Deep Learning System Capable of Triage of Melanoma Specimens Utilizing Dermatopathologist Consensus as Ground Truth  [ :arrow_down: ](https://arxiv.org/pdf/2109.07554.pdf)
>  Although melanoma occurs more rarely than several other skin cancers, patients' long term survival rate is extremely low if the diagnosis is missed. Diagnosis is complicated by a high discordance rate among pathologists when distinguishing between melanoma and benign melanocytic lesions. A tool that allows pathology labs to sort and prioritize melanoma cases in their workflow could improve turnaround time by prioritizing challenging cases and routing them directly to the appropriate subspecialist. We present a pathology deep learning system (PDLS) that performs hierarchical classification of digitized whole slide image (WSI) specimens into six classes defined by their morphological characteristics, including classification of "Melanocytic Suspect" specimens likely representing melanoma or severe dysplastic nevi. We trained the system on 7,685 images from a single lab (the reference lab), including the the largest set of triple-concordant melanocytic specimens compiled to date, and tested the system on 5,099 images from two distinct validation labs. We achieved Area Underneath the ROC Curve (AUC) values of 0.93 classifying Melanocytic Suspect specimens on the reference lab, 0.95 on the first validation lab, and 0.82 on the second validation lab. We demonstrate that the PDLS is capable of automatically sorting and triaging skin specimens with high sensitivity to Melanocytic Suspect cases and that a pathologist would only need between 30% and 60% of the caseload to address all melanoma specimens.      
### 31.Single-camera Two-Wavelength Imaging Pyrometry for Melt Pool Temperature Measurement and Monitoring in Laser Powder Bed Fusion based Additive Manufacturing  [ :arrow_down: ](https://arxiv.org/pdf/2109.07472.pdf)
>  Melt pool (MP) temperature is one of the determining factors and key signatures for the properties of printed components during metal additive manufacturing (AM). The state-of-the art measurement systems are hindered by both the equipment cost and the large-scale data acquisition and processing demands. In this work, we introduce a novel coaxial high-speed single-camera two-wavelength imaging pyrometer (STWIP) system as opposed to the typical utilization of multiple cameras for measuring MP temperature profiles through a laser powder bed fusion (LPBF) process. Developed on a commercial LPBF machine (EOS M290), the STWIP system is demonstrated to be able to quantitatively monitor MP temperature and variation for 50 layers at high framerates (&gt; 30,000 fps) during a print of five standard fatigue specimens. High performance computing is employed to analyze the acquired big data of MP images for determining each MPs average temperature and 2D temperature profile. The MP temperature evolution in the gage section of a fatigue specimen is also examined at a temporal resolution of 1ms by evaluating the derived MP temperatures of the printed samples first, middle and last layers. This paper is first of its kind on monitoring MP temperature distribution and evolution at such a large, detailed scale for longer durations in practical applications. Future work includes MP registration and machine learning of MP-Part Property relations.      
### 32.Validation and Improvement of Data Assimilation for Flood Hydrodynamic Modelling Using SAR Imagery Data  [ :arrow_down: ](https://arxiv.org/pdf/2109.07470.pdf)
>  Relevant comprehension of flood hazards has emerged as a crucial necessity, especially as the severity and the occurrence of flood events intensify with climate changes. Flood simulation and forecast capability have been greatly improved thanks to advances in data assimilation. This approach combines in-situ gauge measurements with hydrodynamic models, aiming to correct the hydraulic states and reduce the uncertainties in the model parameters, e.g., friction coefficients, inflow discharge. These methods depend strongly on the availability and quality of observations, thus requiring other data sources to improve the flood simulation and forecast quality. Sentinel-1 images collected during a flood event were used to classify an observed scene into dry and wet areas. The study area concerns the Garonne Marmandaise catchment, and focuses on recent flood event in January-February 2021. In this paper, seven experiments are carried out, two in free run modes (FR1 and FR2) and five in data assimilation modes (DA1 to DA5). A model-observation bias was diagnosed and corrected over the beginning of the flood event. Quantitative assessments are carried out involving 1D metrics at Vigicrue observing stations and 2D metrics with respect to the Sentinel-1 derived flood extent maps. They demonstrate improvements on flood extent representation thanks to the data assimilation and bias correction.      
### 33.Aesthetics and neural network image representations  [ :arrow_down: ](https://arxiv.org/pdf/2109.08103.pdf)
>  We analyze the spaces of images encoded by generative networks of the BigGAN architecture. We find that generic multiplicative perturbations away from the photo-realistic point often lead to images which appear as "artistic renditions" of the corresponding objects. This demonstrates an emergence of aesthetic properties directly from the structure of the photo-realistic environment coupled with its neural network parametrization. Moreover, modifying a deep semantic part of the neural network encoding leads to the appearance of symbolic visual representations.      
### 34.Urdu text in natural scene images: a new dataset and preliminary text detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.08060.pdf)
>  Text detection in natural scene images for content analysis is an interesting task. The research community has seen some great developments for English/Mandarin text detection. However, Urdu text extraction in natural scene images is a task not well addressed. In this work, firstly, a new dataset is introduced for Urdu text in natural scene images. The dataset comprises of 500 standalone images acquired from real scenes. Secondly, the channel enhanced Maximally Stable Extremal Region (MSER) method is applied to extract Urdu text regions as candidates in an image. Two-stage filtering mechanism is applied to eliminate non-candidate regions. In the first stage, text and noise are classified based on their geometric properties. In the second stage, a support vector machine classifier is trained to discard non-text candidate regions. After this, text candidate regions are linked using centroid-based vertical and horizontal distances. Text lines are further analyzed by a different classifier based on HOG features to remove non-text regions. Extensive experimentation is performed on the locally developed dataset to evaluate the performance. The experimental results show good performance on test set images. The dataset will be made available for research use. To the best of our knowledge, the work is the first of its kind for the Urdu language and would provide a good dataset for free research use and serve as a baseline performance on the task of Urdu text extraction.      
### 35.The pitfalls of using open data to develop deep learning solutions for COVID-19 detection in chest X-rays  [ :arrow_down: ](https://arxiv.org/pdf/2109.08020.pdf)
>  Since the emergence of COVID-19, deep learning models have been developed to identify COVID-19 from chest X-rays. With little to no direct access to hospital data, the AI community relies heavily on public data comprising numerous data sources. Model performance results have been exceptional when training and testing on open-source data, surpassing the reported capabilities of AI in pneumonia-detection prior to the COVID-19 outbreak. In this study impactful models are trained on a widely used open-source data and tested on an external test set and a hospital dataset, for the task of classifying chest X-rays into one of three classes: COVID-19, non-COVID pneumonia and no-pneumonia. Classification performance of the models investigated is evaluated through ROC curves, confusion matrices and standard classification metrics. Explainability modules are implemented to explore the image features most important to classification. Data analysis and model evaluations show that the popular open-source dataset COVIDx is not representative of the real clinical problem and that results from testing on this are inflated. Dependence on open-source data can leave models vulnerable to bias and confounding variables, requiring careful analysis to develop clinically useful/viable AI tools for COVID-19 detection in chest X-rays.      
### 36.Graph Fourier Transform based Audio Zero-watermarking  [ :arrow_down: ](https://arxiv.org/pdf/2109.08007.pdf)
>  The frequent exchange of multimedia information in the present era projects an increasing demand for copyright protection. In this work, we propose a novel audio zero-watermarking technology based on graph Fourier transform for enhancing the robustness with respect to copyright protection. In this approach, the combined shift operator is used to construct the graph signal, upon which the graph Fourier analysis is performed. The selected maximum absolute graph Fourier coefficients representing the characteristics of the audio segment are then encoded into a feature binary sequence using K-means algorithm. Finally, the resultant feature binary sequence is XOR-ed with the watermark binary sequence to realize the embedding of the zero-watermarking. The experimental studies show that the proposed approach performs more effectively in resisting common or synchronization attacks than the existing state-of-the-art methods.      
### 37.Channel Estimation for Extremely Large-Scale Massive MIMO: Far-Field, Near-Field, or Hybrid-Field?  [ :arrow_down: ](https://arxiv.org/pdf/2109.07883.pdf)
>  Extremely large-scale massive MIMO (XL-MIMO) is a promising technique for future 6G communications. The sharp increase of BS antennas leads to the unaffordable channel estimation overhead. Existing low-overhead channel estimation schemes are based on the far-field or near-field channel model. However, the far-field or near-field channel model mismatches the practical XL-MIMO channel feature, where some scatters are in the far-field region while others may locate in the near-field region, i.e., hybrid-field channel. Thus, existing far-field and near-field channel estimation schemes cannot be directly used to accurately estimate the hybrid-field XL-MIMO channel. To solve this problem, we propose an efficient hybrid-field channel estimation scheme by accurately modeling the XL-MIMO channel. Specifically, we firstly reveal the hybrid-field channel feature of the XL-MIMO channel. Then, we propose a hybrid-field channel model to capture this feature, which contains both the far-field and near-field path components. Finally, we propose a hybrid-field channel estimation scheme, where the far-field and near-field path components are respectively estimated. Simulation results show the proposed scheme performs better than existing schemes.      
### 38.Telehealthcare and Covid-19: A Noninvasive &amp; Low Cost Invasive, Scalable and Multimodal Real-Time Smartphone Application for Early Diagnosis of SARS-CoV-2 Infection  [ :arrow_down: ](https://arxiv.org/pdf/2109.07846.pdf)
>  The global coronavirus pandemic overwhelmed many health care systems, enforcing lockdown and encouraged work from home to control the spread of the virus and prevent overrunning of hospitalized patients. This prompted a sharp widespread use of telehealth to provide low-risk care for patients. Nevertheless, a continuous mutation into new variants and widespread unavailability of test kits, especially in developing countries, possess the challenge to control future potential waves of infection. In this paper, we propose a novel Smartphone application-based platform for early diagnosis of possible Covid-19 infected patients. The application provides three modes of diagnosis from possible symptoms, cough sound, and specific blood biomarkers. When a user chooses a particular setting and provides the necessary information, it sends the data to a trained machine learning (ML) model deployed in a remote server using the internet. The ML algorithm then predicts the possibility of contracting Covid-19 and sends the feedback to the user. The entire procedure takes place in real-time. Our machine learning models can identify Covid-19 patients with an accuracy of 100%, 95.65%, and 77.59% from blood parameters, cough sound, and symptoms respectively. Moreover, the ML sensitivity for blood and sound is 100%, which indicates correct identification of Covid positive patients. This is significant in limiting the spread of the virus. The multimodality offers multiplex diagnostic methods to better classify possible infectees and together with the instantaneous nature of our technique, demonstrates the power of telehealthcare as an easy and widespread low-cost scalable diagnostic solution for future pandemics.      
### 39.Model-driven Learning for Generic MIMO Downlink Beamforming With Uplink Channel Information  [ :arrow_down: ](https://arxiv.org/pdf/2109.07819.pdf)
>  Accurate downlink channel information is crucial to the beamforming design, but it is difficult to obtain in practice. This paper investigates a deep learning-based optimization approach of the downlink beamforming to maximize the system sum rate, when only the uplink channel information is available. Our main contribution is to propose a model-driven learning technique that exploits the structure of the optimal downlink beamforming to design an effective hybrid learning strategy with the aim to maximize the sum rate performance. This is achieved by jointly considering the learning performance of the downlink channel, the power and the sum rate in the training stage. The proposed approach applies to generic cases in which the uplink channel information is available, but its relation to the downlink channel is unknown and does not require an explicit downlink channel estimation. We further extend the developed technique to massive multiple-input multiple-output scenarios and achieve a distributed learning strategy for multicell systems without an inter-cell signalling overhead. Simulation results verify that our proposed method provides the performance close to the state of the art numerical algorithms with perfect downlink channel information and significantly outperforms existing data-driven methods in terms of the sum rate.      
### 40.Non-invasive super-resolution imaging through scattering media using fluctuating speckles  [ :arrow_down: ](https://arxiv.org/pdf/2109.07797.pdf)
>  Extending super-resolution imaging techniques to objects hidden in strongly scattering media potentially revolutionize the technical analysis for much broader categories of samples, such as biological tissues. The main challenge is the media's inhomogeneous structures which scramble the light path and create noise-like speckle patterns, hindering the object's visualization even at a low-resolution level. Here, we propose a computational method relying on the object's spatial and temporal fluctuation to visualize nanoscale objects through scattering media non-invasively. The fluctuating object can be achieved by random speckle illumination, illuminating through dynamic scattering media, or flickering emitters. The optical memory effect allows us to derive the object at diffraction limit resolution and estimate the point spreading function (PSF). Multiple images of the fluctuating object are obtained by deconvolution, then super-resolution images are achieved by computing the high order cumulants. Non-linearity of high order cumulant significantly suppresses the noise and artifacts in the resulting images and enhances the resolution by a factor of $\sqrt{N}$, where $N$ is the cumulant order. Our non-invasive super-resolution speckle fluctuation imaging (NISFFI) presents a nanoscopy technique with very simple hardware to visualize samples behind scattering media.      
### 41.Beam Tracking for UAV-Assisted FSO Links With a Four-Quadrant Detector  [ :arrow_down: ](https://arxiv.org/pdf/2109.07774.pdf)
>  A ground-to-air free-space optical link is studied for a hovering unmanned aerial vehicle (UAV) having multiple rotors. For this UAV, a four-quadrant array of photodetectors is used at the optical receiver to alleviate the adverse effect of hovering fluctuations by enlarging the receiver field-of-view. Extensive mathematical analysis is conducted to evaluate the beam tracking performance under the random effects of hovering fluctuations. The accuracy of the derived analytical expressions is corroborated by performing Monte-Carlo simulations. It is shown that the performance of such links depends heavily on the random fluctuations of hovering UAV, and, for each level of instability there is an optimal size for the array that minimizes the tracking error probability      
### 42.Sparse optimal stochastic control  [ :arrow_down: ](https://arxiv.org/pdf/2109.07716.pdf)
>  In this paper, we investigate a sparse optimal control of continuous-time stochastic systems. We adopt the dynamic programming approach and analyze the optimal control via the value function. Due to the non-smoothness of the $L^0$ cost functional, in general, the value function is not differentiable in the domain. Then, we characterize the value function as a viscosity solution to the associated Hamilton-Jacobi-Bellman (HJB) equation. Based on the result, we derive a necessary and sufficient condition for the $L^0$ optimality, which immediately gives the optimal feedback map. Especially for control-affine systems, we consider the relationship with $L^1$ optimal control problem and show an equivalence theorem.      
### 43.Mixed Control for Whole-Body Compliance of a Humanoid Robot  [ :arrow_down: ](https://arxiv.org/pdf/2109.07705.pdf)
>  The hierarchical quadratic programming (HQP) is commonly applied to consider strict hierarchies of multi-tasks and robot's physical inequality constraints during whole-body compliance. However, for the one-step HQP, the solution can oscillate when it is close to the boundary of constraints. It is because the abrupt hit of the bounds gives rise to unrealisable jerks and even infeasible solutions. This paper proposes the mixed control, which blends the single-axis model predictive control (MPC) and proportional derivate (PD) control for the whole-body compliance to overcome these deficiencies. The MPC predicts the distances between the bounds and the control target of the critical tasks, and it provides smooth and feasible solutions by prediction and optimisation in advance. However, applying MPC will inevitably increase the computation time. Therefore, to achieve a 500 Hz servo rate, the PD controllers still regulate other tasks to save computation resources. Also, we use a more efficient null space projection (NSP) whole-body controller instead of the HQP and distribute the single-axis MPCs into four CPU cores for parallel computation. Finally, we validate the desired capabilities of the proposed strategy via Simulations and the experiment on the humanoid robot Walker X.      
### 44.Convex Optimization of the Basic Reproduction Number  [ :arrow_down: ](https://arxiv.org/pdf/2109.07643.pdf)
>  The basic reproduction number $R_0$ is a fundamental quantity in epidemiological modeling, reflecting the typical number of secondary infections that arise from a single infected individual. While $R_0$ is widely known to scientists, policymakers, and the general public, it has received comparatively little attention in the controls community. This note provides two novel characterizations of $R_0$: a stability characterization and a geometric program characterization. The geometric program characterization allows us to write $R_0$-constrained and budget-constrained optimal resource allocation problems as geometric programs, which are easily transformed into convex optimization problems. We apply these programs to a case study of allocating vaccines and antidotes, finding that targeting $R_0$ instead of the spectral abscissa of the Jacobian matrix (a common target in the controls literature) leads to qualitatively different solutions.      
### 45.BacHMMachine: An Interpretable and Scalable Model for Algorithmic Harmonization for Four-part Baroque Chorales  [ :arrow_down: ](https://arxiv.org/pdf/2109.07623.pdf)
>  Algorithmic harmonization - the automated harmonization of a musical piece given its melodic line - is a challenging problem that has garnered much interest from both music theorists and computer scientists. One genre of particular interest is the four-part Baroque chorales of J.S. Bach. Methods for algorithmic chorale harmonization typically adopt a black-box, "data-driven" approach: they do not explicitly integrate principles from music theory but rely on a complex learning model trained with a large amount of chorale data. We propose instead a new harmonization model, called BacHMMachine, which employs a "theory-driven" framework guided by music composition principles, along with a "data-driven" model for learning compositional features within this framework. As its name suggests, BacHMMachine uses a novel Hidden Markov Model based on key and chord transitions, providing a probabilistic framework for learning key modulations and chordal progressions from a given melodic line. This allows for the generation of creative, yet musically coherent chorale harmonizations; integrating compositional principles allows for a much simpler model that results in vast decreases in computational burden and greater interpretability compared to state-of-the-art algorithmic harmonization methods, at no penalty to quality of harmonization or musicality. We demonstrate this improvement via comprehensive experiments and Turing tests comparing BacHMMachine to existing methods.      
### 46.A Column Streaming-Based Convolution Engine and Mapping Algorithm for CNN-based Edge AI accelerators  [ :arrow_down: ](https://arxiv.org/pdf/2109.07601.pdf)
>  Edge AI accelerators have been emerging as a solution for near customers' applications in areas such as unmanned aerial vehicles (UAVs), image recognition sensors, wearable devices, robotics, and remote sensing satellites. These applications not only require meeting performance targets but also meeting strict area and power constraints due to their portable mobility feature and limited power sources. As a result, a column streaming-based convolution engine has been proposed in this paper that includes column sets of processing elements design for flexibility in terms of the applicability for different CNN algorithms in edge AI accelerators. Comparing to a commercialized CNN accelerator, the key results reveal that the column streaming-based convolution engine requires similar execution cycles for processing a 227 x 227 feature map with avoiding zero-padding penalties.      
### 47.Direction-Assisted Beam Management in Full Duplex Millimeter Wave Massive MIMO Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.07596.pdf)
>  Recent applications of the Full Duplex (FD) technology focus on enabling simultaneous control communication and data transmission to reduce the control information exchange overhead, which impacts end-to-end latency and spectral efficiency. In this paper, we present a simultaneous direction estimation and data transmission scheme for millimeter Wave (mmWave) massive Multiple-Input Multiple-Output (MIMO) systems, enabled by a recent FD MIMO technology with reduced hardware complexity Self-Interference (SI) cancellation. We apply the proposed framework in the mmWave analog beam management problem, considering a base station equipped with a large transmit antenna array realizing downlink analog beamforming and few digitally controlled receive antenna elements used for uplink Direction-of-Arrival (DoA) estimation. A joint optimization framework for designing the DoA-assisted analog beamformer and the analog as well as digital SI cancellation is presented with the objective to maximize the achievable downlink rate. Our simulation results showcase that the proposed scheme outperforms its conventional half-duplex counterpart, yielding reduced DoA estimation error and superior downlink data rate.      
### 48.Multi-Task Learning with Sequence-Conditioned Transporter Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.07578.pdf)
>  Enabling robots to solve multiple manipulation tasks has a wide range of industrial applications. While learning-based approaches enjoy flexibility and generalizability, scaling these approaches to solve such compositional tasks remains a challenge. In this work, we aim to solve multi-task learning through the lens of sequence-conditioning and weighted sampling. First, we propose a new suite of benchmark specifically aimed at compositional tasks, MultiRavens, which allows defining custom task combinations through task modules that are inspired by industrial tasks and exemplify the difficulties in vision-based learning and planning methods. Second, we propose a vision-based end-to-end system architecture, Sequence-Conditioned Transporter Networks, which augments Goal-Conditioned Transporter Networks with sequence-conditioning and weighted sampling and can efficiently learn to solve multi-task long horizon problems. Our analysis suggests that not only the new framework significantly improves pick-and-place performance on novel 10 multi-task benchmark problems, but also the multi-task learning with weighted sampling can vastly improve learning and agent performances on individual tasks.      
### 49.Tied &amp; Reduced RNN-T Decoder  [ :arrow_down: ](https://arxiv.org/pdf/2109.07513.pdf)
>  Previous works on the Recurrent Neural Network-Transducer (RNN-T) models have shown that, under some conditions, it is possible to simplify its prediction network with little or no loss in recognition accuracy (<a class="link-https" data-arxiv-id="2003.07705" href="https://arxiv.org/abs/2003.07705">arXiv:2003.07705</a> [eess.AS], [2], <a class="link-https" data-arxiv-id="2012.06749" href="https://arxiv.org/abs/2012.06749">arXiv:2012.06749</a> [cs.CL]). This is done by limiting the context size of previous labels and/or using a simpler architecture for its layers instead of LSTMs. The benefits of such changes include reduction in model size, faster inference and power savings, which are all useful for on-device applications. <br>In this work, we study ways to make the RNN-T decoder (prediction network + joint network) smaller and faster without degradation in recognition performance. Our prediction network performs a simple weighted averaging of the input embeddings, and shares its embedding matrix weights with the joint network's output layer (a.k.a. weight tying, commonly used in language modeling <a class="link-https" data-arxiv-id="1611.01462" href="https://arxiv.org/abs/1611.01462">arXiv:1611.01462</a> [cs.LG]). This simple design, when used in conjunction with additional Edit-based Minimum Bayes Risk (EMBR) training, reduces the RNN-T Decoder from 23M parameters to just 2M, without affecting word-error rate (WER).      
### 50.Federated Contrastive Learning for Decentralized Unlabeled Medical Images  [ :arrow_down: ](https://arxiv.org/pdf/2109.07504.pdf)
>  A label-efficient paradigm in computer vision is based on self-supervised contrastive pre-training on unlabeled data followed by fine-tuning with a small number of labels. Making practical use of a federated computing environment in the clinical domain and learning on medical images poses specific challenges. In this work, we propose FedMoCo, a robust federated contrastive learning (FCL) framework, which makes efficient use of decentralized unlabeled medical data. FedMoCo has two novel modules: metadata transfer, an inter-node statistical data augmentation module, and self-adaptive aggregation, an aggregation module based on representational similarity analysis. To the best of our knowledge, this is the first FCL work on medical images. Our experiments show that FedMoCo can consistently outperform FedAvg, a seminal federated learning framework, in extracting meaningful representations for downstream tasks. We further show that FedMoCo can substantially reduce the amount of labeled data required in a downstream task, such as COVID-19 detection, to achieve a reasonable performance.      
### 51.Data-Driven Theory-guided Learning of Partial Differential Equations using SimultaNeous Basis Function Approximation and Parameter Estimation (SNAPE)  [ :arrow_down: ](https://arxiv.org/pdf/2109.07471.pdf)
>  The measured spatiotemporal response of various physical processes is utilized to infer the governing partial differential equations (PDEs). We propose SimultaNeous Basis Function Approximation and Parameter Estimation (SNAPE), a technique of parameter estimation of PDEs that is robust against high levels of noise nearly 100 %, by simultaneously fitting basis functions to the measured response and estimating the parameters of both ordinary and partial differential equations. The domain knowledge of the general multidimensional process is used as a constraint in the formulation of the optimization framework. SNAPE not only demonstrates its applicability on various complex dynamic systems that encompass wide scientific domains including Schrödinger equation, chaotic duffing oscillator, and Navier-Stokes equation but also estimates an analytical approximation to the process response. The method systematically combines the knowledge of well-established scientific theories and the concepts of data science to infer the properties of the process from the observed data.      
### 52.AMI-FML: A Privacy-Preserving Federated Machine Learning Framework for AMI  [ :arrow_down: ](https://arxiv.org/pdf/2109.05666.pdf)
>  Machine learning (ML) based smart meter data analytics is very promising for energy management and demand-response applications in the advanced metering infrastructure(AMI). A key challenge in developing distributed ML applications for AMI is to preserve user privacy while allowing active end-users participation. This paper addresses this challenge and proposes a privacy-preserving federated learning framework for ML applications in the AMI. We consider each smart meter as a federated edge device hosting an ML application that exchanges information with a central aggregator or a data concentrator, periodically. Instead of transferring the raw data sensed by the smart meters, the ML model weights are transferred to the aggregator to preserve privacy. The aggregator processes these parameters to devise a robust ML model that can be substituted at each edge device. We also discuss strategies to enhance privacy and improve communication efficiency while sharing the ML model parameters, suited for relatively slow network connections in the AMI. We demonstrate the proposed framework on a use case federated ML (FML) application that improves short-term load forecasting (STLF). We use a long short-term memory(LSTM) recurrent neural network (RNN) model for STLF. In our architecture, we assume that there is an aggregator connected to a group of smart meters. The aggregator uses the learned model gradients received from the federated smart meters to generate an aggregate, robust RNN model which improves the forecasting accuracy for individual and aggregated STLF. Our results indicate that with FML, forecasting accuracy is increased while preserving the data privacy of the end-users.      
### 53.Non-invasive optical focusing inside strongly scattering media with linear fluorescence  [ :arrow_down: ](https://arxiv.org/pdf/2002.01260.pdf)
>  Non-invasive optical focusing inside scattering media is still a big challenge because inhomogeneous media scatter both incoming photons for focusing and outgoing photons for observation. Various approaches, utilizing non-linear fluorescence or ultrasound, have been reported to address this difficulty. However, implementation of these methods is complicated and highly expensive, as ultrafast laser systems or photo-acoustic equipment must be employed. Here, we demonstrate a wavefront shaping technique to achieve non-invasive focusing (NiF) inside scattering media using only a linear fluorescent signal. Contrast and mean of incoherent speckles, produced by the linear fluorescence, are utilized as feedback signals to optimize the input wavefront. While increasing speckle contrast makes the focus tighter, and increasing the speckle mean enhances the intensity, fine-tuning the contribution of these two factors in our two-step optimization is essential. An optimal wavefront is found to achieve simultaneously both a micrometer focal spot size (down to 20 um diameter) and high intensity (more than a 100-fold enhancement) inside the scattering media. Our method promises a new route in life science towards focusing, imaging or manipulating deep into biological tissues with linear fluorescent agents.      
### 54.Single shot large field of view imaging with scattering media by spatial demultiplexing  [ :arrow_down: ](https://arxiv.org/pdf/1707.09577.pdf)
>  Optically focusing and imaging through strongly scattering media are challenging tasks but have widespread applications from scientific research to biomedical applications and daily life. Benefiting from the memory effect (ME) for speckle intensity correlations, only one single-shot speckle pattern can be used for the high quality recovery of the objects and avoiding some complicated procedures to reduce scattering effects. In spite of all the spatial information from a large object being embedded in a single speckle image, ME gives a strict limitation to the field of view (FOV) for imaging through scattering media. Objects beyond the ME region cannot be recovered and only produce unwanted speckle patterns, causing reduction in the speckle contrast and recovery quality. Here, we extract the spatial information by utilizing these unavoidable speckle patterns, and enlarge the FOV of the optical imaging system. Regional point spreading functions (PSFs), which are fixed and only need to be recorded once for all time use, are employed to recover corresponding spatial regions of an object by deconvolution algorithm. Then an automatic weighted averaging in an iterative process is performed to obtain the object with significantly enlarged FOV. Our results present an important step toward an advanced imaging technique with strongly scattering media.      
### 55.Single-shot multispectral imaging with a monochromatic camera  [ :arrow_down: ](https://arxiv.org/pdf/1707.09453.pdf)
>  Multispectral imaging plays an important role in many applications from astronomical imaging, earth observation to biomedical imaging. However, the current technologies are complex with multiple alignment-sensitive components, predetermined spatial and spectral parameters by manufactures. Here, we demonstrate a single-shot multispectral imaging technique that gives flexibility to end-users with a very simple optical setup, thank to spatial correlation and spectral decorrelation of speckle patterns. These seemingly random speckle patterns are point spreading functions (PSFs) generated by light from point sources propagating through a strongly scattering medium. The spatial correlation of PSFs allows image recovery with deconvolution techniques, while the spectral decorrelation allows them to play the role of tune-able spectral filters in the deconvolution process. Our demonstrations utilizing optical physics of strongly scattering media and computational imaging present the most cost-effective approach for multispectral imaging with great advantages.      
### 56.Enhancing security of optical cryptosystem with position-multiplexing and ultra-broadband illumination  [ :arrow_down: ](https://arxiv.org/pdf/1707.08701.pdf)
>  A position-multiplexing based cryptosystem is proposed to enhance the information security with an ultra-broadband illumination. The simplified optical encryption system only contains one diffuser acting as the random phase mask (RPM). Light coming from a plaintext passes through this RPM and generates the corresponding ciphertext on a camera. The proposed system effectively reduces problems of misalignment and coherent noise that are found in the coherent illumination. Here, the use of ultra-broadband illumination has the advantage of making a strong scattering and complex ciphertext by reducing the speckle contrast. Reduction of the ciphertext size further increases the strength of the ciphering. The unique spatial keys are utilized for the individual decryption as the plaintext locates at different spatial positions, and a complete decrypted image could be concatenated with high fidelity. Benefiting from the ultra-broadband illumination and position-multiplexing, the information of interest is scrambled together in a small ciphertext. Only the authorized user can decrypt this information with the correct keys. Therefore, a high performance security for a cryptosystem could be achieved.      
### 57.Local Sparse Approximation for Image Restoration with Adaptive Block Size Selection  [ :arrow_down: ](https://arxiv.org/pdf/1612.06738.pdf)
>  In this paper the problem of image restoration (denoising and inpainting) is approached using sparse approximation of local image blocks. The local image blocks are extracted by sliding square windows over the image. An adaptive block size selection procedure for local sparse approximation is proposed, which affects the global recovery of underlying image. Ideally the adaptive local block selection yields the minimum mean square error (MMSE) in recovered image. This framework gives us a clustered image based on the selected block size, then each cluster is restored separately using sparse approximation. The results obtained using the proposed framework are very much comparable with the recently proposed image restoration techniques.      
