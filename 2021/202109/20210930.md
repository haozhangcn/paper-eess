# ArXiv eess --Thu, 30 Sep 2021
### 1.On the properties of Laplacian pseudoinverses  [ :arrow_down: ](https://arxiv.org/pdf/2109.14587.pdf)
>  The pseudoinverse of a graph Laplacian is used in many applications and fields, such as for instance in the computation of the effective resistance in electrical networks, in the calculation of the hitting/commuting times for a Markov chain and in continuous-time distributed averaging problems. In this paper we show that the Laplacian pseudoinverse is in general not a Laplacian matrix but rather a signed Laplacian with the property of being an eventually exponentially positive matrix, i.e., of obeying a strong Perron-Frobenius property. We show further that the set of signed Laplacians with this structure (i.e., eventual exponential positivity) is closed with respect to matrix pseudoinversion. This is true even for signed digraphs, and provided that we restrict to Laplacians that are weight balanced also stability is guaranteed.      
### 2.Towards Flexible Blind JPEG Artifacts Removal  [ :arrow_down: ](https://arxiv.org/pdf/2109.14573.pdf)
>  Training a single deep blind model to handle different quality factors for JPEG image artifacts removal has been attracting considerable attention due to its convenience for practical usage. However, existing deep blind methods usually directly reconstruct the image without predicting the quality factor, thus lacking the flexibility to control the output as the non-blind methods. To remedy this problem, in this paper, we propose a flexible blind convolutional neural network, namely FBCNN, that can predict the adjustable quality factor to control the trade-off between artifacts removal and details preservation. Specifically, FBCNN decouples the quality factor from the JPEG image via a decoupler module and then embeds the predicted quality factor into the subsequent reconstructor module through a quality factor attention block for flexible control. Besides, we find existing methods are prone to fail on non-aligned double JPEG images even with only a one-pixel shift, and we thus propose a double JPEG degradation model to augment the training data. Extensive experiments on single JPEG images, more general double JPEG images, and real-world JPEG images demonstrate that our proposed FBCNN achieves favorable performance against state-of-the-art methods in terms of both quantitative metrics and visual quality.      
### 3.The Impact of Network Design Interventions on CPS Security  [ :arrow_down: ](https://arxiv.org/pdf/2109.14555.pdf)
>  We study a game-theoretic model of the interactions between a Cyber-Physical System's (CPS) operator (the defender) against an attacker who launches stepping-stone attacks to reach critical assets within the CPS. We consider that, in addition to optimally allocating its security budget to protect the assets, the defender may choose to modify the CPS through network design interventions. In particular, we propose and motivate four ways in which the defender can introduce additional nodes in the CPS: these nodes may be intended as additional safeguards, be added for functional or structural redundancies, or introduce additional functionalities in the system. We analyze the security implications of each of these design interventions, and evaluate their impacts on the security of an automotive network as our case study. We motivate the choice of the attack graph for this case study and elaborate how the parameters in the resulting security game are selected using the CVSS metrics and the ISO-26262 ASIL ratings as guidance. We then use numerical experiments to verify and evaluate how our proposed network interventions may be used to guide improvements in automotive security.      
### 4.An Energy Efficient Health Monitoring Approach with Wireless Body Area Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.14546.pdf)
>  Wireless Body Area Networks (WBANs) comprise a network of sensors subcutaneously implanted or placed near the body surface and facilitate continuous monitoring of health parameters of a patient. Research endeavours involving WBAN are directed towards effective transmission of detected parameters to a Local Processing Unit (LPU, usually a mobile device) and analysis of the parameters at the LPU or a back-end cloud. An important concern in WBAN is the lightweight nature of WBAN nodes and the need to conserve their energy. This is especially true for subcutaneously implanted nodes that cannot be recharged or regularly replaced. Work in energy conservation is mostly aimed at optimising the routing of signals to minimise energy expended. In this paper, a simple yet innovative approach to energy conservation and detection of alarming health status is proposed. Energy conservation is ensured through a two-tier approach wherein the first tier eliminates `uninteresting' health parameter readings at the site of a sensing node and prevents these from being transmitted across the WBAN to the LPU. A reading is categorised as uninteresting if it deviates very slightly from its immediately preceding reading and does not provide new insight on the patient's well being. In addition to this, readings that are faulty and emanate from possible sensor malfunctions are also eliminated. These eliminations are done at the site of the sensor using algorithms that are light enough to effectively function in the extremely resource-constrained environments of the sensor nodes. We notice, through experiments, that this eliminates and thus reduces around 90% of the readings that need to be transmitted to the LPU leading to significant energy savings. Furthermore, the proper functioning of these algorithms in such constrained environments is confirmed and validated over a hardware simulation set up. The second tier of assessment includes a proposed anomaly detection model at the LPU that is capable of identifying anomalies from streaming health parameter readings and indicates an adverse medical condition. In addition to being able to handle streaming data, the model works within the resource-constrained environments of an LPU and eliminates the need of transmitting the data to a back-end cloud, ensuring further energy savings. The anomaly detection capability of the model is validated using data available from the critical care units of hospitals and is shown to be superior to other anomaly detection techniques.      
### 5.Distributed Finite-Time Privacy-Preserving Optimal Allocation of Test Kits for Controlling Pandemic Spreading  [ :arrow_down: ](https://arxiv.org/pdf/2109.14481.pdf)
>  In this paper, we analyze the problem of optimally allocating resources in a distributed and privacy-preserving manner. We focus on the scenario where different cities/entities over a country aim to optimally allocate test kits according to their number of infections for controlling the spread of a pandemic. We propose a distributed privacy-preserving optimal resource allocation algorithm with efficient (i.e., quantized) communication over a directed communication network. The proposed algorithm allows each node to calculate the optimal allocation of test kits so that each node's ratio of test kits to infections equals the global ratio of test kits to infections in the network. The algorithm utilizes a fast distributed probabilistic coordination quantized average consensus algorithm with privacy-preserving guarantees. Furthermore, during the algorithm's operation each node processes and transmits quantized messages. We show that the algorithm converges in finite-time, and every node calculates the optimal allocation of test kits to infections. Additionally, we prove that, under specific conditions on the network topology, nodes are able to preserve the privacy of their initial state. Finally, we illustrate our proposed algorithm, present empirical simulation results regarding its convergence rate, and show how the utilized privacy-preservation mechanism affects the number of required time steps for convergence.      
### 6.Adversarial Linear-Quadratic Mean-Field Games over Multigraphs  [ :arrow_down: ](https://arxiv.org/pdf/2109.14461.pdf)
>  In this paper, we propose a game between an exogenous adversary and a network of agents connected via a multigraph. The multigraph is composed of (1) a global graph structure, capturing the virtual interactions among the agents, and (2) a local graph structure, capturing physical/local interactions among the agents. The aim of each agent is to achieve consensus with the other agents in a decentralized manner by minimizing a local cost associated with its local graph and a global cost associated with the global graph. The exogenous adversary, on the other hand, aims to maximize the average cost incurred by all agents in the multigraph. We derive Nash equilibrium policies for the agents and the adversary in the Mean-Field Game setting, when the agent population in the global graph is arbitrarily large and the ``homogeneous mixing" hypothesis holds on local graphs. This equilibrium is shown to be unique and the equilibrium Markov policies for each agent depend on the local state of the agent, as well as the influences on the agent by the local and global mean fields.      
### 7.A Universal Deep Room Acoustics Estimator  [ :arrow_down: ](https://arxiv.org/pdf/2109.14436.pdf)
>  Speech audio quality is subject to degradation caused by an acoustic environment and isotropic ambient and point noises. The environment can lead to decreased speech intelligibility and loss of focus and attention by the listener. Basic acoustic parameters that characterize the environment well are (i) signal-to-noise ratio (SNR), (ii) speech transmission index, (iii) reverberation time, (iv) clarity, and (v) direct-to-reverberant ratio. Except for the SNR, these parameters are usually derived from the Room Impulse Response (RIR) measurements; however, such measurements are often not available. This work presents a universal room acoustic estimator design based on convolutional recurrent neural networks that estimate the acoustic environment measurement blindly and jointly. Our results indicate that the proposed system is robust to non-stationary signal variations and outperforms current state-of-the-art methods.      
### 8.Multi-loss ensemble deep learning for chest X-ray classification  [ :arrow_down: ](https://arxiv.org/pdf/2109.14433.pdf)
>  Class imbalance is common in medical image classification tasks, where the number of abnormal samples is fewer than the number of normal samples. The difficulty of imbalanced classification is compounded by other issues such as the size and distribution of the dataset. Reliable training of deep neural networks continues to be a major challenge in such class-imbalanced conditions. The loss function used to train the deep neural networks highly impact the performance of both balanced and imbalanced tasks. Currently, the cross-entropy loss remains the de-facto loss function for balanced and imbalanced classification tasks. This loss, however, asserts equal learning to all classes, leading to the classification of most samples as the majority normal class. To provide a critical analysis of different loss functions and identify those suitable for class-imbalanced classification, we benchmark various state-of-the-art loss functions and propose novel loss functions to train a DL model and analyze its performance in a multiclass classification setting that classifies pediatric chest X-rays as showing normal lungs, bacterial pneumonia, or viral pneumonia manifestations. We also construct prediction-level and model-level ensembles of the models that are trained with various loss functions to improve classification performance. We performed localization studies to interpret model behavior to ensure that the individual models and their ensembles precisely learned the regions of interest showing disease manifestations to classify the chest X-rays to their respective categories.      
### 9.Multiple Intelligent Reflecting Surface aided Multi-user Weighted Sum-Rate Maximization using Manifold Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2109.14414.pdf)
>  Intelligent reflecting surface (IRS) are able to amend radio propagation condition tasks on account of its functional properties in phase shift optimizing. In fact, there exists geometry manifold in the base-station (BS) beamforming matrix and IRS reflection vector. Therefore, we propose a novel double manifold alternating optimization (DMAO) algorithm which makes use of differential geometry theory to improve optimization performance. First, we develop a multi-IRS multi-user system model to maximize the weighted sum-rate, which may lead to the non-convexity in our optimization procedure. Then in order to allocate an optimized coefficients to each BS antenna and IRS reflecting element, we present the beamforming matrix and reflection vector using complex sphere manifold and complex oblique manifold, respectively, which integrates the inner geometry structure and the constrains. By an innovative alternative iteration method, the system gradually converges an optimized stable state, which is associating with the maximized sum-rate. Furthermore, we quantize the IRS reflection coefficient considering the practical constrains. Experimental results demonstrated that our approach significantly outperforms the conventional methods in terms of weighted sum-rate.      
### 10.Computing the average inter-sample time of event-triggered control using quantitative automata  [ :arrow_down: ](https://arxiv.org/pdf/2109.14391.pdf)
>  Event-triggered control (ETC) is a major recent development in cyber-physical systems due to its capability of reducing resource utilization in networked devices. However, while most of the ETC literature reports simulations indicating massive reductions in the sampling required for control, no method so far has been capable of quantifying these results. In this work, we propose an approach through finite-state abstractions to do formal quantification of the traffic generated by ETC of linear systems, in particular aiming at computing its smallest average inter-sample time (SAIST). The method involves abstracting the traffic model through $l$-complete abstractions, finding the cycle of minimum average length in the graph associated to it, and verifying whether this cycle is an infinitely recurring traffic pattern. The method is proven to be robust to sufficiently small model uncertainties, which allows its application to compute the SAIST of ETC of nonlinear systems.      
### 11.Objective-oriented method for uniformation of various directivity representations  [ :arrow_down: ](https://arxiv.org/pdf/2109.14370.pdf)
>  Over recent years, numerous attempts were taken to provide efficient methods of directivity representation, either regarding sound sources or head-related transfer functions. Because of the wide variety of programming tools and scripts used by different researchers, the resulting representations are inconvevnient to reproduce and compare with each other, hampering the development of the subject. Within this paper, an objective-oriented method is proposed to deal with this issue. The suggested approach bases on defining classes for different directivity models that share some general properties of directivity functions, allowing for easy comparison between different representations. A basic Matlab toolbox utlizing this method is presented alongside exemplary implementations of directivity models based on spherical and hyperspherical harmonics.      
### 12.Comparison of Self-Supervised Speech Pre-Training Methods on Flemish Dutch  [ :arrow_down: ](https://arxiv.org/pdf/2109.14357.pdf)
>  Recent research in speech processing exhibits a growing interest in unsupervised and self-supervised representation learning from unlabelled data to alleviate the need for large amounts of annotated data. We investigate several popular pre-training methods and apply them to Flemish Dutch. We compare off-the-shelf English pre-trained models to models trained on an increasing amount of Flemish data. We find that the most important factors for positive transfer to downstream speech recognition tasks include a substantial amount of data and a matching pre-training domain. Ideally, we also finetune on an annotated subset in the target language. All pre-trained models improve linear phone separability in Flemish, but not all methods improve Automatic Speech Recognition. We experience superior performance with wav2vec 2.0 and we obtain a 30% WER improvement by finetuning the multilingually pre-trained XLSR-53 model on Flemish Dutch, after integration into an HMM-DNN acoustic model.      
### 13.Enhancing Channel Shortening Based Physical Layer Security Using Coordinated Multipoint  [ :arrow_down: ](https://arxiv.org/pdf/2109.14346.pdf)
>  Wireless networks have become imperative in all areas of human life. As such, one of the most critical concerns in next-generation networks is ensuring the security and privacy of user data/communication. Cryptography has been conventionally used to tackle this, but it may not be scalable (in terms of key exchange and management) with the increasingly heterogeneous network deployments. Physical layer security (PLS) provides a promising alternative, but struggles when an attacker boasts a better wireless channel as compared to the legitimate user. This work leverages the coordinated multipoint concept and its distributed transmission points, in conjunction with channel shortening, to address this problem. Results show significant degradation of the bit-error-rate experienced at the eavesdropper as compared to state-of-the-art channel shortening-based PLS methods.      
### 14.From Beginner to Master: A Survey for Deep Learning-based Single-Image Super-Resolution  [ :arrow_down: ](https://arxiv.org/pdf/2109.14335.pdf)
>  Single-image super-resolution (SISR) is an important task in image processing, which aims to enhance the resolution of imaging systems. Recently, SISR has made a huge leap and has achieved promising results with the help of deep learning (DL). In this survey, we give an overview of DL-based SISR methods and group them according to their targets, such as reconstruction efficiency, reconstruction accuracy, and perceptual accuracy. Specifically, we first introduce the problem definition, research background, and the significance of SISR. Secondly, we introduce some related works, including benchmark datasets, upsampling methods, optimization objectives, and image quality assessment methods. Thirdly, we provide a detailed investigation of SISR and give some domain-specific applications of it. Fourthly, we present the reconstruction results of some classic SISR methods to intuitively know their performance. Finally, we discuss some issues that still exist in SISR and summarize some new trends and future directions. This is an exhaustive survey of SISR, which can help researchers better understand SISR and inspire more exciting research in this field. An investigation project for SISR is provided in <a class="link-external link-https" href="https://github.com/CV-JunchengLi/SISR-Survey" rel="external noopener nofollow">this https URL</a>.      
### 15.Joint Estimation of Multiple RF Impairments Using Deep Multi-Task Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.14321.pdf)
>  Radio-frequency (RF) front-end forms a critical part of any radio system, defining its cost as well as communication performance. However, these components frequently exhibit non-ideal behavior, referred to as impairments, due to the imperfections in the manufacturing/design process. Most of the designers rely on simplified closed-form models to estimate these impairments. On the other hand, these models do not holistically or accurately capture the effects of real-world RF front-end components. Recently, machine learning-based algorithms have been proposed to estimate these impairments. However, these algorithms are not capable of estimating multiple RF impairments jointly, which leads to limited estimation accuracy. In this paper, the joint estimation of multiple RF impairments by exploiting the relationship between them is proposed. To do this, a deep multi-task learning-based algorithm is designed. Extensive simulation results reveal that the performance of the proposed joint RF impairments estimation algorithm is superior to the conventional individual estimations in terms of mean-square error. Moreover, the proposed algorithm removes the need of training multiple models for estimating the different impairments.      
### 16.A noise reduction method for force measurements in water entry experiments based on the Ensemble Empirical Mode Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2109.14310.pdf)
>  In this paper a denoising strategy based on the EEMD (Ensemble Empirical Mode Decomposition) is used to reduce the background noise in non-stationary signals, which represent the forces measured in scaled model testing of the emergency water landing of aircraft, generally referred to as ditching. Ditching tests are performed at a constant horizontal speed of 12 m/s with a controlled vertical motion, resulting in a vertical velocity at the beginning of the impact of 0.45 m/s. The measured data are affected by a large amplitude broadband noise, which has both mechanical and electronic origin. Noise sources cannot be easily avoided or removed, since they are associated with the vibrations of the structure of the towing carriage and to the interaction of the measuring chain with the electromagnetic fields. The EEMD noise reduction method is based on the decomposition of the signal into modes and on its partial reconstruction using the residue, the signal-dominant modes and some further modes treated with a thresholding technique, which helps to retain some of the sharp features of the signal. The strategy is developed and tested first on a synthetic signal with a superimposed and known background noise. The method is then verified on the measurement of the inertial force acting on the fuselage when it is moving in air, as in this case the denoised force should equal the product of the mass by the acceleration, both of them being known. Finally, the procedure is applied to denoise the forces measured during the actual ditching experiment. The results look superior to the application of classical filtering methods, such as a moving average filter and a low-pass FIR filter, particularly due to the enhanced capabilities of the EEMD-denoising strategy here developed to preserve the sharp features of the signals and to reduce the residual low-frequency oscillations of spurious origin.      
### 17.BEdgeHealth: A Decentralized Architecture for Edge-based IoMT Networks Using Blockchain  [ :arrow_down: ](https://arxiv.org/pdf/2109.14295.pdf)
>  The healthcare industry has witnessed significant transformations in e-health services by using mobile edge computing (MEC) and blockchain to facilitate healthcare operations. Many MEC-blockchain-based schemes have been proposed, but some critical technical challenges still remain, such as low quality of services (QoS), data privacy and system security vulnerabilities. In this paper, we propose a new decentralized health architecture, called BEdgeHealth that integrates MEC and blockchain for data offloading and data sharing in distributed hospital networks. First, a data offloading scheme is proposed where mobile devices can offload health data to a nearby MEC server for efficient computation with privacy awareness. Moreover, we design a data sharing scheme which enables data exchanges among healthcare users by leveraging blockchain and interplanetary file system. Particularly, a smart contract-based authentication mechanism is integrated with MEC to perform decentralized user access verification at the network edge without requiring any central authority. The real-world experiment results and evaluations demonstrate the effectiveness of the proposed BEdgeHealth architecture in terms of improved QoS with data privacy and security guarantees, compared to the existing schemes.      
### 18.Cooperative Task Offloading and Block Mining in Blockchain-based Edge Computing with Multi-agent Deep Reinforcement Learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.14263.pdf)
>  The convergence of mobile edge computing (MEC) and blockchain is transforming the current computing services in mobile networks, by offering task offloading solutions with security enhancement empowered by blockchain mining. Nevertheless, these important enabling technologies have been studied separately in most existing works. This article proposes a novel cooperative task offloading and block mining (TOBM) scheme for a blockchain-based MEC system where each edge device not only handles data tasks but also deals with block mining for improving the system utility. To address the latency issues caused by the blockchain operation in MEC, we develop a new Proof-of-Reputation consensus mechanism based on a lightweight block verification strategy. A multi-objective function is then formulated to maximize the system utility of the blockchain-based MEC system, by jointly optimizing offloading decision, channel selection, transmit power allocation, and computational resource allocation. We propose a novel distributed deep reinforcement learning-based approach by using a multi-agent deep deterministic policy gradient algorithm. We then develop a game-theoretic solution to model the offloading and mining competition among edge devices as a potential game, and prove the existence of a pure Nash equilibrium. Simulation results demonstrate the significant system utility improvements of our proposed scheme over baseline approaches.      
### 19.Multipath CNN with alpha matte inference for knee tissue segmentation from MRI  [ :arrow_down: ](https://arxiv.org/pdf/2109.14249.pdf)
>  Precise segmentation of knee tissues from magnetic resonance imaging (MRI) is critical in quantitative imaging and diagnosis. Convolutional neural networks (CNNs), which are state of the art, have limitations owing to the lack of image-specific adaptation, such as low tissue contrasts and structural inhomogeneities, thereby leading to incomplete segmentation results. This paper presents a deep learning based automatic segmentation framework for knee tissue segmentation. A novel multipath CNN-based method is proposed, which consists of an encoder decoder-based segmentation network in combination with a low rank tensor-reconstructed segmentation network. Low rank reconstruction in MRI tensor sub-blocks is introduced to exploit the structural and morphological variations in knee tissues. To further improve the segmentation from CNNs, trimap generation, which effectively utilizes superimposed regions, is proposed for defining high, medium and low confidence regions from the multipath CNNs. The secondary path with low rank reconstructed input mitigates the conditions in which the primary segmentation network can potentially fail and overlook the boundary regions. The outcome of the segmentation is solved as an alpha matting problem by blending the trimap with the source input. Experiments on Osteoarthritis Initiative (OAI) datasets and a self prepared scan validate the effectiveness of the proposed method. We specifically demonstrate the application of the proposed method in a cartilage segmentation based thickness map for diagnosis purposes.      
### 20.Can phones, syllables, and words emerge as side-products of cross-situational audiovisual learning? -- A computational investigation  [ :arrow_down: ](https://arxiv.org/pdf/2109.14200.pdf)
>  Decades of research has studied how language learning infants learn to discriminate speech sounds, segment words, and associate words with their meanings. While gradual development of such capabilities is unquestionable, the exact nature of these skills and the underlying mental representations yet remains unclear. In parallel, computational studies have shown that basic comprehension of speech can be achieved by statistical learning between speech and concurrent referentially ambiguous visual input. These models can operate without prior linguistic knowledge such as representations of linguistic units, and without learning mechanisms specifically targeted at such units. This has raised the question of to what extent knowledge of linguistic units, such as phone(me)s, syllables, and words, could actually emerge as latent representations supporting the translation between speech and representations in other modalities, and without the units being proximal learning targets for the learner. In this study, we formulate this idea as the so-called latent language hypothesis (LLH), connecting linguistic representation learning to general predictive processing within and across sensory modalities. We review the extent that the audiovisual aspect of LLH is supported by the existing computational studies. We then explore LLH further in extensive learning simulations with different neural network models for audiovisual cross-situational learning, and comparing learning from both synthetic and real speech data. We investigate whether the latent representations learned by the networks reflect phonetic, syllabic, or lexical structure of input speech by utilizing an array of complementary evaluation metrics related to linguistic selectivity and temporal characteristics of the representations. As a result, we find that representations associated...      
### 21.Semi-Supervised Segmentation of Radiation-Induced Pulmonary Fibrosis from Lung CT Scans with Multi-Scale Guided Dense Attention  [ :arrow_down: ](https://arxiv.org/pdf/2109.14172.pdf)
>  Computed Tomography (CT) plays an important role in monitoring radiation-induced Pulmonary Fibrosis (PF), where accurate segmentation of the PF lesions is highly desired for diagnosis and treatment follow-up. However, the task is challenged by ambiguous boundary, irregular shape, various position and size of the lesions, as well as the difficulty in acquiring a large set of annotated volumetric images for training. To overcome these problems, we propose a novel convolutional neural network called PF-Net and incorporate it into a semi-supervised learning framework based on Iterative Confidence-based Refinement And Weighting of pseudo Labels (I-CRAWL). Our PF-Net combines 2D and 3D convolutions to deal with CT volumes with large inter-slice spacing, and uses multi-scale guided dense attention to segment complex PF lesions. For semi-supervised learning, our I-CRAWL employs pixel-level uncertainty-based confidence-aware refinement to improve the accuracy of pseudo labels of unannotated images, and uses image-level uncertainty for confidence-based image weighting to suppress low-quality pseudo labels in an iterative training process. Extensive experiments with CT scans of Rhesus Macaques with radiation-induced PF showed that: 1) PF-Net achieved higher segmentation accuracy than existing 2D, 3D and 2.5D neural networks, and 2) I-CRAWL outperformed state-of-the-art semi-supervised learning methods for the PF lesion segmentation task. Our method has a potential to improve the diagnosis of PF and clinical assessment of side effects of radiotherapy for lung cancers.      
### 22.Sample Complexity of the Robust LQG Regulator with Coprime Factors Uncertainty  [ :arrow_down: ](https://arxiv.org/pdf/2109.14164.pdf)
>  This paper addresses the end-to-end sample complexity bound for learning the H2 optimal controller (the Linear Quadratic Gaussian (LQG) problem) with unknown dynamics, for potentially unstable Linear Time-Invariant (LTI) systems. The robust LQG synthesis procedure is performed by considering bounded additive model uncertainty on the coprime factors of the plant. A nominal model of the true plant is estimated by constructing a Hankel-like matrix from a single time series of noisy finite length input-output data using the ordinary least squares algorithm of Sarkar and Rakhlin (2019). Next, an H-infinity bound on the estimated error is provided and the robust controller is designed via convex optimization starting from the estimated model, building on existing methods from Mania et al. (2019) and Zheng et al. (2020) while allowing for bounded additive uncertainty on the coprime factors of the model. Our conclusions are consistent with previous results on learning the LQG and LQR controllers.      
### 23.Multi-frame Joint Enhancement for Early Interlaced Videos  [ :arrow_down: ](https://arxiv.org/pdf/2109.14151.pdf)
>  Early interlaced videos usually contain multiple and interlacing and complex compression artifacts, which significantly reduce the visual quality. Although the high-definition reconstruction technology for early videos has made great progress in recent years, related research on deinterlacing is still lacking. Traditional methods mainly focus on simple interlacing mechanism, and cannot deal with the complex artifacts in real-world early videos. Recent interlaced video reconstruction deep deinterlacing models only focus on single frame, while neglecting important temporal information. Therefore, this paper proposes a multiframe deinterlacing network joint enhancement network for early interlaced videos that consists of three modules, i.e., spatial vertical interpolation module, temporal alignment and fusion module, and final refinement module. The proposed method can effectively remove the complex artifacts in early videos by using temporal redundancy of multi-fields. Experimental results demonstrate that the proposed method can recover high quality results for both synthetic dataset and real-world early interlaced videos.      
### 24.Peak Infection Time for a Networked SIR Epidemic with Opinion Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2109.14135.pdf)
>  We propose an SIR epidemic model coupled with opinion dynamics to study an epidemic and opinions spreading in a network of communities. Our model couples networked SIR epidemic dynamics with opinions towards the severity of the epidemic, and vice versa. We develop an epidemic-opinion based threshold condition to capture the moment when a weighted average of the epidemic states starts to decrease exponentially fast over the network, namely the peak infection time. We define an effective reproduction number to characterize the behavior of the model through the peak infection time. We use both analytical and simulation-based results to illustrate that the opinions reflect the recovered levels within the communities after the epidemic dies out.      
### 25.Comparison of atlas-based and neural-network-based semantic segmentation for DENSE MRI images  [ :arrow_down: ](https://arxiv.org/pdf/2109.14116.pdf)
>  Two segmentation methods, one atlas-based and one neural-network-based, were compared to see how well they can each automatically segment the brain stem and cerebellum in Displacement Encoding with Stimulated Echoes Magnetic Resonance Imaging (DENSE-MRI) data. The segmentation is a pre-requisite for estimating the average displacements in these regions, which have recently been proposed as biomarkers in the diagnosis of Chiari Malformation type I (CMI). In numerical experiments, the segmentations of both methods were similar to manual segmentations provided by trained experts. It was found that, overall, the neural-network-based method alone produced more accurate segmentations than the atlas-based method did alone, but that a combination of the two methods -- in which the atlas-based method is used for the segmentation of the brain stem and the neural-network is used for the segmentation of the cerebellum -- may be the most successful.      
### 26.Modeling and Control of Google bittide Synchronization  [ :arrow_down: ](https://arxiv.org/pdf/2109.14111.pdf)
>  Distributed system applications rely on a fine-grain common sense of time. Existing systems maintain the common sense of time by keeping each independent machine as close as possible to wall-clock time through a combination of software protocols like NTP and GPS signals and/or precision references like atomic clocks. This approach is expensive and has tolerance limitations that require protocols to deal with asynchrony and its performance consequences. Moreover, at data-center scale it is impractical to distribute a physical clock as is done on a chip or printed circuit board. In this paper we introduce a distributed system design that removes the need for physical clock distribution or mechanisms for maintaining close alignment to wall-clock time, and instead provides applications with a perfectly synchronized logical clock. We discuss the abstract frame model (AFM), a mathematical model that underpins the system synchronization. The model is based on the rate of communication between nodes in a topology without requiring a global clock. We show that there are families of controllers that satisfy the properties required for existence and uniqueness of solutions to the AFM, and give examples.      
### 27.Human Reliability Analysis for Oil and Gas Operations: Analysis of Existing Methods  [ :arrow_down: ](https://arxiv.org/pdf/2109.14096.pdf)
>  In the petroleum industry, Quantitative Risk Analysis (QRA) has been one of the main tools for risk management. To date, QRA has mostly focused on technical barriers, despite many accidents having human failure as a primary cause or a contributing factor. Human Reliability Analysis (HRA) allows for the assessment of the human contribution to risk to be assessed both qualitatively and quantitatively. Most credible and highly advanced HRA methods have largely been developed and applied in support of nuclear power plants control room operations and in context of probabilistic risk analysis. Moreover, many of the HRA methods have issues that have led to inconsistencies, insufficient traceability and reproducibility in both the qualitative and quantitative phases. Given the need to assess human error in the context of the oil industry, it is necessary to evaluate available HRA methodologies and assess its applicability to petroleum operations. Furthermore, it is fundamental to assess these methods against good practices of HRA and the requirements for advanced HRA methods. The present paper accomplishes this by analyzing seven HRA methods. The evaluation of the methods was performed in three stages. The first stage consisted of an evaluation of the degree of adaptability of the method for the Oil and Gas industry. In the second stage the methods were evaluated against desirable items in an HRA method. The higher-ranked methods were evaluated, in the third stage, against requirements for advanced HRA methods. In addition to the methods' evaluation, this paper presents an overview of state-of-the-art discussions on HRA, led by the Nuclear industry community. It remarks that these discussions must be seriously considered in defining a technical roadmap to a credible HRA method for the Oil and Gas industry.      
### 28.Privacy-Preserving Stealthy Attack Detection in Multi-Agent Control Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.14094.pdf)
>  This paper develops a glocal (global-local) attack detection framework to detect stealthy cyber-physical attacks, namely covert attack and zero-dynamics attack, against a class of multi-agent control systems seeking average consensus. The detection structure consists of a global (central) observer and local observers for the multi-agent system partitioned into clusters. The proposed structure addresses the scalability of the approach and the privacy preservation of the multi-agent system's state information. The former is addressed by using decentralized local observers, and the latter is achieved by imposing unobservability conditions at the global level. Also, the communication graph model is subject to topology switching, triggered by local observers, allowing for the detection of stealthy attacks by the global observer. Theoretical conditions are derived for detectability of the stealthy attacks using the proposed detection framework. Finally, a numerical simulation is provided to validate the theoretical findings.      
### 29.Orbital Angular Momentum (OAM) Carrying Vortex Wave generation in Dielectric Filled Circular Waveguide  [ :arrow_down: ](https://arxiv.org/pdf/2109.14064.pdf)
>  In this paper, we propose a method to generate Orbital Angular Momentum (OAM) carrying vortex waves inside a metallic circular waveguide (CW). These waves feature ability to carry multiple orthogonal modes at the same frequency, by the virtue of their unique spatial structure. In essence high data rate channels can be developed using such waves. In free space, OAM carrying vortex waves has beam divergence issues and a central NULL, which makes the waves unfavourable for free space communication. But, OAM modes in guided structures do not suffer from these drawbacks. This prospect of enhancement of communication spectrum provides the background for the study of vortex wave in the circular waveguides. In this work, a radial array of monopoles is designed to generate the vortex wave inside the waveguide. Further, we introduced the dielectric materials inside the waveguide in order to manipulate the operating frequency of the OAM modes. Simulation results shows that the various dielectric materials allow us to tune the working frequency of the OAM beam to a desired frequency.      
### 30.The impact of non-target events in synthetic soundscapes for sound event detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.14061.pdf)
>  Detection and Classification Acoustic Scene and Events Challenge 2021 Task 4 uses a heterogeneous dataset that includes both recorded and synthetic soundscapes. Until recently only target sound events were considered when synthesizing the soundscapes. However, recorded soundscapes often contain a substantial amount of non-target events that may affect the performance. In this paper, we focus on the impact of these non-target events in the synthetic soundscapes. Firstly, we investigate to what extent using non-target events alternatively during the training or validation phase (or none of them) helps the system to correctly detect target events. Secondly, we analyze to what extend adjusting the signal-to-noise ratio between target and non-target events at training improves the sound event detection performance. The results show that using both target and non-target events for only one of the phases (validation or training) helps the system to properly detect sound events, outperforming the baseline (which uses non-target events in both phases). The paper also reports the results of a preliminary study on evaluating the system on clips that contain only non-target events. This opens questions for future work on non-target subset and acoustic similarity between target and non-target events which might confuse the system.      
### 31.Design of Maximum-Gain Dielectric Lens Antenna via Phase Center Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2109.14057.pdf)
>  In this work, a method is presented to maximize the obtained gain from millimeter-wave (mm-wave) lens antennas using phase center analysis. Commonly, for designing a lens antenna, the lens is positioned just on top of the antenna element which is not capable of providing the maximum gain/aperture efficiency. A novel solution method is proposed where the lens will be placed at a distance calculated using phase center analysis to produce the maximum gain from the system. A mm-wave microstrip antenna array is designed and the proposed method is applied for the gain enhancement. Simulation results suggest that the propose scheme obtains around 25% gain enhancement compared to the traditional method.      
### 32.On the robustness of model-based algorithms for photoacoustic tomography: comparison between time and frequency domains  [ :arrow_down: ](https://arxiv.org/pdf/2109.14028.pdf)
>  For photoacoustic image reconstruction, certain parameters such as sensor positions and speed of sound have a major impact in the reconstruction process and must be carefully determined before data acquisition. Uncertainties in these parameters can lead to errors produced by a modeling mismatch, hindering the reconstruction process and severely affecting the resulting image quality. Therefore, in this work we study how modeling errors arising from uncertainty in sensor locations affect the images obtained by matrix model-based reconstruction algorithms based on time domain and frequency domain models of the photoacoustic problem. The effects on the reconstruction performance with respect to the uncertainty in the knowledge of the sensors location is compared and analyzed both in a qualitative and quantitative fashion for both time and frequency models. Ultimately, our study shows that the frequency domain approach is more sensitive to this kind of modeling errors. These conclusions are supported by numerical experiments and a theoretical sensitivity analysis of the mathematical operator for the direct problem.      
### 33.Longitudinal Deep Truck: Deep learning and deep reinforcement learning for modeling and control of longitudinal dynamics of heavy duty trucks  [ :arrow_down: ](https://arxiv.org/pdf/2109.14019.pdf)
>  Heavy duty truck mechanical configuration is often tailor designed and built for specific truck mission requirements. This renders the precise derivation of analytical dynamical models and controls for these trucks from first principles challenging, tedious, and often requires several theoretical and applied areas of expertise to carry through. This article investigates deep learning and deep reinforcement learning as truck-configuration-agnostic longitudinal modeling and control approaches for heavy duty trucks. The article outlines a process to develop and validate such models and controllers and highlights relevant practical considerations. The process is applied to simulation and real-full size trucks for validation and experimental performance evaluation. The results presented demonstrate applicability of this approach to trucks of multiple configurations; models generated were accurate for control development purposes both in simulation and the field.      
### 34.The Impact of Vaccine Hesitancy on Epidemic Spreading  [ :arrow_down: ](https://arxiv.org/pdf/2109.14000.pdf)
>  The COVID-19 pandemic has devastated the world in an unprecedented way, causing enormous loss of life. Time and again, public health authorities have urged people to become vaccinated to protect themselves and mitigate the spread of the disease. However, vaccine hesitancy has stalled vaccination levels in the United States. This study explores the effect of vaccine hesitancy on the spread of disease by introducing an SIRS-V$_\kappa$ model, with compartments of susceptible (S), infected (I), recovered (R), and vaccinated (V). We leverage the concept of carrying capacity to account for vaccine hesitancy by defining a vaccine confidence level $\kappa$, which is the maximum number of people that will become vaccinated during the course of a disease. The inverse of vaccine confidence is vaccine hesitance, $(\frac{1}{\kappa})$. We explore the equilibria of the SIRS-V$_\kappa$ model and their stability, and illustrate the impact of vaccine hesitance on epidemic spread analytically and via simulations.      
### 35.Guided Probabilistic Simulation of Complex Systems Toward Rare and Extreme Events  [ :arrow_down: ](https://arxiv.org/pdf/2109.13966.pdf)
>  Simulation based or dynamic probabilistic risk assessment methodologies were primarily developed for proving a more realistic and complete representation of complex systems accident response. Such simulation based methodologies have proven to be particularly powerful for systems with control loops and complex interactions between its elements, be they hardware, software, or human, as they provide a natural probabilistic environment to include physical models of system behavior (e.g., coupled neutronics and thermal hydraulic codes for nuclear power plants), mechanistic models of materials or hardware systems to predict failure, and those of natural hazards. Despite the advancements in simulation based methodologies, the fundamental challenge still persists as the space of possible probabilistic system trajectories is nearly infinite in size in simulating even systems of relatively low complexity. In this paper, a framework is developed to identify rare and extreme events and enabling the use of reverse trajectories to trace failures (or other system states) to causes for potential mitigation actions. This framework consists of an Intelligent Guidance module, Trajectory Generation module and Physical Simulation module. The Intelligent Guidance module provides planning information to the Trajectory Generation module that creates scenarios by interacting with the Physical Simulation in its environment. In turn, system trajectories or scenarios are created and post processed to provide updating information to the Intelligent Guidance module or aggregate the results when stopping criteria are met. The objective of guided simulation is to control the growth of the scenario tree and to efficiently identify important scenarios that meet single or multiple criteria. We present several solution strategies, both qualitative and data driven for each module.      
### 36.An Accelerated Stochastic Gradient for Canonical Polyadic Decomposition  [ :arrow_down: ](https://arxiv.org/pdf/2109.13964.pdf)
>  We consider the problem of structured canonical polyadic decomposition. If the size of the problem is very big, then stochastic gradient approaches are viable alternatives to classical methods, such as Alternating Optimization and All-At-Once optimization. We extend a recent stochastic gradient approach by employing an acceleration step (Nesterov momentum) in each iteration. We compare our approach with state-of-the-art alternatives, using both synthetic and real-world data, and find it to be very competitive.      
### 37.All-Around Real Label Supervision: Cyclic Prototype Consistency Learning for Semi-supervised Medical Image Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.13930.pdf)
>  Semi-supervised learning has substantially advanced medical image segmentation since it alleviates the heavy burden of acquiring the costly expert-examined annotations. Especially, the consistency-based approaches have attracted more attention for their superior performance, wherein the real labels are only utilized to supervise their paired images via supervised loss while the unlabeled images are exploited by enforcing the perturbation-based \textit{"unsupervised"} consistency without explicit guidance from those real labels. However, intuitively, the expert-examined real labels contain more reliable supervision signals. Observing this, we ask an unexplored but interesting question: can we exploit the unlabeled data via explicit real label supervision for semi-supervised training? To this end, we discard the previous perturbation-based consistency but absorb the essence of non-parametric prototype learning. Based on the prototypical network, we then propose a novel cyclic prototype consistency learning (CPCL) framework, which is constructed by a labeled-to-unlabeled (L2U) prototypical forward process and an unlabeled-to-labeled (U2L) backward process. Such two processes synergistically enhance the segmentation network by encouraging more discriminative and compact features. In this way, our framework turns previous \textit{"unsupervised"} consistency into new \textit{"supervised"} consistency, obtaining the \textit{"all-around real label supervision"} property of our method. Extensive experiments on brain tumor segmentation from MRI and kidney segmentation from CT images show that our CPCL can effectively exploit the unlabeled data and outperform other state-of-the-art semi-supervised medical image segmentation methods.      
### 38.Towards 6G Non-Terrestrial Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.14581.pdf)
>  Sixth-Generation (6G) technologies will revolutionize the wireless ecosystem by enabling the delivery of futuristic services through terrestrial and non-terrestrial transmissions. In this context, the Non-Terrestrial Network (NTN) is growing in importance owing to its capability to deliver services anywhere and anytime and also provide coverage in areas that are unreachable by any conventional Terrestrial Network (TN). The exploitation of the same radio technology could greatly facilitate the integration of NTNs and TNs into a unified wireless system. Since New Radio (NR) is the de facto standard to deliver manifold heterogeneous services in terrestrial wireless systems, 3GPP is investigating new solutions to extend NR to NTNs. In this paper, the constraints that NTN features place on NR procedures are investigated by going thoroughly into 3GPP specifications; strengths and weaknesses of the NR technology in enabling typical 6G services on NR-enabled NTNs are identified; finally, open issues and insights are provided as guidelines to steer future research towards 6G NTNs.      
### 39.On Assessing the Usefulness of Proxy Domains for Developing and Evaluating Embodied Agents  [ :arrow_down: ](https://arxiv.org/pdf/2109.14516.pdf)
>  In many situations it is either impossible or impractical to develop and evaluate agents entirely on the target domain on which they will be deployed. This is particularly true in robotics, where doing experiments on hardware is much more arduous than in simulation. This has become arguably more so in the case of learning-based agents. To this end, considerable recent effort has been devoted to developing increasingly realistic and higher fidelity simulators. However, we lack any principled way to evaluate how good a ``proxy domain'' is, specifically in terms of how useful it is in helping us achieve our end objective of building an agent that performs well in the target domain. In this work, we investigate methods to address this need. We begin by clearly separating two uses of proxy domains that are often conflated: 1) their ability to be a faithful predictor of agent performance and 2) their ability to be a useful tool for learning. In this paper, we attempt to clarify the role of proxy domains and establish new proxy usefulness (PU) metrics to compare the usefulness of different proxy domains. We propose the relative predictive PU to assess the predictive ability of a proxy domain and the learning PU to quantify the usefulness of a proxy as a tool to generate learning data. Furthermore, we argue that the value of a proxy is conditioned on the task that it is being used to help solve. We demonstrate how these new metrics can be used to optimize parameters of the proxy domain for which obtaining ground truth via system identification is not trivial.      
### 40.Cross-domain Semi-Supervised Audio Event Classification Using Contrastive Regularization  [ :arrow_down: ](https://arxiv.org/pdf/2109.14508.pdf)
>  In this study, we proposed a novel semi-supervised training method that uses unlabeled data with a class distribution that is completely different from the target data or data without a target label. To this end, we introduce a contrastive regularization that is designed to be target task-oriented and trained simultaneously. In addition, we propose an audio mixing based simple augmentation strategy that performed in batch samples. Experimental results validate that the proposed method successfully contributed to the performance improvement, and particularly showed that it has advantages in stable training and generalization.      
### 41.Variational Inference for Continuous-Time Switching Dynamical Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.14492.pdf)
>  Switching dynamical systems provide a powerful, interpretable modeling framework for inference in time-series data in, e.g., the natural sciences or engineering applications. Since many areas, such as biology or discrete-event systems, are naturally described in continuous time, we present a model based on an Markov jump process modulating a subordinated diffusion process. We provide the exact evolution equations for the prior and posterior marginal densities, the direct solutions of which are however computationally intractable. Therefore, we develop a new continuous-time variational inference algorithm, combining a Gaussian process approximation on the diffusion level with posterior inference for Markov jump processes. By minimizing the path-wise Kullback-Leibler divergence we obtain (i) Bayesian latent state estimates for arbitrary points on the real axis and (ii) point estimates of unknown system parameters, utilizing variational expectation maximization. We extensively evaluate our algorithm under the model assumption and for real-world examples.      
### 42.Distributed Feedback Optimisation for Robotic Coordination  [ :arrow_down: ](https://arxiv.org/pdf/2109.14486.pdf)
>  Feedback optimisation is an emerging technique aiming at steering a system to an optimal steady state for a given objective function. We show that it is possible to employ this control strategy in a distributed manner. Moreover, we prove asymptotic convergence to the set of optimal configurations. To this scope, we show that exponential stability is needed only for the portion of the state that affects the objective function. This is showcased by driving a swarm of agents towards a target location while maintaining a target formation. Finally, we provide a sufficient condition on the topological structure of the specified formation to guarantee convergence of the swarm in formation around the target location.      
### 43.Programmable Spectral Filter Arrays for Hyperspectral Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2109.14450.pdf)
>  Modulating the spectral dimension of light has numerous applications in computational imaging. While there are many techniques for achieving this, there are few, if any, for implementing a spatially-varying and programmable spectral filter. This paper provides an optical design for implementing such a capability. Our key insight is that spatially-varying spectral modulation can be implemented using a liquid crystal spatial light modulator since it provides an array of liquid crystal cells, each of which can be purposed to act as a programmable spectral filter array. Relying on this insight, we provide an optical schematic and an associated lab prototype for realizing the capability, as well as address the associated challenges at implementation using optical and computational innovations. We show a number of unique operating points with our prototype including single- and multi-image hyperspectral imaging, as well as its application in material identification.      
### 44.Minimal Expected Regret in Linear Quadratic Control  [ :arrow_down: ](https://arxiv.org/pdf/2109.14429.pdf)
>  We consider the problem of online learning in Linear Quadratic Control systems whose state transition and state-action transition matrices $A$ and $B$ may be initially unknown. We devise an online learning algorithm and provide guarantees on its expected regret. This regret at time $T$ is upper bounded (i) by $\widetilde{O}((d_u+d_x)\sqrt{d_xT})$ when $A$ and $B$ are unknown, (ii) by $\widetilde{O}(d_x^2\log(T))$ if only $A$ is unknown, and (iii) by $\widetilde{O}(d_x(d_u+d_x)\log(T))$ if only $B$ is unknown and under some mild non-degeneracy condition ($d_x$ and $d_u$ denote the dimensions of the state and of the control input, respectively). These regret scalings are minimal in $T$, $d_x$ and $d_u$ as they match existing lower bounds in scenario (i) when $d_x\le d_u$ [SF20], and in scenario (ii) [lai1986]. We conjecture that our upper bounds are also optimal in scenario (iii) (there is no known lower bound in this setting). <br>Existing online algorithms proceed in epochs of (typically exponentially) growing durations. The control policy is fixed within each epoch, which considerably simplifies the analysis of the estimation error on $A$ and $B$ and hence of the regret. Our algorithm departs from this design choice: it is a simple variant of certainty-equivalence regulators, where the estimates of $A$ and $B$ and the resulting control policy can be updated as frequently as we wish, possibly at every step. Quantifying the impact of such a constantly-varying control policy on the performance of these estimates and on the regret constitutes one of the technical challenges tackled in this paper.      
### 45.FastCorrect 2: Fast Error Correction on Multiple Candidates for Automatic Speech Recognition  [ :arrow_down: ](https://arxiv.org/pdf/2109.14420.pdf)
>  Error correction is widely used in automatic speech recognition (ASR) to post-process the generated sentence, and can further reduce the word error rate (WER). Although multiple candidates are generated by an ASR system through beam search, current error correction approaches can only correct one sentence at a time, failing to leverage the voting effect from multiple candidates to better detect and correct error tokens. In this work, we propose FastCorrect 2, an error correction model that takes multiple ASR candidates as input for better correction accuracy. FastCorrect 2 adopts non-autoregressive generation for fast inference, which consists of an encoder that processes multiple source sentences and a decoder that generates the target sentence in parallel from the adjusted source sentence, where the adjustment is based on the predicted duration of each source token. However, there are some issues when handling multiple source sentences. First, it is non-trivial to leverage the voting effect from multiple source sentences since they usually vary in length. Thus, we propose a novel alignment algorithm to maximize the degree of token alignment among multiple sentences in terms of token and pronunciation similarity. Second, the decoder can only take one adjusted source sentence as input, while there are multiple source sentences. Thus, we develop a candidate predictor to detect the most suitable candidate for the decoder. Experiments on our inhouse dataset and AISHELL-1 show that FastCorrect 2 can further reduce the WER over the previous correction model with single candidate by 3.2% and 2.6%, demonstrating the effectiveness of leveraging multiple candidates in ASR error correction. FastCorrect 2 achieves better performance than the cascaded re-scoring and correction pipeline and can serve as a unified post-processing module for ASR.      
### 46.From CCS-Planning to Testautomation: The Digital Testfield of Deutsche Bahn in Scheibenberg -- A Case Study  [ :arrow_down: ](https://arxiv.org/pdf/2109.14378.pdf)
>  The digitalization of railway systems should increase the efficiency of the train operation to achieve future mobility challenges and climate goals. But this digitalization also comes with several new challenges in providing a secure and reliable train operation. The work resulting in this paper tackles two major challenges. First, there is no single university curriculum combining computer science, railway operation, and certification processes. Second, many railway processes are still manual and without the usage of digital tools and result in static implementations and configurations of the railway infrastructure devices. This case study occurred as part of the Digital Rail Summer School 2021, a university course combining the three mentioned aspects as cooperation of several German universities with partners from the railway industry. It passes through all steps from a digital Control-Command and Signalling (CCS) planning in ProSig 7.3, the transfer, and validation of the planning in the PlanPro data format and toolbox, to the generation of code of an interlocking for the digital CCS planning to contribute to the vision of test automation. This paper contributes the experiences of the case study and a proof-of-concept of the whole lifecycle for the Digital Testfield of Deutsche Bahn in Scheibenberg. This proof-of-concept will be continued in ongoing and following projects to fulfill the vision of test automation and automated launching of new devices.      
### 47.Signal power and energy-per-bit optimization problems in systems mMTC  [ :arrow_down: ](https://arxiv.org/pdf/2109.14355.pdf)
>  Currently, the issues of the operation of the Internet of Things technology are being actively studied. The operation of a large number of different self-powered sensors is within the framework of a massive machine-type communications scenario using random access methods. Topical issues in this type of communication are: reducing the transmission signal power and increasing the duration of the device by reducing the consumption energy per bit. Formulation and analysis of the tasks of minimizing transmission power and spent energy per bit in systems without retransmissions and with retransmissions to obtain achievability bounds. A model of the system is described, within which four problems of minimizing signal power and energy consumption for given parameters (the number of information bits, the spectral efficiency of the system, and the Packet Delivery Ratio) are formulated and described. The numerical results of solving these optimization problems are presented, which make it possible to obtain the achievability bounds for the considered characteristics in systems with and without losses. The lower bounds obtained by the Shannon formula are presented, assuming that the message length is not limited. The results obtained showed that solving the minimization problem with respect to one of the parameters (signal power or consumption energy per bit) does not minimize the second parameter. This difference is most significant for small information message lengths, which corresponds to IoT scenarios. The results obtained allow assessing the potential for minimizing transmission signal power and consumption energy per bit in random multiple access systems with massive machine-type communications scenarios. The presented problems were solved without taking into account the average delay of message transmission.      
### 48.Smart-home anomaly detection using combination of in-home situation and user behavior  [ :arrow_down: ](https://arxiv.org/pdf/2109.14348.pdf)
>  Internet-of-things (IoT) devices are vulnerable to malicious operations by attackers, which can cause physical and economic harm to users; therefore, we previously proposed a sequence-based method that modeled user behavior as sequences of in-home events and a base home state to detect anomalous operations. However, that method modeled users' home states based on the time of day; hence, attackers could exploit the system to maximize attack opportunities. Therefore, we then proposed an estimation-based detection method that estimated the home state using not only the time of day but also the observable values of home IoT sensors and devices. However, it ignored short-term operational behaviors. Consequently, in the present work, we propose a behavior-modeling method that combines home state estimation and event sequences of IoT devices within the home to enable a detailed understanding of long- and short-term user behavior. We compared the proposed model to our previous methods using data collected from real homes. Compared with the estimation-based method, the proposed method achieved a 15.4% higher detection ratio with fewer than 10% misdetections. Compared with the sequence-based method, the proposed method achieved a 46.0% higher detection ratio with fewer than 10% misdetections.      
### 49.Deep Reinforcement Q-Learning for Intelligent Traffic Signal Control with Partial Detection  [ :arrow_down: ](https://arxiv.org/pdf/2109.14337.pdf)
>  Intelligent traffic signal controllers, applying DQN algorithms to traffic light policy optimization, efficiently reduce traffic congestion by adjusting traffic signals to real-time traffic. Most propositions in the literature however consider that all vehicles at the intersection are detected, an unrealistic scenario. Recently, new wireless communication technologies have enabled cost-efficient detection of connected vehicles by infrastructures. With only a small fraction of the total fleet currently equipped, methods able to perform under low detection rates are desirable. In this paper, we propose a deep reinforcement Q-learning model to optimize traffic signal control at an isolated intersection, in a partially observable environment with connected vehicles. First, we present the novel DQN model within the RL framework. We introduce a new state representation for partially observable environments and a new reward function for traffic signal control, and provide a network architecture and tuned hyper-parameters. Second, we evaluate the performances of the model in numerical simulations on multiple scenarios, in two steps. At first in full detection against existing actuated controllers, then in partial detection with loss estimates for proportions of connected vehicles. Finally, from the obtained results, we define thresholds for detection rates with acceptable and optimal performance levels.      
### 50.Distributionally Robust Frequency Constrained Scheduling for an Integrated Electricity-Gas System  [ :arrow_down: ](https://arxiv.org/pdf/2109.14317.pdf)
>  Power systems are shifted from conventional bulk generation toward renewable generation. This trend leads to the frequency security problem due to the decline of system inertia. On the other hand, natural gas-fired units are frequently scheduled to provide operational flexibility due to their fast adjustment ability. The interdependence between power and natural gas systems is thus intensified. In this paper, we study the frequency constrained scheduling problem from the perspective of an integrated electricity-gas system under variable wind power. We propose a distributionally robust (DR) chance constrained optimization model to co-optimize the unit commitment and virtual inertia provision from wind farm systems. This model incorporates both frequency constraints and natural gas system (NGS) operational constraints and addresses the wind power uncertainty by designing DR joint chance constraints. We show that this model admits a mixed-integer second-order cone programming. Case studies demonstrate that the proposed approach can provide a highly reliable and computationally efficient solution and show the importance of incorporating NGS operational constraints in the frequency constrained scheduling problem.      
### 51.Self-Supervised Learning for 3D Medical Image Analysis using 3D SimCLR and Monte Carlo Dropout  [ :arrow_down: ](https://arxiv.org/pdf/2109.14288.pdf)
>  Self-supervised learning methods can be used to learn meaningful representations from unlabeled data that can be transferred to supervised downstream tasks to reduce the need for labeled data. In this paper, we propose a 3D self-supervised method that is based on the contrastive (SimCLR) method. Additionally, we show that employing Bayesian neural networks (with Monte-Carlo Dropout) during the inference phase can further enhance the results on the downstream tasks. We showcase our models on two medical imaging segmentation tasks: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT. Our experimental results demonstrate the benefits of our proposed methods in both downstream data-efficiency and performance.      
### 52.Computing Necessary Conditions for Near-Optimality in Capacity Expansion Planning Problems  [ :arrow_down: ](https://arxiv.org/pdf/2109.14272.pdf)
>  In power systems, large-scale optimisation problems are extensively used to plan for capacity expansion at the supra-national level. However, their cost-optimal solutions are often not exploitable by decision-makers who are preferably looking for features of solutions that can accommodate their different requirements. This paper proposes a generic framework for addressing this problem. It is based on the concept of the epsilon-optimal feasible space of a given optimisation problem and the identification of necessary conditions over this space. This framework has been developed in a generic case, and an approach for solving this problem is subsequently described for a specific case where conditions are constrained sums of variables. The approach is tested on a case study about transmission expansion planning of the European electricity network to determine necessary conditions on the minimal investments in transmission, storage and generation capacity.      
### 53.Formulation and validation of a car-following model based on deep reinforcement learning  [ :arrow_down: ](https://arxiv.org/pdf/2109.14268.pdf)
>  We propose and validate a novel car following model based on deep reinforcement learning. Our model is trained to maximize externally given reward functions for the free and car-following regimes rather than reproducing existing follower trajectories. The parameters of these reward functions such as desired speed, time gap, or accelerations resemble that of traditional models such as the Intelligent Driver Model (IDM) and allow for explicitly implementing different driving styles. Moreover, they partially lift the black-box nature of conventional neural network models. The model is trained on leading speed profiles governed by a truncated Ornstein-Uhlenbeck process reflecting a realistic leader's kinematics. <br>This allows for arbitrary driving situations and an infinite supply of training data. For various parameterizations of the reward functions, and for a wide variety of artificial and real leader data, the model turned out to be unconditionally string stable, comfortable, and crash-free. String stability has been tested with a platoon of five followers following an artificial and a real leading trajectory. A cross-comparison with the IDM calibrated to the goodness-of-fit of the relative gaps showed a higher reward compared to the traditional model and a better goodness-of-fit.      
### 54.REFLACX, a dataset of reports and eye-tracking data for localization of abnormalities in chest x-rays  [ :arrow_down: ](https://arxiv.org/pdf/2109.14187.pdf)
>  Deep learning has shown recent success in classifying anomalies in chest x-rays, but datasets are still small compared to natural image datasets. Supervision of abnormality localization has been shown to improve trained models, partially compensating for dataset sizes. However, explicitly labeling these anomalies requires an expert and is very time-consuming. We propose a method for collecting implicit localization data using an eye tracker to capture gaze locations and a microphone to capture a dictation of a report, imitating the setup of a reading room, and potentially scalable for large datasets. The resulting REFLACX (Reports and Eye-Tracking Data for Localization of Abnormalities in Chest X-rays) dataset was labeled by five radiologists and contains 3,032 synchronized sets of eye-tracking data and timestamped report transcriptions. We also provide bounding boxes around lungs and heart and validation labels consisting of ellipses localizing abnormalities and image-level labels. Furthermore, a small subset of the data contains readings from all radiologists, allowing for the calculation of inter-rater scores.      
### 55.Towards a Pantograph-based Interventional AUV for Under-ice Measurement  [ :arrow_down: ](https://arxiv.org/pdf/2109.14182.pdf)
>  This paper addresses the design of a novel interventional robotic platform, aiming to perform an autonomous sampling and measurement under the thin ice in the Antarctic environment. We propose a pantograph mechanism, which can effectively generate a constant interaction force to the surface during the contact, which is crucial for reliable measurements. We provide the proof-of-concept design of the pantograph with a robotic prototype with foldable actuation, and preliminary results of the pantograph mechanism and the localisation system are provided, confirming the feasibility of the system.      
### 56.Second-Order Neural ODE Optimizer  [ :arrow_down: ](https://arxiv.org/pdf/2109.14158.pdf)
>  We propose a novel second-order optimization framework for training the emerging deep continuous-time models, specifically the Neural Ordinary Differential Equations (Neural ODEs). Since their training already involves expensive gradient computation by solving a backward ODE, deriving efficient second-order methods becomes highly nontrivial. Nevertheless, inspired by the recent Optimal Control (OC) interpretation of training deep networks, we show that a specific continuous-time OC methodology, called Differential Programming, can be adopted to derive backward ODEs for higher-order derivatives at the same O(1) memory cost. We further explore a low-rank representation of the second-order derivatives and show that it leads to efficient preconditioned updates with the aid of Kronecker-based factorization. The resulting method converges much faster than first-order baselines in wall-clock time, and the improvement remains consistent across various applications, e.g. image classification, generative flow, and time-series prediction. Our framework also enables direct architecture optimization, such as the integration time of Neural ODEs, with second-order feedback policies, strengthening the OC perspective as a principled tool of analyzing optimization in deep learning.      
### 57.Labor-right Protecting Dispatch of Meal Delivery Platforms  [ :arrow_down: ](https://arxiv.org/pdf/2109.14156.pdf)
>  The boom in the meal delivery industry brings growing concern about the labor rights of riders. Current dispatch policies of meal-delivery platforms focus mainly on satisfying consumers or minimizing the number of riders for cost savings. There are few discussions on improving the working conditions of riders by algorithm design. The lack of concerns on labor rights in mechanism and dispatch design has resulted in a very large time waste for riders and their risky driving. In this research, we propose a queuing-model-based framework to discuss optimal dispatch policy with the goal of labor rights protection. We apply our framework to develop an algorithm minimizing the waiting time of food delivery riders with guaranteed user experience. Our framework also allows us to manifest the value of restaurants' data about their offline-order numbers on improving the benefits of riders.      
### 58.Lyapunov-stable neural-network control  [ :arrow_down: ](https://arxiv.org/pdf/2109.14152.pdf)
>  Deep learning has had a far reaching impact in robotics. Specifically, deep reinforcement learning algorithms have been highly effective in synthesizing neural-network controllers for a wide range of tasks. However, despite this empirical success, these controllers still lack theoretical guarantees on their performance, such as Lyapunov stability (i.e., all trajectories of the closed-loop system are guaranteed to converge to a goal state under the control policy). This is in stark contrast to traditional model-based controller design, where principled approaches (like LQR) can synthesize stable controllers with provable guarantees. To address this gap, we propose a generic method to synthesize a Lyapunov-stable neural-network controller, together with a neural-network Lyapunov function to simultaneously certify its stability. Our approach formulates the Lyapunov condition verification as a mixed-integer linear program (MIP). Our MIP verifier either certifies the Lyapunov condition, or generates counter examples that can help improve the candidate controller and the Lyapunov function. We also present an optimization program to compute an inner approximation of the region of attraction for the closed-loop system. We apply our approach to robots including an inverted pendulum, a 2D and a 3D quadrotor, and showcase that our neural-network controller outperforms a baseline LQR controller. The code is open sourced at \url{<a class="link-external link-https" href="https://github.com/StanfordASL/neural-network-lyapunov" rel="external noopener nofollow">this https URL</a>}.      
### 59.Fast optimal trajectory generation for a tiltwing VTOL aircraft with application to urban air mobility  [ :arrow_down: ](https://arxiv.org/pdf/2109.14118.pdf)
>  We solve the minimum-thrust optimal trajectory generation problem for the transition of a tiltwing Vertical Take-Off and Landing (VTOL) aircraft using convex optimisation. The method is based on a change of differential operator that allows us to express the simplified point-mass dynamics along a prescribed path and formulate the original nonlinear problem in terms of a pair of convex programs. A case study involving the Airbus A3 Vahana VTOL aircraft is considered for forward and backward transitions. The presented approach provides a fast method to generate a safe optimal transition for a tiltwing VTOL aircraft that can further be leveraged online for control, and guidance purposes.      
### 60.Neuron Growth Control by PDE Backstepping: Axon Length Regulation by Tubulin Flux Actuation in Soma  [ :arrow_down: ](https://arxiv.org/pdf/2109.14095.pdf)
>  In this work, stabilization of an axonal growth in a neuron associated with the dynamics of tubulin concentration is proposed by designing a boundary control. The dynamics are given by a parabolic Partial Differential Equation (PDE) of the tubulin concentration, with a spatial domain of the axon's length governed by an Ordinary Differential Equation (ODE) coupled with the tubulin concentration in the growth cone. We propose a novel backstepping method for the coupled PDE-ODE dynamics with a moving boundary, and design a control law for the tubulin concentration flux in the soma. Through employing the Lyapunov analysis to a nonlinear target system, we prove a local exponential stability of the closed-loop system under the proposed control law in the spatial $H_1$-norm.      
### 61.Local Repair of Neural Networks Using Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2109.14041.pdf)
>  In this paper, we propose a framework to repair a pre-trained feed-forward neural network (NN) to satisfy a set of properties. We formulate the properties as a set of predicates that impose constraints on the output of NN over the target input domain. We define the NN repair problem as a Mixed Integer Quadratic Program (MIQP) to adjust the weights of a single layer subject to the given predicates while minimizing the original loss function over the original training domain. We demonstrate the application of our framework in bounding an affine transformation, correcting an erroneous NN in classification, and bounding the inputs of a NN controller.      
### 62.Deep Unrolled Recovery in Sparse Biological Imaging  [ :arrow_down: ](https://arxiv.org/pdf/2109.14025.pdf)
>  Deep algorithm unrolling has emerged as a powerful model-based approach to develop deep architectures that combine the interpretability of iterative algorithms with the performance gains of supervised deep learning, especially in cases of sparse optimization. This framework is well-suited to applications in biological imaging, where physics-based models exist to describe the measurement process and the information to be recovered is often highly structured. Here, we review the method of deep unrolling, and show how it improves source localization in several biological imaging settings.      
### 63.Phasing Parameter Analysis for Satellite Collision Avoidance in Starlink and Kuiper Constellations  [ :arrow_down: ](https://arxiv.org/pdf/2109.13994.pdf)
>  The phasing parameter F determines the relative phasing between satellites in different orbital planes and thereby affects the relative position of the satellites in a constellation. The collisions between satellites within the constellation can be avoided if the minimum distance among them is large. From among the possible values of F in a constellation, a value of F is desired that leads to the maximum value of the minimum distance between satellites. We investigate F for two biggest upcoming satellite constellations including Starlink Phase 1 Version 3 and Kuiper Shell 2. No existing work or FCC filing provides a value of F that is suitable for these two constellations. We look for the best value of F in these constellations that provides the maximum value of the minimum distance to ensure intra-constellation avoidance of collisions between satellites. To this end, we simulate each constellation for each value of F to find its best value based on ranking. Out of the 22 and 36 possible values of F for Starlink Phase 1 Version 3 and Kuiper Shell 2, respectively, it is observed that the best value of F with highest ranking is 17 and 11 that leads to the largest minimum distance between satellites of 61.83 km and 55.89 km in these constellations, respectively.      
### 64.A framework for quantitative analysis of Computed Tomography images of viral pneumonitis: radiomic features in COVID and non-COVID patients  [ :arrow_down: ](https://arxiv.org/pdf/2109.13931.pdf)
>  Purpose: to optimize a pipeline of clinical data gathering and CT images processing implemented during the COVID-19 pandemic crisis and to develop artificial intelligence model for different of viral pneumonia. Methods: 1028 chest CT image of patients with positive swab were segmented automatically for lung extraction. A Gaussian model developed in Python language was applied to calculate quantitative metrics (QM) describing well-aerated and ill portions of the lungs from the histogram distribution of lung CT numbers in both lungs of each image and in four geometrical subdivision. Furthermore, radiomic features (RF) of first and second order were extracted from bilateral lungs using PyRadiomic tools. QM and RF were used to develop 4 different Multi-Layer Perceptron (MLP) classifier to discriminate images of patients with COVID (n=646) and non-COVID (n=382) viral pneumonia. Results: The Gaussian model applied to lung CT histogram correctly described healthy parenchyma 94% of the patients. The resulting accuracy of the models for COVID diagnosis were in the range 0.76-0.87, as the integral of the receiver operating curve. The best diagnostic performances were associated to the model based on RF of first and second order, with 21 relevant features after LASSO regression and an accuracy of 0.81$\pm$0.02 after 4-fold cross validation Conclusions: Despite these results were obtained with CT images from a single center, a platform for extracting useful quantitative metrics from CT images was developed and optimized. Four artificial intelligence-based models for classifying patients with COVID and non-COVID viral pneumonia were developed and compared showing overall good diagnostic performances      
### 65.Distributed UAV-enabled zero-forcing cooperative jamming scheme for safeguarding future wireless networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.13929.pdf)
>  In this work, we investigate the impact of two cooperative unmanned aerial vehicle (UAV)-based jammers on the secrecy performance of a ground wireless network in the presence of an eavesdropper. For that purpose, we investigate the secrecy-area related metrics, Jamming Coverage and Jamming Efficiency. Moreover, we propose a hybrid metric, the so-called Weighted Secrecy Coverage (WSC) and a virtual distributed multiple-input-multiple-output (MIMO)-based zero-forcing precoding scheme to avoid the jamming effects on the legitimate receiver. For evaluating these metrics, we derive a closed-form position-based metric, the secrecy improvement. Our mathematical derivations and comparative simulations show that the proposed zero-forcing scheme leads to an improvement on the secrecy performance in terms of the WSC, and provides conditions for improvement of Jamming Efficiency. They also show positioning trends on the UAVs over a fixed orbit around the legitimate transmitter as well as power allocation trends for optimal secrecy.      
