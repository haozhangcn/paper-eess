# ArXiv eess --Wed, 22 Sep 2021
### 1.Computing Complexity-aware Plans Using Kolmogorov Complexity  [ :arrow_down: ](https://arxiv.org/pdf/2109.10303.pdf)
>  In this paper, we introduce complexity-aware planning for finite-horizon deterministic finite automata with rewards as outputs, based on Kolmogorov complexity. Kolmogorov complexity is considered since it can detect computational regularities of deterministic optimal policies. We present a planning objective yielding an explicit trade-off between a policy's performance and complexity. It is proven that maximising this objective is non-trivial in the sense that dynamic programming is infeasible. We present two algorithms obtaining low-complexity policies, where the first algorithm obtains a low-complexity optimal policy, and the second algorithm finds a policy maximising performance while maintaining local (stage-wise) complexity constraints. We evaluate the algorithms on a simple navigation task for a mobile robot, where our algorithms yield low-complexity policies that concur with intuition.      
### 2.On Linear Time-Invariant Systems Analysis via A Single Trajectory: A Linear Programming Approach  [ :arrow_down: ](https://arxiv.org/pdf/2109.10198.pdf)
>  In this note, a novel methodology that can extract a number of analysis results for linear time-invariant systems (LTI) given only a single trajectory of the considered system is proposed. The superiority of the proposed technique relies on the fact that it provides an automatic and formal way to obtain valuable information about the controlled system by only having access to a single trajectory over a finite period of time (i.e., the system dynamics is assumed to be unknown). At first, we characterize the stability region of LTI systems given only a single trajectory dataset by constructing the associated Lyapunov function of the system. The Lyapunov function is found by formulating and solving a linear programming (LP) problem. Then, we extend the same methodology to a variety of essential analysis results for LTI systems such as deriving bounds on the output energy, deriving bounds on output peak, deriving $\mathbf{L}_2$ and RMS gains. To illustrate the efficacy of the proposed data-driven paradigm, a comparison analysis between the learned LTI system metrics and the true ones is provided.      
### 3.Intelligent Traffic Control System by Using Image Information  [ :arrow_down: ](https://arxiv.org/pdf/2109.10181.pdf)
>  This paper implements a traffic signal control system by using real-time traffic flow feedback. This system is designed to deal with two-lane intersections. We construct an experiment field similar to the roads and drivers in Taiwan using an autonomous simulation software called Virtual Test Drive (VTD) released by MSC Software. We erect four cameras on the side of the roads to get the image of the intersection, then transfer the image information into traffic flow information. Analyze the traffic information in each lane by using Greenshields traffic flow model. Control the traffic signals by using Webster's method to increase the performance and soothe the traffic.      
### 4.On the Performance of HAPS-assisted Hybrid RF-FSO Multicast Communication Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.10131.pdf)
>  Multicast routing is considered a promising approach for real-time applications to address the massive data traffic demands. In this work, we study the outage probability of a multicast downlink communication network for non-terrestrial communication systems. More precisely, we propose two practical use-cases. In the former model, we propose a high altitude platform station (HAPS) aided mixed radio frequency (RF)/ free-space optical (FSO)/RF communication scheme where a terrestrial ground station intends to communicate with a cluster of nodes through two stratospheric HAPS systems. In the latter model, we assume that the line of sight (LOS) connectivity is inaccessible between the two HAPS systems due to high attenuation caused by large propagation distances. Thereby, we propose a low Earth orbit (LEO) satellite-aided mixed RF/FSO/FSO/RF communication. For the proposed scenarios, outage probability expressions are derived and validated with Monte Carlo (MC) simulations under different conditions.      
### 5.Virtual Reality Gaming on the Cloud: A Reality Check  [ :arrow_down: ](https://arxiv.org/pdf/2109.10114.pdf)
>  Cloud virtual reality (VR) gaming traffic characteristics such as frame size, inter-arrival time, and latency need to be carefully studied as a first step toward scalable VR cloud service provisioning. To this end, in this paper we analyze the behavior of VR gaming traffic and Quality of Service (QoS) when VR rendering is conducted remotely in the cloud. We first build a VR testbed utilizing a cloud server, a commercial VR headset, and an off-the-shelf WiFi router. Using this testbed, we collect and process cloud VR gaming traffic data from different games under a number of network conditions and fixed and adaptive video encoding schemes. To analyze the application-level characteristics such as video frame size, frame inter-arrival time, frame loss and frame latency, we develop an interval threshold based identification method for video frames. Based on the frame identification results, we present two statistical models that capture the behaviour of the VR gaming video traffic. The models can be used by researchers and practitioners to generate VR traffic models for simulations and experiments - and are paramount in designing advanced radio resource management (RRM) and network optimization for cloud VR gaming services. To the best of the authors' knowledge, this is the first measurement study and analysis conducted using a commercial cloud VR gaming platform, and under both fixed and adaptive bitrate streaming. We make our VR traffic data-sets publicly available for further research by the community.      
### 6.Nyquist-Sampling and Degrees of Freedom of Electromagnetic Fields  [ :arrow_down: ](https://arxiv.org/pdf/2109.10040.pdf)
>  A signal-space approach is presented to study the Nyquist sampling and number of degrees of freedom of an electromagnetic field under arbitrary propagation conditions. Conventional signal processing tools such as the multidimensional sampling theorem and Fourier theory are used to build a linear system theoretic interpretation of electromagnetic wave propagations and revisit classical electromagnetic theory results, e.g., bandlimited property of an electromagnetic field, from a signal processing perspective. Scalar electromagnetic fields are considered for simplicity, which physically correspond to acoustic propagation in general or electromagnetic propagation under certain conditions. The developed approach is extended to study ensembles of a stationary random electromagnetic field that is representative of different propagation conditions.      
### 7.Automated segmentation and extraction of posterior eye segment using OCT scans  [ :arrow_down: ](https://arxiv.org/pdf/2109.10000.pdf)
>  This paper proposes an automated method for the segmentation and extraction of the posterior segment of the human eye, including the vitreous, retina, choroid, and sclera compartments, using multi-vendor optical coherence tomography (OCT) scans. The proposed method works in two phases. First extracts the retinal pigment epithelium (RPE) layer by applying the adaptive thresholding technique to identify the retina-choroid junction. Then, it exploits the structure tensor guided approach to extract the inner limiting membrane (ILM) and the choroidal stroma (CS) layers, locating the vitreous-retina and choroid-sclera junctions in the candidate OCT scan. Furthermore, these three junction boundaries are utilized to conduct posterior eye compartmentalization effectively for both healthy and disease eye OCT scans. The proposed framework is evaluated over 1000 OCT scans, where it obtained the mean intersection over union (IoU) and mean Dice similarity coefficient (DSC) scores of 0.874 and 0.930, respectively.      
### 8.On Net Energy Metering X: Optimal Prosumer Decisions, Social Welfare, and Cross-subsidies  [ :arrow_down: ](https://arxiv.org/pdf/2109.09977.pdf)
>  We introduce NEM X, an inclusive retail tariff model that captures features of existing net energy metering (NEM) policies. It is shown that the optimal prosumer decision has three modes: (a) the net-consuming mode where the prosumer consumes more than its behind-the-meter distributed energy resource (DER) production when the DER production is below a predetermined lower threshold, (b) the net-producing mode where the prosumer consumes less than its DER production when the DER production is above a predetermined upper threshold, and (c) the net-zero energy mode where the prosumer's consumption matches to its DER generation when its DER production is between the lower and upper thresholds. Both thresholds are obtained in closed-form. Next, we analyze the regulator's rate-setting process that determines NEM X parameters such as retail/sell rates, fixed charges, and price differentials in TOU tariffs in on and off-peak periods. A stochastic Ramsey pricing program is formulated that maximizes social welfare subject to the revenue break-even constraint for the regulated utility. Performance of NEM X policies is evaluated using real and synthetic data to illuminate impacts of NEM policy designs on social welfare, cross-subsidies of prosumers by consumers, and payback time of DER investments that affect long-run DER adoptions.      
### 9.Ultrasound Domain Adaptation Using Frequency Domain Analysis  [ :arrow_down: ](https://arxiv.org/pdf/2109.09969.pdf)
>  A common issue in exploiting simulated ultrasound data for training neural networks is the domain shift problem, where the trained models on synthetic data are not generalizable to clinical data. Recently, Fourier Domain Adaptation (FDA) has been proposed in the field of computer vision to tackle the domain shift problem by replacing the magnitude of the low-frequency spectrum of a synthetic sample (source) with a real sample (target). This method is attractive in ultrasound imaging given that two important differences between synthetic and real ultrasound data are caused by unknown values of attenuation and speed of sound (SOS) in real tissues. Attenuation leads to slow variations in the amplitude of the B-mode image, and SOS mismatch creates aberration and subsequent blurring. As such, both domain shifts cause differences in the low-frequency components of the envelope data, which are replaced in the proposed method. We demonstrate that applying the FDA method to the synthetic data, simulated by Field II, obtains an 3.5\% higher Dice similarity coefficient for a breast lesion segmentation task.      
### 10.Identifiability of Chemical Reaction Networks with Intrinsic and Extrinsic Noise from Stationary Distributions  [ :arrow_down: ](https://arxiv.org/pdf/2109.09943.pdf)
>  Many biological systems can be modeled by a chemical reaction network with unknown parameters through the chemical master equation. Data available to identify these parameters are often in the form of a stationary distribution, such as obtained from single cell measurements in a cell population. In this work, we introduce a framework for analyzing the identifiability of the reaction rate coefficients of stochastic chemical reaction networks from stationary distribution data. Working with the linear noise approximation, which is a diffusive approximation to the chemical master equation, we give a computational procedure to certify global identifiability based on Hilbert's Nullstellensatz. We present a variety of examples that show the applicability of our method to chemical reaction networks of interest in systems and synthetic biology.      
### 11.Generalization of Safe Optimal Control Actions on Networked Multi-Agent Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.09909.pdf)
>  We propose a unified framework to fast generate a safe optimal control action for a new task from existing controllers on Multi-Agent Systems (MASs). The control action composition is achieved by taking a weighted mixture of the existing controllers according to the contribution of each component task. Instead of sophisticatedly tuning the cost parameters and other hyper-parameters for safe and reliable behavior in the optimal control framework, the safety of each single task solution is guaranteed using the control barrier functions (CBFs) for high-degree stochastic systems, which constrains the system state within a known safe operation region where it originates from. Linearity of CBF constraints in control enables the control action composition. The discussed framework can immediately provide reliable solutions to new tasks by taking a weighted mixture of solved component-task actions and filtering on some CBF constraints, instead of performing an extensive sampling to achieve a new controller. Our results are verified and demonstrated on both a single UAV and two cooperative UAV teams in an environment with obstacles.      
### 12.Estimation of the Scatterer Size Distributions in Quantitative Ultrasound Using Constrained Optimization  [ :arrow_down: ](https://arxiv.org/pdf/2109.09900.pdf)
>  Quantitative ultrasound (QUS) parameters such as the effective scatterer diameter (ESD) reveal tissue properties by analyzing ultrasound backscattered echo signal. ESD can be attained through parametrizing backscatter coefficient using form factor models. However, reporting a single scatterer size cannot accurately characterize a tissue, particularly when the media contains scattering sources with a broad range of sizes. Here we estimate the probability of contribution of each scatterer size by modeling the measured form factor as a linear combination of form factors from individual sacatterer sizes. We perform the estimation using two novel techniques. In the first technique, we cast scatterer size distribution as an optimization problem, and efficiently solve it using a linear system of equations. In the second technique, we use the solution of this system of equations to constrain the optimization function, and solve the constrained problem. The methods are evaluated in simulated backscattered coefficients using Faran theory. We evaluate the robustness of the proposed techniques by adding Gaussian noise. The results show that both methods can accurately estimate the scatterer size distribution, and that the second method outperforms the first one.      
### 13.Load Estimation for Electric Power Distribution Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.09873.pdf)
>  Distribution Load Estimation (DLE) is a key function of Distribution Management System (DMS). In this paper a novel method for presenting historical load data in the form of Representative Load Curves (RLC) is presented. Adaptive Neuro-Fuzzy Inference Systems (ANFIS) is used in this regard to estimate the RLC. Accurate RLCs provide better pseudo-measurements for real-time load estimation in distribution networks. The performance of the proposed method is demonstrated on an 11kV radial distribution network with the aid of the MATLAB software. The mean absolute percent error (MAPE) criterion is used to quantify the accuracy of the estimated RLC.      
### 14.Variational Embedding Multiscale Sample Entropy:complexity-based analysis for multichannel systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.09845.pdf)
>  To quantify the complexity of a system, entropy-based methods have received considerable critical attentions in real-world data analysis. Among numerous entropy algorithms, amplitude-based formulas, represented by Sample Entropy, suffer from a limitation of data length especially when it comes to practical scenarios. And this shortcoming is further highlighted by involving coarse graining procedure in multi-scale process. The unbalance between embedding dimension and data size will undoubtedly result in inaccurate and undefined estimation. To that cause, Variational Embedding Multiscale Sample Entropy is proposed in this paper, which assigns signals from various channels with distinct embedding dimensions. And this algorithm is tested by both stimulated and real signals. Furthermore, the performance of the new entropy is investigated and compared with Multivariate Multiscale Sample Entropy and Variational Embedding Multiscale Diversity Entropy. Two real-world database, wind data sets with varying regimes and physiological database recorded from young and elderly people, were utilized. As a result, the proposed algorithm gives an improved separation for both situations.      
### 15.Assessing clinical utility of Machine Learning and Artificial Intelligence approaches to analyze speech recordings in Multiple Sclerosis: A Pilot Study  [ :arrow_down: ](https://arxiv.org/pdf/2109.09844.pdf)
>  Background: An early diagnosis together with an accurate disease progression monitoring of multiple sclerosis is an important component of successful disease management. Prior studies have established that multiple sclerosis is correlated with speech discrepancies. Early research using objective acoustic measurements has discovered measurable dysarthria. <br>Objective: To determine the potential clinical utility of machine learning and deep learning/AI approaches for the aiding of diagnosis, biomarker extraction and progression monitoring of multiple sclerosis using speech recordings. <br>Methods: A corpus of 65 MS-positive and 66 healthy individuals reading the same text aloud was used for targeted acoustic feature extraction utilizing automatic phoneme segmentation. A series of binary classification models was trained, tuned, and evaluated regarding their Accuracy and area-under-curve. <br>Results: The Random Forest model performed best, achieving an Accuracy of 0.82 on the validation dataset and an area-under-curve of 0.76 across 5 k-fold cycles on the training dataset. 5 out of 7 acoustic features were statistically significant. <br>Conclusion: Machine learning and artificial intelligence in automatic analyses of voice recordings for aiding MS diagnosis and progression tracking seems promising. Further clinical validation of these methods and their mapping onto multiple sclerosis progression is needed, as well as a validating utility for English-speaking populations.      
### 16.A Data-Driven Democratized Control Architecture for Regional Transmission Operators  [ :arrow_down: ](https://arxiv.org/pdf/2109.09813.pdf)
>  As probably the most complicated and critical infrastructure system, U.S. power grids become increasingly vulnerable to extreme events such as cyber-attacks and severe weather, as well as higher DER penetrations and growing information mismatch among system operators, utilities (transmission or generation owners), and end-users. This paper proposes a data-driven democratized control architecture considering two democratization pathways to assist transmission system operators, with a targeted use case of developing online proactive islanding strategies. Detailed discussions on load capability profiling at transmission buses and disaggregation of DER generations are provided and illustrated with real-world utility data. By Combining network and operational constraints, transmission system operators can be equipped with new tools built on top of this architecture, to derive accurate, proactive, and strategic islanding decisions to incorporate the wide range of dynamic portfolios and needs when facing extreme events or unseen grid contingencies.      
### 17.Robust economic model predictive control with zone tracking  [ :arrow_down: ](https://arxiv.org/pdf/2109.09810.pdf)
>  This paper presents a robust economic model predictive control (EMPC) formulation with zone tracking for discrete-time uncertain nonlinear systems. The proposed design ensures that the zone tracking objective is achieved in finite steps and at the same time optimizes the economic performance. In the proposed design, instead of tracking the original target zone, a robust control invariant set within the target zone is determined and is used as the actual zone tracked in the proposed EMPC. This approach ensures that the zone tracking objective is achieved within finite steps and once the zone tracking objective is achieved (the system state enters the robust control invariant set), the system state does not come out of the target zone anymore. To optimize the economic performance within the zone in the presence of disturbances, we introduce the notion of risk factor in the controller design. An algorithm to determine the economic zone to be tracked is provided. The risk factor determines the conservativeness of the controller and provides a way to tune the EMPC for better economic performance. A nonlinear chemical example is presented to demonstrate the performance of the proposed formulation.      
### 18.Stochastic MPC with Multi-modal Predictions for Traffic Intersections  [ :arrow_down: ](https://arxiv.org/pdf/2109.09792.pdf)
>  We propose a Stochastic MPC (SMPC) formulation for autonomous driving at traffic intersections which incorporates multi-modal predictions of surrounding vehicles for collision avoidance constraints. The multi-modal predictions are obtained with Gaussian Mixture Models (GMM) and constraints are formulated as chance-constraints. Our main theoretical contribution is a SMPC formulation that optimizes over a novel feedback policy class designed to exploit additional structure in the GMM predictions, and that is amenable to convex programming. The use of feedback policies for prediction is motivated by the need for reduced conservatism in handling multi-modal predictions of the surrounding vehicles, especially prevalent in traffic intersection scenarios. We evaluate our algorithm along axes of mobility, comfort, conservatism and computational efficiency at a simulated intersection in CARLA. Our simulations use a kinematic bicycle model and multimodal predictions trained on a subset of the Lyft Level 5 prediction dataset. To demonstrate the impact of optimizing over feedback policies, we compare our algorithm with two SMPC baselines that handle multi-modal collision avoidance chance constraints by optimizing over open-loop sequences.      
### 19.An Optimal Control Framework for Joint-channel Parallel MRI Reconstruction without Coil Sensitivities  [ :arrow_down: ](https://arxiv.org/pdf/2109.09738.pdf)
>  Goal: This work aims at developing a novel calibration-free fast parallel MRI (pMRI) reconstruction method incorporate with discrete-time optimal control framework. The reconstruction model is designed to learn a regularization that combines channels and extracts features by leveraging the information sharing among channels of multi-coil images. We propose to recover both magnitude and phase information by taking advantage of structured multiplayer convolutional networks in image and Fourier spaces. Methods: We develop a novel variational model with a learnable objective function that integrates an adaptive multi-coil image combination operator and effective image regularization in the image and Fourier spaces. We cast the reconstruction network as a structured discrete-time optimal control system, resulting in an optimal control formulation of parameter training where the parameters of the objective function play the role of control variables. We demonstrate that the Lagrangian method for solving the control problem is equivalent to back-propagation, ensuring the local convergence of the training algorithm. Results: We conduct a large number of numerical experiments of the proposed method with comparisons to several state-of-the-art pMRI reconstruction networks on real pMRI datasets. The numerical results demonstrate the promising performance of the proposed method evidently. Conclusion: The proposed method provides a general deep network design and training framework for efficient joint-channel pMRI reconstruction. Significance: By learning multi-coil image combination operator and performing regularizations in both image domain and k-space domain, the proposed method achieves a highly efficient image reconstruction network for pMRI.      
### 20.Unsupervised Domain Adaptation with Semantic Consistency across Heterogeneous Modalities for MRI Prostate Lesion Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.09736.pdf)
>  Any novel medical imaging modality that differs from previous protocols e.g. in the number of imaging channels, introduces a new domain that is heterogeneous from previous ones. This common medical imaging scenario is rarely considered in the domain adaptation literature, which handles shifts across domains of the same dimensionality. In our work we rely on stochastic generative modeling to translate across two heterogeneous domains at pixel space and introduce two new loss functions that promote semantic consistency. Firstly, we introduce a semantic cycle-consistency loss in the source domain to ensure that the translation preserves the semantics. Secondly, we introduce a pseudo-labelling loss, where we translate target data to source, label them by a source-domain network, and use the generated pseudo-labels to supervise the target-domain network. Our results show that this allows us to extract systematically better representations for the target domain. In particular, we address the challenge of enhancing performance on VERDICT-MRI, an advanced diffusion-weighted imaging technique, by exploiting labeled mp-MRI data. When compared to several unsupervised domain adaptation approaches, our approach yields substantial improvements, that consistently carry over to the semi-supervised and supervised learning settings.      
### 21.Source-Free Domain Adaptive Fundus Image Segmentation with Denoised Pseudo-Labeling  [ :arrow_down: ](https://arxiv.org/pdf/2109.09735.pdf)
>  Domain adaptation typically requires to access source domain data to utilize their distribution information for domain alignment with the target data. However, in many real-world scenarios, the source data may not be accessible during the model adaptation in the target domain due to privacy issue. This paper studies the practical yet challenging source-free unsupervised domain adaptation problem, in which only an existing source model and the unlabeled target data are available for model adaptation. We present a novel denoised pseudo-labeling method for this problem, which effectively makes use of the source model and unlabeled target data to promote model self-adaptation from pseudo labels. Importantly, considering that the pseudo labels generated from source model are inevitably noisy due to domain shift, we further introduce two complementary pixel-level and class-level denoising schemes with uncertainty estimation and prototype estimation to reduce noisy pseudo labels and select reliable ones to enhance the pseudo-labeling efficacy. Experimental results on cross-domain fundus image segmentation show that without using any source images or altering source training, our approach achieves comparable or even higher performance than state-of-the-art source-dependent unsupervised domain adaptation methods.      
### 22.MetaMedSeg: Volumetric Meta-learning for Few-Shot Organ Segmentation  [ :arrow_down: ](https://arxiv.org/pdf/2109.09734.pdf)
>  The lack of sufficient annotated image data is a common issue in medical image segmentation. For some organs and densities, the annotation may be scarce, leading to poor model training convergence, while other organs have plenty of annotated data. In this work, we present MetaMedSeg, a gradient-based meta-learning algorithm that redefines the meta-learning task for the volumetric medical data with the goal to capture the variety between the slices. We also explore different weighting schemes for gradients aggregation, arguing that different tasks might have different complexity, and hence, contribute differently to the initialization. We propose an importance-aware weighting scheme to train our model. In the experiments, we present an evaluation of the medical decathlon dataset by extracting 2D slices from CT and MRI volumes of different organs and performing semantic segmentation. The results show that our proposed volumetric task definition leads to up to 30% improvement in terms of IoU compared to related baselines. The proposed update rule is also shown to improve the performance for complex scenarios where the data distribution of the target organ is very different from the source organs.      
### 23.Assured Neural Network Architectures for Control and Identification of Nonlinear Systems  [ :arrow_down: ](https://arxiv.org/pdf/2109.10298.pdf)
>  In this paper, we consider the problem of automatically designing a Rectified Linear Unit (ReLU) Neural Network (NN) architecture (number of layers and number of neurons per layer) with the assurance that it is sufficiently parametrized to control a nonlinear system; i.e. control the system to satisfy a given formal specification. This is unlike current techniques, which provide no assurances on the resultant architecture. Moreover, our approach requires only limited knowledge of the underlying nonlinear system and specification. We assume only that the specification can be satisfied by a Lipschitz-continuous controller with a known bound on its Lipschitz constant; the specific controller need not be known. From this assumption, we bound the number of affine functions needed to construct a Continuous Piecewise Affine (CPWA) function that can approximate any Lipschitz-continuous controller that satisfies the specification. Then we connect this CPWA to a NN architecture using the authors' recent results on the Two-Level Lattice (TLL) NN architecture; the TLL architecture was shown to be parameterized by the number of affine functions present in the CPWA function it realizes.      
### 24.Secrecy Offloading Rate Maximization for Multi-Access Mobile Edge Computing Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.10273.pdf)
>  This letter considers a multi-access mobile edge computing (MEC) network consisting of multiple users, multiple base stations, and a malicious eavesdropper. Specifically, the users adopt the partial offloading strategy by partitioning the computation task into several parts. One is executed locally and the others are securely offloaded to multiple MEC servers integrated into the base stations by leveraging the physical layer security to combat the eavesdropping. We jointly optimize power allocation, task partition, subcarrier allocation, and computation resource to maximize the secrecy offloading rate of the users, subject to communication and computation resource constraints. Numerical results demonstrate that our proposed scheme can respectively improve the secrecy offloading rate 1.11%--1.39% and 15.05%--17.35% (versus the increase of tasks' latency requirements), and 1.30%--1.75% and 6.08%--9.22% (versus the increase of the maximum transmit power) compared with the two benchmarks. Moreover, it further emphasizes the necessity of conducting computation offloading over multiple MEC servers.      
### 25.Comparison of single and multitask learning for predicting cognitive decline based on MRI data  [ :arrow_down: ](https://arxiv.org/pdf/2109.10266.pdf)
>  The Alzheimer's Disease Assessment Scale-Cognitive subscale (ADAS-Cog) is a neuropsychological tool that has been designed to assess the severity of cognitive symptoms of dementia. Personalized prediction of the changes in ADAS-Cog scores could help in timing therapeutic interventions in dementia and at-risk populations. In the present work, we compared single and multitask learning approaches to predict the changes in ADAS-Cog scores based on T1-weighted anatomical magnetic resonance imaging (MRI). In contrast to most machine learning-based prediction methods ADAS-Cog changes, we stratified the subjects based on their baseline diagnoses and evaluated the prediction performances in each group. Our experiments indicated a positive relationship between the predicted and observed ADAS-Cog score changes in each diagnostic group, suggesting that T1-weighted MRI has a predictive value for evaluating cognitive decline in the entire AD continuum. We further studied whether correction of the differences in the magnetic field strength of MRI would improve the ADAS-Cog score prediction. The partial least square-based domain adaptation slightly improved the prediction performance, but the improvement was marginal. In summary, this study demonstrated that ADAS-Cog change could be, to some extent, predicted based on anatomical MRI. Based on this study, the recommended method for learning the predictive models is a single-task regularized linear regression due to its simplicity and good performance. It appears important to combine the training data across all subject groups for the most effective predictive models.      
### 26.Short-term traffic prediction using physics-aware neural networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.10253.pdf)
>  In this work, we propose an algorithm performing short-term predictions of the flux of vehicles on a stretch of road, using past measurements of the flux. This algorithm is based on a physics-aware recurrent neural network. A discretization of a macroscopic traffic flow model (using the so-called Traffic Reaction Model) is embedded in the architecture of the network and yields flux predictions based on estimated and predicted space-time dependent traffic parameters. These parameters are themselves obtained using a succession of LSTM ans simple recurrent neural networks. Besides, on top of the predictions, the algorithm yields a smoothing of its inputs which is also physically-constrained by the macroscopic traffic flow model. The algorithm is tested on raw flux measurements obtained from loop detectors.      
### 27.Audiomer: A Convolutional Transformer for Keyword Spotting  [ :arrow_down: ](https://arxiv.org/pdf/2109.10252.pdf)
>  Transformers have seen an unprecedented rise in Natural Language Processing and Computer Vision tasks. However, in audio tasks, they are either infeasible to train due to extremely large sequence length of audio waveforms or reach competitive performance after feature extraction through Fourier-based methods, incurring a loss-floor. In this work, we introduce an architecture, Audiomer, where we combine 1D Residual Networks with Performer Attention to achieve state-of-the-art performance in Keyword Spotting with raw audio waveforms, out-performing all previous methods while also being computationally cheaper, much more parameter and data-efficient. Audiomer allows for deployment in compute-constrained devices and training on smaller datasets.      
### 28.Beam Refinement and User State Acquisition via Integrated Sensing and Communication with OFDM  [ :arrow_down: ](https://arxiv.org/pdf/2109.10243.pdf)
>  The performance of millimeter wave (mmWave) communications strongly relies on accurate beamforming both at base station and user terminal sides, referred to as beam alignment (BA). Existing BA algorithms provide initial yet coarse angle estimates as they typically use a codebook of a finite number of discreteized beams (angles). Towards emerging applications requiring timely and precise tracking of dynamically changing state of users, we consider a system where a base station with a co-located radar receiver estimates relevant state parameters of users and simultaneously sends OFDM-modulated data symbols. In particular, based on a hybrid digital analog data transmitter/radar receiver architecture, we propose a simple beam refinement and initial state acquisition scheme that can be used for beam and user location tracking in a dynamic environment. Numerical results inspired by IEEE802.11ad parameters demonstrate that the proposed method is able to improve significantly the communication rate and further achieve accurate state estimation.      
### 29.A Simple and Fast Coordinate-Descent Augmented-Lagrangian Solver for Model Predictive Control  [ :arrow_down: ](https://arxiv.org/pdf/2109.10205.pdf)
>  This paper proposes a novel Coordinate-Descent Augmented-Lagrangian (CDAL) solver for linear, possibly parameter-varying, model predictive control problems. At each iteration, an augmented Lagrangian (AL) subproblem is solved by coordinate descent (CD), whose computation cost depends linearly on the prediction horizon and quadratically on the state and input dimensions. CDAL is simple to implement and does not require constructing explicitly the matrices of the quadratic programming problem to solve. To favor convergence speed, CDAL employs a reverse cyclic rule for the CD method, the accelerated Nesterov's scheme for updating the dual variables, and a simple diagonal preconditioner. We show that CDAL competes with other state-of-the-art first-order methods, both in case of unstable linear time-invariant and prediction models linearized at runtime. All numerical results are obtained from a very compact, library-free, C implementation of the proposed CDAL solver.      
### 30.Codebook Design and Beam Training for Extremely Large-Scale RIS: Far-Field or Near-Field?  [ :arrow_down: ](https://arxiv.org/pdf/2109.10143.pdf)
>  Reconfigurable intelligent surface (RIS) can improve the capacity of the wireless communication system by providing the extra link between the base station (BS) and the user. In order to resist the "multiplicative fading" effect, RIS is more likely to develop into extremely large-scale RIS (XL-RIS) for future 6G communications. Beam training is an effective way to acquire channel state information (CSI) for the XL-RIS assisted system. Existing beam training schemes rely on the far-field codebook, which is designed based on the far-field channel model. However, due to the large aperture of XL-RIS, the user is more likely to be in the near-field region of XL-RIS. The far-field codebook mismatches the near-field channel model. Thus, the existing far-field beam training scheme will cause severe performance loss in the XL-RIS assisted near-field communications. To solve this problem, we propose the efficient near-field beam training schemes by designing the near-field codebook to match the near-field channel model. Specifically, we firstly design the near-field codebook by considering the near-field cascaded array steering vector of XL-RIS. Then, the optimal codeword for XL-RIS is obtained by the exhausted training procedure between the XL-RIS and the user. In order to reduce the beam training overhead, we further design a hierarchical near-field codebook and propose the corresponding hierarchical near-field beam training scheme, where different levels of sub-codebooks are searched in turn with reduced codebook size. Simulation results show the two proposed near-field beam training schemes both perform better than the existing far-field beam training scheme. Particulary, the hierarchical near-field beam training scheme can greatly reduce the beam training overhead with acceptable performance loss.      
### 31.On the Difficulty of Segmenting Words with Attention  [ :arrow_down: ](https://arxiv.org/pdf/2109.10107.pdf)
>  Word segmentation, the problem of finding word boundaries in speech, is of interest for a range of tasks. Previous papers have suggested that for sequence-to-sequence models trained on tasks such as speech translation or speech recognition, attention can be used to locate and segment the words. We show, however, that even on monolingual data this approach is brittle. In our experiments with different input types, data sizes, and segmentation algorithms, only models trained to predict phones from words succeed in the task. Models trained to predict words from either phones or speech (i.e., the opposite direction needed to generalize to new data), yield much worse results, suggesting that attention-based segmentation is only useful in limited scenarios.      
### 32.Near-Field Wideband Beamforming for Extremely Large Antenna Array  [ :arrow_down: ](https://arxiv.org/pdf/2109.10054.pdf)
>  Extremely large antenna array for wideband communications is a promising technology to achieve a Tbps data rate for next-generation communications. However, due to the extremely large bandwidth and antenna array aperture, the near-field beam split effect will severely decrease the actual transmission rates, which has not been investigated in existing works. To solve this challenging problem, we first reveal the near-field beam split effect and analyze the corresponding array gain loss. Then, a piecewise-far-field model with piecewise-linear phase property is proposed to approximate the near-field channel, based on which we propose a phase-delay focusing method to effectively mitigate the near-field beam split effect. Moreover, a new metric called effective Rayleigh distance is defined, which is more accurate than the classical Rayleigh distance to distinguish the far-field and near-field regions for practical communications. Finally, theoretical and numerical results are provided to demonstrate the effectiveness of our methods.      
### 33.Learning Adaptive Control for SE(3) Hamiltonian Dynamics  [ :arrow_down: ](https://arxiv.org/pdf/2109.09974.pdf)
>  Fast adaptive control is a critical component for reliable robot autonomy in rapidly changing operational conditions. While a robot dynamics model may be obtained from first principles or learned from data, updating its parameters is often too slow for online adaptation to environment changes. This motivates the use of machine learning techniques to learn disturbance descriptors from trajectory data offline as well as the design of adaptive control to estimate and compensate the disturbances online. This paper develops adaptive geometric control for rigid-body systems, such as ground, aerial, and underwater vehicles, that satisfy Hamilton's equations of motion over the SE(3) manifold. Our design consists of an offline system identification stage, followed by an online adaptive control stage. In the first stage, we learn a Hamiltonian model of the system dynamics using a neural ordinary differential equation (ODE) network trained from state-control trajectory data with different disturbance realizations. The disturbances are modeled as a linear combination of nonlinear descriptors. In the second stage, we design a trajectory tracking controller with disturbance compensation from an energy-based perspective. An adaptive control law is employed to adjust the disturbance model online proportional to the geometric tracking errors on the SE(3) manifold. We verify our adaptive geometric controller for trajectory tracking on a fully-actuated pendulum and an under-actuated quadrotor.      
### 34.MESSFN : a Multi-level and Enhanced Spectral-Spatial Fusion Network for Pan-sharpening  [ :arrow_down: ](https://arxiv.org/pdf/2109.09937.pdf)
>  Dominant pan-sharpening frameworks simply concatenate the MS stream and the PAN stream once at a specific level. This way of fusion neglects the multi-level spectral-spatial correlation between the two streams, which is vital to improving the fusion performance. In consideration of this, we propose a Multi-level and Enhanced Spectral-Spatial Fusion Network (MESSFN) with the following innovations: First, to fully exploit and strengthen the above correlation, a Hierarchical Multi-level Fusion Architecture (HMFA) is carefully designed. A novel Spectral-Spatial (SS) stream is established to hierarchically derive and fuse the multi-level prior spectral and spatial expertise from the MS stream and the PAN stream. This helps the SS stream master a joint spectral-spatial representation in the hierarchical network for better modeling the fusion relationship. Second, to provide superior expertise, consequently, based on the intrinsic characteristics of the MS image and the PAN image, two feature extraction blocks are specially developed. In the MS stream, a Residual Spectral Attention Block (RSAB) is proposed to mine the potential spectral correlations between different spectra of the MS image through adjacent cross-spectrum interaction. While in the PAN stream, a Residual Multi-scale Spatial Attention Block (RMSAB) is proposed to capture multi-scale information and reconstruct precise high-frequency details from the PAN image through an improved spatial attention-based inception structure. The spectral and spatial feature representations are enhanced. Extensive experiments on two datasets demonstrate that the proposed network is competitive with or better than state-of-the-art methods. Our code can be found in github.      
### 35.Balancing Control and Pose Optimization for Wheel-legged Robots Navigating Uneven Terrains  [ :arrow_down: ](https://arxiv.org/pdf/2109.09934.pdf)
>  In this paper, we propose a novel approach on controlling wheel-legged quadrupedal robots using pose optimization and force control via quadratic programming (QP). Our method allows the robot to leverage wheel torques to navigate the terrain while keeping the wheel traction and balancing the robot body. In detail, we present a rigid body dynamics with wheels that can be used for real-time balancing control of wheel-legged robots. In addition, we introduce an effective pose optimization method for wheel-legged robot's locomotion over uneven terrains with ramps and stairs. The pose optimization utilized a nonlinear programming (NLP) solver to solve for the optimal poses in terms of joint positions based on kinematic and contact constraints during a stair-climbing task with rolling wheels. In simulation, our approach has successfully validated for the problem of a wheel-legged robot climbing up a 0.34m stair with a slope angle of 80 degrees and shown its versatility in multiple-stair climbing with varied stair runs and rises with wheel traction. Experimental validation on the real robot demonstrated the capability of climbing up on a 0.25m stair with a slope angle of 30 degrees.      
### 36.Meta-Model Structure Selection: Building Polynomial NARX Model for Regression and Classification  [ :arrow_down: ](https://arxiv.org/pdf/2109.09917.pdf)
>  This work presents a new meta-heuristic approach to select the structure of polynomial NARX models for regression and classification problems. The method takes into account the complexity of the model and the contribution of each term to build parsimonious models by proposing a new cost function formulation. The robustness of the new algorithm is tested on several simulated and experimental system with different nonlinear characteristics. The obtained results show that the proposed algorithm is capable of identifying the correct model, for cases where the proper model structure is known, and determine parsimonious models for experimental data even for those systems for which traditional and contemporary methods habitually fails. The new algorithm is validated over classical methods such as the FROLS and recent randomized approaches.      
### 37.Audio Interval Retrieval using Convolutional Neural Networks  [ :arrow_down: ](https://arxiv.org/pdf/2109.09906.pdf)
>  Modern streaming services are increasingly labeling videos based on their visual or audio content. This typically augments the use of technologies such as AI and ML by allowing to use natural speech for searching by keywords and video descriptions. Prior research has successfully provided a number of solutions for speech to text, in the case of a human speech, but this article aims to investigate possible solutions to retrieve sound events based on a natural language query, and estimate how effective and accurate they are. In this study, we specifically focus on the YamNet, AlexNet, and ResNet-50 pre-trained models to automatically classify audio samples using their respective melspectrograms into a number of predefined classes. The predefined classes can represent sounds associated with actions within a video fragment. Two tests are conducted to evaluate the performance of the models on two separate problems: audio classification and intervals retrieval based on a natural language query. Results show that the benchmarked models are comparable in terms of performance, with YamNet slightly outperforming the other two models. YamNet was able to classify single fixed-size audio samples with 92.7% accuracy and 68.75% precision while its average accuracy on intervals retrieval was 71.62% and precision was 41.95%. The investigated method may be embedded into an automated event marking architecture for streaming services.      
### 38.Label-free virtual Hematoxylin and Eosin (H&amp;E) staining using second generation Photoacoustic Remote Sensing (PARS)  [ :arrow_down: ](https://arxiv.org/pdf/2109.09737.pdf)
>  In the past decades, absorption modalities have emerged as powerful tools for label-free functional and structural imaging of cells and tissues. Many biomolecules present unique absorption spectra providing chromophore-specific information on properties such as chemical bonding, and sample composition. As chromophores absorb photons the absorbed energy is emitted as photons (radiative relaxation) or converted to heat and under specific conditions pressure (non-radiative relaxation). Modalities like fluorescence microscopy may capture radiative relaxation to provide contrast, while modalities like photoacoustic microscopy may leverage non-radiative heat and pressures. Here we show an all-optical non-contact total-absorption photoacoustic remote sensing (TA-PARS) microscope, which can capture both radiative and non-radiative absorption effects in a single acquisition. The TA-PARS yields an absorption metric proposed as the quantum efficiency ratio (QER), which visualizes a biomolecules proportional radiative and non-radiative absorption response. The TA-PARS provides label-free visualization of a range of biomolecules enabling convincing analogues to traditional histochemical staining of tissues, effectively providing label-free Hematoxylin and Eosin (H&amp;E)-like visualizations. These findings represent the establishment of an effective all-optical non-contact total-absorption microscope for label-free inspection of biological media.      
